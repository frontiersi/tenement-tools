{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDVSpectra Threshold module tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set globals paths\n",
    "FOLDER_MODULES = r'C:\\Users\\Lewis\\Documents\\GitHub\\tenement-tools\\modules'  \n",
    "FOLDER_SHARED = r'C:\\Users\\Lewis\\Documents\\GitHub\\tenement-tools\\shared'\n",
    "TEST_MODULE = r'C:\\Users\\Lewis\\Documents\\GitHub\\tenement-tools\\tests\\code'\n",
    "GRP_LYR_FILE = r'C:\\Users\\Lewis\\Documents\\GitHub\\tenement-tools\\arc\\lyr\\group_template.lyrx'    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'toolbox'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# import testing functions\n",
    "sys.path.append(TEST_MODULE)\n",
    "import test_funcs\n",
    "\n",
    "# import full arcpy toolbox\n",
    "arcpy.ImportToolbox(r\"C:\\Users\\Lewis\\Documents\\GitHub\\tenement-tools\\arc\\toolbox\\tenement-tools-toolbox.pyt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'test_funcs' from 'C:\\\\Users\\\\Lewis\\\\Documents\\\\GitHub\\\\tenement-tools\\\\tests\\\\code\\\\test_funcs.py'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if scripts change, reload\n",
    "from importlib import reload\n",
    "reload(test_funcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set data files and locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup general io\n",
    "input_folder = r'E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_threshold\\inputs'\n",
    "output_folder = r'E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_threshold\\outputs'\n",
    "\n",
    "# temp nc file and shape file for use when breaking ncs, shapefiles\n",
    "temp_nc = os.path.join(input_folder, 'temp_nc.nc')     # temp nc file for use when breaking ncs\n",
    "temp_shp = os.path.join(input_folder, 'temp_shp.shp')  # temp shp file for use when breaking shapefiles\n",
    "\n",
    "# setup landsat cubes paths\n",
    "ls_cubes = [\n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_threshold\\inputs\\yandi_1_ls_90_20_thresh_odc.nc\",\n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_threshold\\inputs\\yandi_2_ls_90_20_thresh_odc.nc\",\n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_threshold\\inputs\\yandi_3_ls_90_20_thresh_odc.nc\",\n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_threshold\\inputs\\yandi_4_ls_90_20_thresh_odc.nc\",\n",
    "]\n",
    "\n",
    "# setup sentinel2 cubes paths\n",
    "s2_cubes = [\n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_threshold\\inputs\\yandi_1_s2_16_20_thresh_odc.nc\",\n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_threshold\\inputs\\yandi_2_s2_16_20_thresh_odc.nc\",\n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_threshold\\inputs\\yandi_3_s2_16_20_thresh_odc.nc\",\n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_threshold\\inputs\\yandi_4_s2_16_20_thresh_odc.nc\",\n",
    "]\n",
    "\n",
    "# set up presence/absence point shapefile\n",
    "shps = [\n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_threshold\\inputs\\pa_esri_albers.shp\",\n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_threshold\\inputs\\pa_qgis_albers.shp\",\n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_threshold\\inputs\\pa_esri_albers.gdb\\pa_esri_albers\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set specific raw netcdf and shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set top level netcdf and shapefile\n",
    "nc_file = ls_cubes[1]\n",
    "shp_path = shps[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup netcdf corruptor functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are numerous netcdf corruptors. feed a raw nc in, break it, output as temp nc\n",
    "# comment out any that are irrelevant\n",
    "# each of these uncommented will be fed through the tests below\n",
    "def build_nc_corruptors():\n",
    "    \n",
    "    # set up list\n",
    "    nc_corruptors = []\n",
    "\n",
    "    # func: raw default dataset, no changes \n",
    "    nc_corruptors.append({'func': test_funcs.nc_default, 'in_nc': temp_nc})\n",
    "\n",
    "    # func: remove x, y, time, spatial_ref coords only (note: seperate tests)\n",
    "    #nc_corruptors.append({'func': test_funcs.remove_coord, 'in_nc': temp_nc, 'coord': 'x'})\n",
    "    #nc_corruptors.append({'func': test_funcs.remove_coord, 'in_nc': temp_nc, 'coord': 'y'})\n",
    "    #nc_corruptors.append({'func': test_funcs.remove_coord, 'in_nc': temp_nc, 'coord': 'time'})\n",
    "    #nc_corruptors.append({'func': test_funcs.remove_coord, 'in_nc': temp_nc, 'coord': 'spatial_ref'})\n",
    "\n",
    "    # func: remove like band var\n",
    "    #nc_corruptors.append({'func': test_funcs.remove_var, 'in_nc': temp_nc, 'var': 'like'})\n",
    "    \n",
    "    # func: limit number of years in various circumstances (1, 2, 3, 5 years) (note: seperate tests)\n",
    "    #nc_corruptors.append({'func': test_funcs.limit_years, 'in_nc': temp_nc, 's_year': 1990, 'e_year': 1990})\n",
    "    #nc_corruptors.append({'func': test_funcs.limit_years, 'in_nc': temp_nc, 's_year': 2000, 'e_year': 2000})\n",
    "    #nc_corruptors.append({'func': test_funcs.limit_years, 'in_nc': temp_nc, 's_year': 2010, 'e_year': 2010})\n",
    "    #nc_corruptors.append({'func': test_funcs.limit_years, 'in_nc': temp_nc, 's_year': 1990, 'e_year': 1991})\n",
    "    #nc_corruptors.append({'func': test_funcs.limit_years, 'in_nc': temp_nc, 's_year': 2005, 'e_year': 2006})\n",
    "    #nc_corruptors.append({'func': test_funcs.limit_years, 'in_nc': temp_nc, 's_year': 2019, 'e_year': 2020})\n",
    "    #nc_corruptors.append({'func': test_funcs.limit_years, 'in_nc': temp_nc, 's_year': 1990, 'e_year': 1992})\n",
    "    #nc_corruptors.append({'func': test_funcs.limit_years, 'in_nc': temp_nc, 's_year': 2010, 'e_year': 2012})\n",
    "    #nc_corruptors.append({'func': test_funcs.limit_years, 'in_nc': temp_nc, 's_year': 2011, 'e_year': 2019})    \n",
    "\n",
    "    # func: set all vars to nan\n",
    "    #nc_corruptors.append({'func': test_funcs.set_nc_vars_all_nan, 'in_nc': temp_nc})\n",
    "    \n",
    "    # func: set all vars to zero\n",
    "    #nc_corruptors.append({'func': test_funcs.set_nc_vars_all_zero, 'in_nc': temp_nc})\n",
    "\n",
    "    # func: set all vars for 3 rand times to all nan\n",
    "    #nc_corruptors.append({'func': test_funcs.set_nc_vars_random_all_nan, 'in_nc': temp_nc, 'num': 3})\n",
    "\n",
    "    # func: strip all attrs from nc    \n",
    "    #nc_corruptors.append({'func': test_funcs.strip_nc_attributes, 'in_nc': temp_nc})\n",
    "\n",
    "    # func: set vars in first and last time index to all nan\n",
    "    #nc_corruptors.append({'func': test_funcs.set_end_times_to_all_nan, 'in_nc': temp_nc})\n",
    "\n",
    "    # func: reduce whole nc to one random time slice\n",
    "    #nc_corruptors.append({'func': test_funcs.reduce_to_one_scene, 'in_nc': temp_nc})\n",
    "    \n",
    "    # func: remove crs attribute\n",
    "    #nc_corruptors.append({'func': test_funcs.remove_crs_attr, 'in_nc': temp_nc})    \n",
    "    \n",
    "    # func: invalidate crs attribute\n",
    "    #nc_corruptors.append({'func': test_funcs.invalidate_crs_attr, 'in_nc': temp_nc, 'crs_text': 'EPSG:4326'})\n",
    "    #nc_corruptors.append({'func': test_funcs.invalidate_crs_attr, 'in_nc': temp_nc, 'crs_text': ''})\n",
    "    \n",
    "    # func: remove nodatavals attribute\n",
    "    #nc_corruptors.append({'func': test_funcs.remove_nodatavals_attr, 'in_nc': temp_nc})    \n",
    "    \n",
    "    return nc_corruptors\n",
    "\n",
    "nc_corruptors = build_nc_corruptors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup shapefile corruptor functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are numerous shapefile corruptors. feed a raw shp in, break it, output as temp shp\n",
    "# comment out any that are irrelevant\n",
    "# each of these uncommented will be fed through some tests below\n",
    "def build_shp_corruptors():\n",
    "    \n",
    "    # set up list\n",
    "    shp_corruptors = []\n",
    "\n",
    "    # func: raw default dataset, no changes \n",
    "    #shp_corruptors.append({'func': test_funcs.shp_default, 'shp_path': shp_path, 'temp_shp': temp_shp})\n",
    "\n",
    "    # func: convert shapefile to wgs84\n",
    "    #shp_corruptors.append({'func': test_funcs.project_shp_to_wgs84, 'shp_path': shp_path, 'temp_shp': temp_shp})\n",
    "\n",
    "    # func: subset rows to each study area seperately\n",
    "    #shp_corruptors.append({'func': test_funcs.subset_shp_to_area, 'shp_path': shp_path, 'temp_shp': temp_shp, 'area_code': 'a'})\n",
    "    #shp_corruptors.append({'func': test_funcs.subset_shp_to_area, 'shp_path': shp_path, 'temp_shp': temp_shp, 'area_code': 'b'})\n",
    "    #shp_corruptors.append({'func': test_funcs.subset_shp_to_area, 'shp_path': shp_path, 'temp_shp': temp_shp, 'area_code': 'c'})\n",
    "    #shp_corruptors.append({'func': test_funcs.subset_shp_to_area, 'shp_path': shp_path, 'temp_shp': temp_shp, 'area_code': 'd'})\n",
    "    \n",
    "    # func: strip shapefile of projection info\n",
    "    #shp_corruptors.append({'func': test_funcs.strip_shp_proj_file, 'shp_path': shp_path, 'temp_shp': temp_shp})\n",
    "    \n",
    "    # func: convert shapefile pres/abse field to text type\n",
    "    #shp_corruptors.append({'func': test_funcs.convert_shp_pa_field_to_text, 'shp_path': shp_path, 'temp_shp': temp_shp, 'pa_column': 'p_a'})\n",
    "    \n",
    "    # func: randomlly set specific num of pres/abse values to something else\n",
    "    shp_corruptors.append({'func': test_funcs.random_set_shp_p_a_value, 'shp_path': shp_path, 'temp_shp': temp_shp, 'pa_column': 'p_a', 'num_rand_samples': 100, 'set_to_value': 3})\n",
    "    shp_corruptors.append({'func': test_funcs.random_set_shp_p_a_value, 'shp_path': shp_path, 'temp_shp': temp_shp, 'pa_column': 'p_a', 'num_rand_samples': 5, 'set_to_value': 7})\n",
    "    shp_corruptors.append({'func': test_funcs.random_set_shp_p_a_value, 'shp_path': shp_path, 'temp_shp': temp_shp, 'pa_column': 'p_a', 'num_rand_samples': 13, 'set_to_value': -1})\n",
    "    shp_corruptors.append({'func': test_funcs.random_set_shp_p_a_value, 'shp_path': shp_path, 'temp_shp': temp_shp, 'pa_column': 'p_a', 'num_rand_samples': 200, 'set_to_value': 5})\n",
    "    \n",
    "    # func: randomlly set specific num of pres/abse values to null\n",
    "    #shp_corruptors.append({'func': test_funcs.random_set_shp_p_a_null, 'shp_path': shp_path, 'temp_shp': temp_shp, 'pa_column': 'p_a', 'num_rand_samples': 5})\n",
    "    \n",
    "    # func: reduce shapefile pres/abse point number to specified number\n",
    "    shp_corruptors.append({'func': test_funcs.reduce_shp_pa_num_points, 'shp_path': shp_path, 'temp_shp': temp_shp, 'area_code': 'a', 'pa_column': 'p_a', 'num_points': 5})\n",
    "    shp_corruptors.append({'func': test_funcs.reduce_shp_pa_num_points, 'shp_path': shp_path, 'temp_shp': temp_shp, 'area_code': 'b', 'pa_column': 'p_a', 'num_points': 2})\n",
    "    shp_corruptors.append({'func': test_funcs.reduce_shp_pa_num_points, 'shp_path': shp_path, 'temp_shp': temp_shp, 'area_code': 'c', 'pa_column': 'p_a', 'num_points': 15})\n",
    "    shp_corruptors.append({'func': test_funcs.reduce_shp_pa_num_points, 'shp_path': shp_path, 'temp_shp': temp_shp, 'area_code': 'd', 'pa_column': 'p_a', 'num_points': 50})\n",
    "    \n",
    "    # remove all points from shapefile \n",
    "    shp_corruptors.append({'func': test_funcs.remove_all_shp_points, 'shp_path': shp_path, 'temp_shp': temp_shp, 'pa_column': 'p_a'})\n",
    "    \n",
    "    # set all points in shapefile pres/abse column to specific values\n",
    "    shp_corruptors.append({'func': test_funcs.set_all_shp_points_to_value, 'shp_path': shp_path, 'temp_shp': temp_shp, 'pa_column': 'p_a', 'new_val': 1})\n",
    "    shp_corruptors.append({'func': test_funcs.set_all_shp_points_to_value, 'shp_path': shp_path, 'temp_shp': temp_shp, 'pa_column': 'p_a', 'new_val': 0})\n",
    "    shp_corruptors.append({'func': test_funcs.set_all_shp_points_to_value, 'shp_path': shp_path, 'temp_shp': temp_shp, 'pa_column': 'p_a', 'new_val': 3})\n",
    "    \n",
    "    return shp_corruptors\n",
    "\n",
    "shp_corruptors = build_shp_corruptors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test One: Use Median of All Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _test_one_use_all_dates(in_nc, out_func_name='default'):\n",
    "    \"\"\"tests different set ups of the use all dates (median)\"\"\"\n",
    "\n",
    "    # set default params for tool\n",
    "    inputs = {\n",
    "        'in_nc': in_nc,                        # input nc (i.e. temp nc)\n",
    "        'out_nc': '',                          # output nc (i.e. t1a nc)\n",
    "        'in_use_all_dates': True,              # use all dates\n",
    "        'in_specific_year': None,              # select a specific year\n",
    "        'in_occurrence_feat': '',              # occurrence shapefile\n",
    "        'in_pa_column': '',                    # presence/absence column\n",
    "        'in_std_dev': 2.0,                     # standard dev value\n",
    "        'in_if_nodata': 'Any',                 # nodata valyes handling\n",
    "        'in_remove_stray': True,               # remove salt and pepper\n",
    "        'in_convert_binary': True,             # binarise output into 1, 0\n",
    "        'in_add_result_to_map': True,          # add result to map\n",
    "    }\n",
    "    \n",
    "    # set output test string convention\n",
    "    out_nc = os.path.join(output_folder, 't1{}_{}.nc')\n",
    "    \n",
    "    try:\n",
    "        print('\\n\\nRunning t1default (test one default). use all dates input is True (with no shapefile).')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('default', out_func_name), 'in_use_all_dates': True})\n",
    "        arcpy.GDVSpectra_Threshold_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    try:\n",
    "        print('\\n\\nRunning t1a (test one a). use all dates input is False (with no shapefile).')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('a', out_func_name), 'in_use_all_dates': False})\n",
    "        arcpy.GDVSpectra_Threshold_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t1b (test one b). use all dates input is None (with no shapefile).')  \n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('b', out_func_name), 'in_use_all_dates': None})  # none defaults to true\n",
    "        arcpy.GDVSpectra_Threshold_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)    \n",
    "        \n",
    "    # update shapefile and pa column here for followign funcs\n",
    "    inputs.update({'in_occurrence_feat': shp_path, 'in_pa_column': 'p_a'})\n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t1c (test one c). use all dates input is True (with shapefile).')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('c', out_func_name), 'in_use_all_dates': True})\n",
    "        arcpy.GDVSpectra_Threshold_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)   \n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t1d (test one d). use all dates input is False (with shapefile).')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('d', out_func_name), 'in_use_all_dates': False})\n",
    "        arcpy.GDVSpectra_Threshold_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)   \n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t1e (test one e). use all dates input is None (with shapefile).')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('e', out_func_name), 'in_use_all_dates': None})\n",
    "        arcpy.GDVSpectra_Threshold_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)   \n",
    "        \n",
    "def perform_test_one(in_nc, temp_nc, nc_corruption_funcs):\n",
    "    \"\"\"iterates corrupt ncs and runs through parameter tests\"\"\"\n",
    "    \n",
    "    print('\\nRunning all data corruption functions through test one.')\n",
    "    print('- ' * 50)\n",
    "\n",
    "    # iterate different dataset corruptors and feed each into test func\n",
    "    for func in nc_corruption_funcs:\n",
    "\n",
    "        # get func name and notify\n",
    "        print('\\nRunning data corruption function: {}'.format(func['func'].__name__))\n",
    "        print('- ' * 50)\n",
    "        help(func['func'])\n",
    "\n",
    "        # make new instance of func dict so we dont destroy keys\n",
    "        func_copy = func.copy()\n",
    "\n",
    "        # get parameter names and entered value\n",
    "        params, inputs = [], []\n",
    "        for k, v in func_copy.items():\n",
    "            if k != 'func':\n",
    "                print('With parameter: {} of value: {}'.format(k, v))\n",
    "\n",
    "        # start the test\n",
    "        print('\\nBeginning test.')\n",
    "\n",
    "        # duplicate raw netcdf for testing\n",
    "        test_funcs.create_temp_nc(in_nc=in_nc, out_nc=temp_nc)\n",
    "\n",
    "        # prepare current function with associated parameters and run\n",
    "        f = func_copy['func']  # get func only\n",
    "        func_copy.pop('func')  # get params only\n",
    "        f(**func_copy)         # run func\n",
    "\n",
    "        # test current corrupted netcdf\n",
    "        _test_one_use_all_dates(temp_nc, out_func_name=str(func['func'].__name__))\n",
    "\n",
    "        # add some space to print out\n",
    "        print('- ' * 50)\n",
    "        print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test One: Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running all data corruption functions through test one.\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "Running data corruption function: remove_nodatavals_attr\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Help on function remove_nodatavals_attr in module test_funcs:\n",
      "\n",
      "remove_nodatavals_attr(in_nc)\n",
      "    strip nodatavals attribute from nc\n",
      "\n",
      "With parameter: in_nc of value: E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_threshold\\inputs\\temp_nc.nc\n",
      "\n",
      "Beginning test.\n",
      "Duplicating cube: E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_threshold\\inputs\\yandi_2_ls_90_20_thresh_odc.nc\n",
      "Stripping nodatavals attribute from nc dim\n",
      "\n",
      "\n",
      "Running t1default (test one default). use all dates input is True (with no shapefile).\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 2730, in execute\n",
      "RuntimeError: No active exception to reraise\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetCDF nodatavals attribute not found.\n",
      "Failed to execute (GDVSpectra_Threshold).\n",
      "\n",
      "\n",
      "\n",
      "Running t1a (test one a). use all dates input is False (with no shapefile).\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 2730, in execute\n",
      "RuntimeError: No active exception to reraise\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetCDF nodatavals attribute not found.\n",
      "Failed to execute (GDVSpectra_Threshold).\n",
      "\n",
      "\n",
      "\n",
      "Running t1b (test one b). use all dates input is None (with no shapefile).\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 2730, in execute\n",
      "RuntimeError: No active exception to reraise\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetCDF nodatavals attribute not found.\n",
      "Failed to execute (GDVSpectra_Threshold).\n",
      "\n",
      "\n",
      "\n",
      "Running t1c (test one c). use all dates input is True (with shapefile).\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Reading records within shapefile.\n",
      "Rows read from shapefile successfully.\n",
      "Subsetting records from dataframe.\n",
      "Subset records successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 2730, in execute\n",
      "RuntimeError: No active exception to reraise\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetCDF nodatavals attribute not found.\n",
      "Failed to execute (GDVSpectra_Threshold).\n",
      "\n",
      "\n",
      "\n",
      "Running t1d (test one d). use all dates input is False (with shapefile).\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Reading records within shapefile.\n",
      "Rows read from shapefile successfully.\n",
      "Subsetting records from dataframe.\n",
      "Subset records successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 2730, in execute\n",
      "RuntimeError: No active exception to reraise\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetCDF nodatavals attribute not found.\n",
      "Failed to execute (GDVSpectra_Threshold).\n",
      "\n",
      "\n",
      "\n",
      "Running t1e (test one e). use all dates input is None (with shapefile).\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Reading records within shapefile.\n",
      "Rows read from shapefile successfully.\n",
      "Subsetting records from dataframe.\n",
      "Subset records successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 2730, in execute\n",
      "RuntimeError: No active exception to reraise\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetCDF nodatavals attribute not found.\n",
      "Failed to execute (GDVSpectra_Threshold).\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run test one\n",
    "perform_test_one(in_nc=nc_file, temp_nc=temp_nc, nc_corruption_funcs=nc_corruptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "scrolled": false
   },
   "source": [
    "### Test Two: Specific Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _test_two_specific_year(in_nc, out_func_name='default'):\n",
    "    \"\"\"tests different set ups of the dry months input\"\"\"\n",
    "\n",
    "    # set default params for tool\n",
    "    inputs = {\n",
    "        'in_nc': in_nc,                        # input nc (i.e. temp nc)\n",
    "        'out_nc': '',                          # output nc (i.e. t1a nc)\n",
    "        'in_use_all_dates': False,              # use all dates\n",
    "        'in_specific_year': '',              # select a specific year\n",
    "        'in_occurrence_feat': '',              # occurrence shapefile\n",
    "        'in_pa_column': '',                    # presence/absence column\n",
    "        'in_std_dev': 2.0,                     # standard dev value\n",
    "        'in_if_nodata': 'Any',                 # nodata valyes handling\n",
    "        'in_remove_stray': True,               # remove salt and pepper\n",
    "        'in_convert_binary': True,             # binarise output into 1, 0\n",
    "        'in_add_result_to_map': True,          # add result to map\n",
    "    }\n",
    "    \n",
    "    # set output test string convention\n",
    "    out_nc = os.path.join(output_folder, 't2{}_{}.nc')\n",
    "    \n",
    "    try:\n",
    "        print('\\n\\nRunning t2default (test two default). in specific year input is 2018 (without shapefile).')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('default', out_func_name), 'in_specific_year': 2018})\n",
    "        arcpy.GDVSpectra_Threshold_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    try:\n",
    "        print('\\n\\nRunning t2a (test two a). in specific year input is None (without shapefile).')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('a', out_func_name), 'in_specific_year': None})\n",
    "        arcpy.GDVSpectra_Threshold_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t2b (test two b). in specific year input is 1990 (without shapefile).')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('b', out_func_name), 'in_specific_year': 1993})\n",
    "        arcpy.GDVSpectra_Threshold_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)      \n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t2c (test two c). in specific year input is 2012 (without shapefile).')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('c', out_func_name), 'in_specific_year': 2012})\n",
    "        arcpy.GDVSpectra_Threshold_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)  \n",
    "        \n",
    "    # update shapefile and pa column here for followign funcs\n",
    "    inputs.update({'in_occurrence_feat': shp_path, 'in_pa_column': 'p_a'})        \n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t2d (test two d). in specific year input is 2018 (with shapefile).')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('d', out_func_name), 'in_specific_year': None})\n",
    "        arcpy.GDVSpectra_Threshold_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        print('\\n\\nRunning t2e (test two e). in specific year input is 2018 (with shapefile).')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('e', out_func_name), 'in_specific_year': 2018})\n",
    "        arcpy.GDVSpectra_Threshold_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t2e (test two f). in specific year input is 1990 (with shapefile).')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('f', out_func_name), 'in_specific_year': 1993})\n",
    "        arcpy.GDVSpectra_Threshold_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)      \n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t2g (test two g). in specific year input is 2012 (with shapefile).')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('g', out_func_name), 'in_specific_year': 2012})\n",
    "        arcpy.GDVSpectra_Threshold_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)  \n",
    "        \n",
    "\n",
    "def perform_test_two(in_nc, temp_nc, nc_corruption_funcs):\n",
    "    \"\"\"iterates corrupt ncs and runs through parameter tests\"\"\"\n",
    "    \n",
    "    print('\\nRunning all data corruption functions through test two.')\n",
    "    print('- ' * 50)\n",
    "\n",
    "    # iterate different dataset corruptors and feed each into test func\n",
    "    for func in nc_corruption_funcs:\n",
    "\n",
    "        # get func name and notify\n",
    "        print('\\nRunning data corruption function: {}'.format(func['func'].__name__))\n",
    "        print('- ' * 50)\n",
    "        help(func['func'])\n",
    "\n",
    "        # make new instance of func dict so we dont destroy keys\n",
    "        func_copy = func.copy()\n",
    "\n",
    "        # get parameter names and entered value\n",
    "        params, inputs = [], []\n",
    "        for k, v in func_copy.items():\n",
    "            if k != 'func':\n",
    "                print('With parameter: {} of value: {}'.format(k, v))\n",
    "\n",
    "        # start the test\n",
    "        print('\\nBeginning test.')\n",
    "\n",
    "        # duplicate raw netcdf for testing\n",
    "        test_funcs.create_temp_nc(in_nc=in_nc, out_nc=temp_nc)\n",
    "\n",
    "        # prepare current function with associated parameters and run\n",
    "        f = func_copy['func']  # get func only\n",
    "        func_copy.pop('func')  # get params only\n",
    "        f(**func_copy)         # run func\n",
    "\n",
    "        # test current corrupted netcdf\n",
    "        _test_two_specific_year(temp_nc, out_func_name=str(func['func'].__name__))\n",
    "\n",
    "        # add some space to print out\n",
    "        print('- ' * 50)\n",
    "        print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Two: Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running all data corruption functions through test two.\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "Running data corruption function: remove_nodatavals_attr\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Help on function remove_nodatavals_attr in module test_funcs:\n",
      "\n",
      "remove_nodatavals_attr(in_nc)\n",
      "    strip nodatavals attribute from nc\n",
      "\n",
      "With parameter: in_nc of value: E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_threshold\\inputs\\temp_nc.nc\n",
      "\n",
      "Beginning test.\n",
      "Duplicating cube: E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_threshold\\inputs\\yandi_2_ls_90_20_thresh_odc.nc\n",
      "Stripping nodatavals attribute from nc dim\n",
      "\n",
      "\n",
      "Running t2default (test two default). in specific year input is 2018 (without shapefile).\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 2730, in execute\n",
      "RuntimeError: No active exception to reraise\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetCDF nodatavals attribute not found.\n",
      "Failed to execute (GDVSpectra_Threshold).\n",
      "\n",
      "\n",
      "\n",
      "Running t2a (test two a). in specific year input is None (without shapefile).\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 2730, in execute\n",
      "RuntimeError: No active exception to reraise\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetCDF nodatavals attribute not found.\n",
      "Failed to execute (GDVSpectra_Threshold).\n",
      "\n",
      "\n",
      "\n",
      "Running t2b (test two b). in specific year input is 1990 (without shapefile).\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 2730, in execute\n",
      "RuntimeError: No active exception to reraise\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetCDF nodatavals attribute not found.\n",
      "Failed to execute (GDVSpectra_Threshold).\n",
      "\n",
      "\n",
      "\n",
      "Running t2c (test two c). in specific year input is 2012 (without shapefile).\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 2730, in execute\n",
      "RuntimeError: No active exception to reraise\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetCDF nodatavals attribute not found.\n",
      "Failed to execute (GDVSpectra_Threshold).\n",
      "\n",
      "\n",
      "\n",
      "Running t2d (test two d). in specific year input is 2018 (with shapefile).\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Reading records within shapefile.\n",
      "Rows read from shapefile successfully.\n",
      "Subsetting records from dataframe.\n",
      "Subset records successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 2730, in execute\n",
      "RuntimeError: No active exception to reraise\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetCDF nodatavals attribute not found.\n",
      "Failed to execute (GDVSpectra_Threshold).\n",
      "\n",
      "\n",
      "\n",
      "Running t2e (test two e). in specific year input is 2018 (with shapefile).\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Reading records within shapefile.\n",
      "Rows read from shapefile successfully.\n",
      "Subsetting records from dataframe.\n",
      "Subset records successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 2730, in execute\n",
      "RuntimeError: No active exception to reraise\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetCDF nodatavals attribute not found.\n",
      "Failed to execute (GDVSpectra_Threshold).\n",
      "\n",
      "\n",
      "\n",
      "Running t2e (test two f). in specific year input is 1990 (with shapefile).\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Reading records within shapefile.\n",
      "Rows read from shapefile successfully.\n",
      "Subsetting records from dataframe.\n",
      "Subset records successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 2730, in execute\n",
      "RuntimeError: No active exception to reraise\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetCDF nodatavals attribute not found.\n",
      "Failed to execute (GDVSpectra_Threshold).\n",
      "\n",
      "\n",
      "\n",
      "Running t2g (test two g). in specific year input is 2012 (with shapefile).\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Reading records within shapefile.\n",
      "Rows read from shapefile successfully.\n",
      "Subsetting records from dataframe.\n",
      "Subset records successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 2730, in execute\n",
      "RuntimeError: No active exception to reraise\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetCDF nodatavals attribute not found.\n",
      "Failed to execute (GDVSpectra_Threshold).\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run test two\n",
    "perform_test_two(in_nc=nc_file, temp_nc=temp_nc, nc_corruption_funcs=nc_corruptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Three: Occurrence Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _test_three_occ_points(in_nc, in_shp, out_func_name='default'):\n",
    "    \"\"\"tests different set ups of the occurrence point input\"\"\"\n",
    "\n",
    "    # set default params for tool\n",
    "    inputs = {\n",
    "        'in_nc': in_nc,                        # input nc (i.e. temp nc)\n",
    "        'out_nc': '',                          # output nc (i.e. t1a nc)\n",
    "        'in_use_all_dates': True,              # use all dates\n",
    "        'in_specific_year': None,              # select a specific year\n",
    "        'in_occurrence_feat': '',              # occurrence shapefile\n",
    "        'in_pa_column': '',                    # presence/absence column\n",
    "        'in_std_dev': 2.0,                     # standard dev value\n",
    "        'in_if_nodata': '',                    # nodata valyes handling\n",
    "        'in_remove_stray': True,               # remove salt and pepper\n",
    "        'in_convert_binary': True,             # binarise output into 1, 0\n",
    "        'in_add_result_to_map': True,          # add result to map\n",
    "    }\n",
    "    \n",
    "    # set output test string convention\n",
    "    out_nc = os.path.join(output_folder, 't3{}_{}.nc')\n",
    "            \n",
    "    try:\n",
    "        print('\\n\\nRunning t3default (test three default). occurrence points input is esri albers.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('default', out_func_name), 'in_occurrence_feat': in_shp, 'in_pa_column': 'p_a'})\n",
    "        arcpy.GDVSpectra_Threshold_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    #try:\n",
    "        #print('\\n\\nRunning t3a (test three a). occurrence points input is esri albers.')\n",
    "        #print('- ' * 50)\n",
    "        #inputs.update({'out_nc': out_nc.format('a', out_func_name), 'in_occurrence_feat': in_shp, 'in_pa_column': 'p_a'})\n",
    "        #arcpy.GDVSpectra_Threshold_toolbox(*list(inputs.values()))\n",
    "    #except Exception as e:\n",
    "        #print(e)        \n",
    "        \n",
    "def perform_test_three(in_nc, temp_nc, in_shp, temp_shp, nc_corruption_funcs, shp_corruption_funcs):\n",
    "    \"\"\"iterates corrupt ncs and runs through parameter tests\"\"\"\n",
    "\n",
    "    print('\\nRunning all data corruption functions through test three.')\n",
    "    print('- ' * 50)\n",
    "\n",
    "    # iterate different dataset corruptors and feed each into test func\n",
    "    for nc_func in nc_corruption_funcs:\n",
    "\n",
    "        # get func name and notify\n",
    "        print('\\nRunning nc corruption function: {}'.format(nc_func['func'].__name__))\n",
    "        print('- ' * 50)\n",
    "        help(nc_func['func'])\n",
    "\n",
    "        # make new instance of func dict so we dont destroy keys\n",
    "        nc_func_copy = nc_func.copy()\n",
    "\n",
    "        # get parameter names and entered value\n",
    "        nc_params, nc_inputs = [], []\n",
    "        for k, v in nc_func_copy.items():\n",
    "            if k != 'func':\n",
    "                print('With parameter: {} of value: {}'.format(k, v))\n",
    "\n",
    "        # duplicate raw netcdf for testing\n",
    "        test_funcs.create_temp_nc(in_nc=in_nc, out_nc=temp_nc)\n",
    "\n",
    "        # prepare current function with associated parameters and run\n",
    "        nc_f = nc_func_copy['func']   # get func only\n",
    "        nc_func_copy.pop('func')      # get params only\n",
    "        nc_f(**nc_func_copy)          # run func\n",
    "\n",
    "        for shp_func in shp_corruption_funcs:\n",
    "            \n",
    "            # get func name and notify           \n",
    "            print('\\nRunning shp corruption function: {}'.format(shp_func['func'].__name__))\n",
    "            help(shp_func['func'])\n",
    "            \n",
    "            # make new instance of func dict so we dont destroy keys\n",
    "            shp_func_copy = shp_func.copy()\n",
    "                        \n",
    "            # get parameter names and entered value\n",
    "            shp_params, shp_inputs = [], []\n",
    "            for k, v in shp_func_copy.items():\n",
    "                if k != 'func':\n",
    "                    print('With parameter: {} of value: {}'.format(k, v))\n",
    "                    \n",
    "            # prepare current function with associated parameters and run\n",
    "            shp_f = shp_func_copy['func']   # get func only\n",
    "            shp_func_copy.pop('func')       # get params only\n",
    "            shp_f(**shp_func_copy)          # run func\n",
    "            \n",
    "            # start the test\n",
    "            print('\\nBeginning test.')\n",
    "            \n",
    "            # test current corrupted netcdf and shapefile\n",
    "            _test_three_occ_points(temp_nc, temp_shp, out_func_name=str(nc_func['func'].__name__))\n",
    "\n",
    "\n",
    "            # add some space to print out\n",
    "            print('- ' * 50)\n",
    "            print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Three: Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running all data corruption functions through test three.\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "Running nc corruption function: nc_default\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Help on function nc_default in module test_funcs:\n",
      "\n",
      "nc_default(in_nc)\n",
      "    no changes to raw dataset, used for default test\n",
      "\n",
      "With parameter: in_nc of value: E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_threshold\\inputs\\temp_nc.nc\n",
      "Duplicating cube: E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_threshold\\inputs\\yandi_2_ls_90_20_thresh_odc.nc\n",
      "No changes, setting up for default dataset.\n",
      "\n",
      "Running shp corruption function: random_set_shp_p_a_value\n",
      "Help on function random_set_shp_p_a_value in module test_funcs:\n",
      "\n",
      "random_set_shp_p_a_value(shp_path, temp_shp, pa_column, num_rand_samples, set_to_value)\n",
      "    random set a specified number of pres/abse values to specified value\n",
      "\n",
      "With parameter: shp_path of value: E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_threshold\\inputs\\pa_esri_albers.shp\n",
      "With parameter: temp_shp of value: E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_threshold\\inputs\\temp_shp.shp\n",
      "With parameter: pa_column of value: p_a\n",
      "With parameter: num_rand_samples of value: 5\n",
      "With parameter: set_to_value of value: 7\n",
      "Setting random 5 pres/abse values to 7\n",
      "\n",
      "Beginning test.\n",
      "\n",
      "\n",
      "Running t3default (test three default). occurrence points input is esri albers.\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Presence/absence column does not contain just 1s and 0s.\n",
      "Failed to execute (GDVSpectra_Threshold).\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run test three\n",
    "perform_test_three(in_nc=nc_file, temp_nc=temp_nc, in_shp=shp_path, temp_shp=temp_shp, \n",
    "                   nc_corruption_funcs=nc_corruptors, shp_corruption_funcs=shp_corruptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_occurrence_feat = r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_threshold\\inputs\\temp_shp.shp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with arcpy.da.SearchCursor(in_occurrence_feat, ['p_a']) as cursor:\n",
    "    vals = np.unique([row[0] for row in cursor])\n",
    "    if len(vals) != 2 or (0 not in vals or 1 not in vals):\n",
    "        arcpy.AddError('Presence/absence column does not contain just 1s and 0s.')\n",
    "        #return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "if len(vals) != 2 or (0 not in vals or 1 not in vals):\n",
    "    print('1')\n",
    "    arcpy.AddError('Presence/absence column does not contain just 1s and 0s.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(vals) != 2 or (0 not in vals or 1 not in vals):\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Four: Moisture Idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _test_four_mst_idx(in_nc, out_func_name='default'):\n",
    "    \"\"\"tests different set ups of the mst idx input\"\"\"\n",
    "\n",
    "    # set default params for tool\n",
    "    inputs = {\n",
    "        'in_nc': in_nc,                        # input nc (i.e. temp nc)\n",
    "        'out_nc': '',                          # output nc (i.e. t1a nc)\n",
    "        'in_wet_months': '1;2;3',              # wet months \n",
    "        'in_dry_months': '9;10;11',            # dry months \n",
    "        'in_veg_idx': 'MAVI',                  # vege index name\n",
    "        'in_mst_idx': '',                      # moisture index name       \n",
    "        'in_zscore_pvalue': None,              # zscore pvalue\n",
    "        'in_ivt_qupper': 0.99,                 # upper quantile for standardisation\n",
    "        'in_ivt_qlower': 0.05,                 # lower quantile for standardisation\n",
    "        'in_fmask_flags': 'Valid;Snow;Water',  # fmask flag values\n",
    "        'in_max_cloud': 10,                    # max cloud percentage\n",
    "        'in_interpolate': True,                # interpolate missing pixels\n",
    "        'in_add_result_to_map': True,          # add result to map\n",
    "    }\n",
    "    \n",
    "    # set output test string convention\n",
    "    out_nc = os.path.join(output_folder, 't4{}_{}.nc')\n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t4default (test four default). Mst Idx input is \"NDMI\".')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('default', out_func_name), 'in_mst_idx': 'NDMI'})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)        \n",
    "    \n",
    "    try:\n",
    "        print('\\n\\nRunning t4a (test four a). Mst Idx input is \"\".')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('a', out_func_name), 'in_mst_idx': ''})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t4b (test four b). Mst Idx input is NDMI.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('b', out_func_name), 'in_mst_idx': 'NDMI'})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)    \n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t4c (test four c). Mst Idx input is GVMI.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('c', out_func_name), 'in_mst_idx': 'GVMI'})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)  \n",
    "\n",
    "    try:\n",
    "        print('\\n\\nRunning t4d (test four d). Mst Idx input does not exist (MRVA).')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('d', out_func_name), 'in_mst_idx': 'MRVA'})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "def perform_test_four(in_nc, temp_nc, nc_corruption_funcs):\n",
    "    \"\"\"iterates corrupt ncs and runs through parameter tests\"\"\"\n",
    "\n",
    "    print('\\nRunning all data corruption functions through test four.')\n",
    "    print('- ' * 50)\n",
    "\n",
    "    # iterate different dataset corruptors and feed each into test func\n",
    "    for func in nc_corruption_funcs:\n",
    "\n",
    "        # get func name and notify\n",
    "        print('\\nRunning data corruption function: {}'.format(func['func'].__name__))\n",
    "        print('- ' * 50)\n",
    "        help(func['func'])\n",
    "\n",
    "        # make new instance of func dict so we dont destroy keys\n",
    "        func_copy = func.copy()\n",
    "\n",
    "        # get parameter names and entered value\n",
    "        params, inputs = [], []\n",
    "        for k, v in func_copy.items():\n",
    "            if k != 'func':\n",
    "                print('With parameter: {} of value: {}'.format(k, v))\n",
    "\n",
    "        # start the test\n",
    "        print('\\nBeginning test.')\n",
    "\n",
    "        # duplicate raw netcdf for testing\n",
    "        test_funcs.create_temp_nc(in_nc=in_nc, out_nc=temp_nc)\n",
    "\n",
    "        # prepare current function with associated parameters and run\n",
    "        f = func_copy['func']  # get func only\n",
    "        func_copy.pop('func')  # get params only\n",
    "        f(**func_copy)         # run func\n",
    "\n",
    "        # test current corrupted netcdf\n",
    "        _test_four_mst_idx(temp_nc, out_func_name=str(func['func'].__name__))\n",
    "\n",
    "        # add some space to print out\n",
    "        print('- ' * 50)\n",
    "        print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Four: Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# run test four\n",
    "perform_test_four(in_nc=nc_file, temp_nc=temp_nc, nc_corruption_funcs=nc_corruptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Five: Outlier Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _test_five_outlier(in_nc, out_func_name='default'):\n",
    "    \"\"\"tests different set ups of the outlier correction input\"\"\"\n",
    "\n",
    "    # set default params for tool\n",
    "    inputs = {\n",
    "        'in_nc': in_nc,                        # input nc (i.e. temp nc)\n",
    "        'out_nc': '',                          # output nc (i.e. t1a nc)\n",
    "        'in_wet_months': '1;2;3',              # wet months \n",
    "        'in_dry_months': '9;10;11',            # dry months \n",
    "        'in_veg_idx': 'MAVI',                  # vege index name\n",
    "        'in_mst_idx': 'NDMI',                  # moisture index name       \n",
    "        'in_zscore_pvalue': '',                # zscore pvalue\n",
    "        'in_ivt_qupper': 0.99,                 # upper quantile for standardisation\n",
    "        'in_ivt_qlower': 0.05,                 # lower quantile for standardisation\n",
    "        'in_fmask_flags': 'Valid;Snow;Water',  # fmask flag values\n",
    "        'in_max_cloud': 10,                    # max cloud percentage\n",
    "        'in_interpolate': True,                # interpolate missing pixels\n",
    "        'in_add_result_to_map': True,          # add result to map\n",
    "    }\n",
    "    \n",
    "    # set output test string convention\n",
    "    out_nc = os.path.join(output_folder, 't5{}_{}.nc')\n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t5default (test five default). Outlier correction input is None.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('default', out_func_name), 'in_zscore_pvalue': None})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)        \n",
    "\n",
    "    try:\n",
    "        print('\\n\\nRunning t5a (test five a). Outlier correction input is \"\".')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('a', out_func_name), 'in_zscore_pvalue': ''})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t5b (test five b). Outlier correction input is None.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('b', out_func_name), 'in_zscore_pvalue': None})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)    \n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t5c (test five c). Outlier correction input is string of 0.01.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('c', out_func_name), 'in_zscore_pvalue': '0.01'})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)  \n",
    "\n",
    "    try:\n",
    "        print('\\n\\nRunning t5d (test five d). Outlier correction input is string of 0.05.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('d', out_func_name), 'in_zscore_pvalue': '0.05'})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)  \n",
    "             \n",
    "    try:\n",
    "        print('\\n\\nRunning t5e (test five e). Outlier correction input is string of 0.1.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('e', out_func_name), 'in_zscore_pvalue': '0.1'})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)  \n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t5f (test five f). Outlier correction input is string of 1.0 (not supported).')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('f', out_func_name), 'in_zscore_pvalue': '1.0'})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)      \n",
    "    \n",
    "    try:\n",
    "        print('\\n\\nRunning t5g (test five g). Outlier correction input is float of 0.01.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('g', out_func_name), 'in_zscore_pvalue': 0.01})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)   \n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t5h (test five h). Outlier correction input is int of 1.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('h', out_func_name), 'in_zscore_pvalue': 1})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e) \n",
    "        \n",
    "def perform_test_five(in_nc, temp_nc, nc_corruption_funcs):\n",
    "    \"\"\"iterates corrupt ncs and runs through parameter tests\"\"\"\n",
    "\n",
    "    print('\\nRunning all data corruption functions through test five.')\n",
    "    print('- ' * 50)\n",
    "\n",
    "    # iterate different dataset corruptors and feed each into test func\n",
    "    for func in nc_corruption_funcs:\n",
    "\n",
    "        # get func name and notify\n",
    "        print('\\nRunning data corruption function: {}'.format(func['func'].__name__))\n",
    "        print('- ' * 50)\n",
    "        help(func['func'])\n",
    "\n",
    "        # make new instance of func dict so we dont destroy keys\n",
    "        func_copy = func.copy()\n",
    "\n",
    "        # get parameter names and entered value\n",
    "        params, inputs = [], []\n",
    "        for k, v in func_copy.items():\n",
    "            if k != 'func':\n",
    "                print('With parameter: {} of value: {}'.format(k, v))\n",
    "\n",
    "        # start the test\n",
    "        print('\\nBeginning test.')\n",
    "\n",
    "        # duplicate raw netcdf for testing\n",
    "        test_funcs.create_temp_nc(in_nc=in_nc, out_nc=temp_nc)\n",
    "\n",
    "        # prepare current function with associated parameters and run\n",
    "        f = func_copy['func']  # get func only\n",
    "        func_copy.pop('func')  # get params only\n",
    "        f(**func_copy)         # run func\n",
    "\n",
    "        # test current corrupted netcdf\n",
    "        _test_five_outlier(temp_nc, out_func_name=str(func['func'].__name__))\n",
    "\n",
    "        # add some space to print out\n",
    "        print('- ' * 50)\n",
    "        print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Five: Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# run test five\n",
    "perform_test_five(in_nc=nc_file, temp_nc=temp_nc, nc_corruption_funcs=nc_corruptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Six: IVT Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _test_six_ivt_standardisation(in_nc, out_func_name='default'):\n",
    "    \"\"\"tests different set ups of the ivt standardisation input\"\"\"\n",
    "\n",
    "    # set default params for tool\n",
    "    inputs = {\n",
    "        'in_nc': in_nc,                        # input nc (i.e. temp nc)\n",
    "        'out_nc': '',                          # output nc (i.e. t1a nc)\n",
    "        'in_wet_months': '1;2;3',              # wet months \n",
    "        'in_dry_months': '9;10;11',            # dry months \n",
    "        'in_veg_idx': 'MAVI',                  # vege index name\n",
    "        'in_mst_idx': 'NDMI',                  # moisture index name       \n",
    "        'in_zscore_pvalue': None,              # zscore pvalue\n",
    "        'in_ivt_qupper': '',                   # upper quantile for standardisation\n",
    "        'in_ivt_qlower': '',                   # lower quantile for standardisation\n",
    "        'in_fmask_flags': 'Valid;Snow;Water',  # fmask flag values\n",
    "        'in_max_cloud': 10,                    # max cloud percentage\n",
    "        'in_interpolate': True,                # interpolate missing pixels\n",
    "        'in_add_result_to_map': True,          # add result to map\n",
    "    }\n",
    "    \n",
    "    # set output test string convention\n",
    "    out_nc = os.path.join(output_folder, 't6{}_{}.nc')   \n",
    "    \n",
    "    try:\n",
    "        print('\\n\\nRunning t6default (test six default). IVT lower, upper input is 0.05, 0.99.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('default', out_func_name), 'in_ivt_qlower': 0.05, 'in_ivt_qupper': 0.99})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e) \n",
    "\n",
    "    try:\n",
    "        print('\\n\\nRunning t6a (test six a). IVT lower, upper input is \"\", \"\".')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('a', out_func_name), 'in_ivt_qlower': '', 'in_ivt_qupper': ''})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t6b (test six b). IVT lower, upper input is None, ''.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('b', out_func_name), 'in_ivt_qlower': None, 'in_ivt_qupper': ''})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t6c (test six c). IVT lower, upper input is 0.0, 0.99.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('c', out_func_name), 'in_ivt_qlower': 0.0, 'in_ivt_qupper': 0.99})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)        \n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t6d (test six d). IVT lower, upper input is 0.05, 0.0.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('d', out_func_name), 'in_ivt_qlower': 0.05, 'in_ivt_qupper': 0.0})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)               \n",
    "\n",
    "    try:\n",
    "        print('\\n\\nRunning t6e (test six e). IVT lower, upper input is 0.25, 0.80.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('e', out_func_name), 'in_ivt_qlower': 0.25, 'in_ivt_qupper': 0.80})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)  \n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t6f (test six f). IVT lower, upper input is 0.4, 0.6.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('f', out_func_name), 'in_ivt_qlower': 0.4, 'in_ivt_qupper': 0.6})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)          \n",
    "\n",
    "    try:\n",
    "        print('\\n\\nRunning t6g (test six g). IVT lower, upper input is 0.6, 0.4.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('g', out_func_name), 'in_ivt_qlower': 0.6, 'in_ivt_qupper': 0.4})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)   \n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t6h (test six h). IVT lower, upper input is 0.2, 1.4.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('h', out_func_name), 'in_ivt_qlower': 0.2, 'in_ivt_qupper': 1.4})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)  \n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t6i (test six i). IVT lower, upper input is 0.5, 0.5.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('i', out_func_name), 'in_ivt_qlower': 0.5, 'in_ivt_qupper': 0.5})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)  \n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t6j (test six j). IVT lower, upper input is \"0.2\", \"0.8\".')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('j', out_func_name), 'in_ivt_qlower': \"0.2\", 'in_ivt_qupper': \"0.8\"})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)          \n",
    "    \n",
    "        \n",
    "def perform_test_six(in_nc, temp_nc, nc_corruption_funcs):\n",
    "    \"\"\"iterates corrupt ncs and runs through parameter tests\"\"\"\n",
    "\n",
    "    print('\\nRunning all data corruption functions through test six.')\n",
    "    print('- ' * 50)\n",
    "\n",
    "    # iterate different dataset corruptors and feed each into test func\n",
    "    for func in nc_corruption_funcs:\n",
    "\n",
    "        # get func name and notify\n",
    "        print('\\nRunning data corruption function: {}'.format(func['func'].__name__))\n",
    "        print('- ' * 50)\n",
    "        help(func['func'])\n",
    "\n",
    "        # make new instance of func dict so we dont destroy keys\n",
    "        func_copy = func.copy()\n",
    "\n",
    "        # get parameter names and entered value\n",
    "        params, inputs = [], []\n",
    "        for k, v in func_copy.items():\n",
    "            if k != 'func':\n",
    "                print('With parameter: {} of value: {}'.format(k, v))\n",
    "\n",
    "        # start the test\n",
    "        print('\\nBeginning test.')\n",
    "\n",
    "        # duplicate raw netcdf for testing\n",
    "        test_funcs.create_temp_nc(in_nc=in_nc, out_nc=temp_nc)\n",
    "\n",
    "        # prepare current function with associated parameters and run\n",
    "        f = func_copy['func']  # get func only\n",
    "        func_copy.pop('func')  # get params only\n",
    "        f(**func_copy)         # run func\n",
    "\n",
    "        # test current corrupted netcdf\n",
    "        _test_six_ivt_standardisation(temp_nc, out_func_name=str(func['func'].__name__))\n",
    "\n",
    "        # add some space to print out\n",
    "        print('- ' * 50)\n",
    "        print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Six: Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# run test six\n",
    "perform_test_six(in_nc=nc_file, temp_nc=temp_nc, nc_corruption_funcs=nc_corruptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Seven: Pixel Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _test_seven_pixel_flags(in_nc, out_func_name='default'):\n",
    "    \"\"\"tests different set ups of the pixel flag input\"\"\"\n",
    "\n",
    "    # set default params for tool\n",
    "    inputs = {\n",
    "        'in_nc': in_nc,                        # input nc (i.e. temp nc)\n",
    "        'out_nc': '',                          # output nc (i.e. t1a nc)\n",
    "        'in_wet_months': '1;2;3',              # wet months \n",
    "        'in_dry_months': '9;10;11',            # dry months \n",
    "        'in_veg_idx': 'MAVI',                  # vege index name\n",
    "        'in_mst_idx': 'NDMI',                  # moisture index name       \n",
    "        'in_zscore_pvalue': None,              # zscore pvalue\n",
    "        'in_ivt_qupper': 0.99,                 # upper quantile for standardisation\n",
    "        'in_ivt_qlower': 0.05,                 # lower quantile for standardisation\n",
    "        'in_fmask_flags': '',                  # fmask flag values\n",
    "        'in_max_cloud': 10,                    # max cloud percentage\n",
    "        'in_interpolate': True,                # interpolate missing pixels\n",
    "        'in_add_result_to_map': True,          # add result to map\n",
    "    }\n",
    "    \n",
    "    # set output test string convention\n",
    "    out_nc = os.path.join(output_folder, 't7{}_{}.nc')\n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t7default (test seven default). pixel flag input is \"Valid;Snow;Water\".')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('default', out_func_name), 'in_fmask_flags': 'Valid;Snow;Water'})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)        \n",
    "    \n",
    "    try:\n",
    "        print('\\n\\nRunning t7a (test seven a). pixel flag input is \"\".')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('a', out_func_name), 'in_fmask_flags': ''})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        print('\\n\\nRunning t7b (test seven b). pixel flag input is \"NoData;Valid;Cloud;Shadow;Snow;Water\".')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('b', out_func_name), 'in_fmask_flags': 'NoData;Valid;Cloud;Shadow;Snow;Water'})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t7c (test seven c). pixel flag input is \"Valid\".')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('c', out_func_name), 'in_fmask_flags': 'Valid'})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)   \n",
    "\n",
    "    try:\n",
    "        print('\\n\\nRunning t7d (test seven d). pixel flag input is \"Cloud;Shadow\".')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('d', out_func_name), 'in_fmask_flags': 'Cloud;Shadow'})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        print('\\n\\nRunning t7e (test seven e). pixel flag input is \"Water\".')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('e', out_func_name), 'in_fmask_flags': 'Water'})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t7f (test seven f). pixel flag input is \"Water;Water\".')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('f', out_func_name), 'in_fmask_flags': 'Water;Water'})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t7g (test seven g). pixel flag input is \"water\".')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('g', out_func_name), 'in_fmask_flags': 'water'})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "        \n",
    "def perform_test_seven(in_nc, temp_nc, nc_corruption_funcs):\n",
    "    \"\"\"iterates corrupt ncs and runs through parameter tests\"\"\"\n",
    "\n",
    "    print('\\nRunning all data corruption functions through test seven.')\n",
    "    print('- ' * 50)\n",
    "\n",
    "    # iterate different dataset corruptors and feed each into test func\n",
    "    for func in nc_corruption_funcs:\n",
    "\n",
    "        # get func name and notify\n",
    "        print('\\nRunning data corruption function: {}'.format(func['func'].__name__))\n",
    "        print('- ' * 50)\n",
    "        help(func['func'])\n",
    "\n",
    "        # make new instance of func dict so we dont destroy keys\n",
    "        func_copy = func.copy()\n",
    "\n",
    "        # get parameter names and entered value\n",
    "        params, inputs = [], []\n",
    "        for k, v in func_copy.items():\n",
    "            if k != 'func':\n",
    "                print('With parameter: {} of value: {}'.format(k, v))\n",
    "\n",
    "        # start the test\n",
    "        print('\\nBeginning test.')\n",
    "\n",
    "        # duplicate raw netcdf for testing\n",
    "        test_funcs.create_temp_nc(in_nc=in_nc, out_nc=temp_nc)\n",
    "\n",
    "        # prepare current function with associated parameters and run\n",
    "        f = func_copy['func']  # get func only\n",
    "        func_copy.pop('func')  # get params only\n",
    "        f(**func_copy)         # run func\n",
    "\n",
    "        # test current corrupted netcdf\n",
    "        _test_seven_pixel_flags(temp_nc, out_func_name=str(func['func'].__name__))\n",
    "\n",
    "        # add some space to print out\n",
    "        print('- ' * 50)\n",
    "        print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Seven: Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# run test seven\n",
    "perform_test_seven(in_nc=nc_file, temp_nc=temp_nc, nc_corruption_funcs=nc_corruptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Eight: Max Cloud Cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _test_eight_max_cloud_cover(in_nc, out_func_name='default'):\n",
    "    \"\"\"tests different set ups of the max cloud cover\"\"\"\n",
    "\n",
    "    # set default params for tool\n",
    "    inputs = {\n",
    "        'in_nc': in_nc,                        # input nc (i.e. temp nc)\n",
    "        'out_nc': '',                          # output nc (i.e. t1a nc)\n",
    "        'in_wet_months': '1;2;3',              # wet months \n",
    "        'in_dry_months': '9;10;11',            # dry months \n",
    "        'in_veg_idx': 'MAVI',                  # vege index name\n",
    "        'in_mst_idx': 'NDMI',                  # moisture index name       \n",
    "        'in_zscore_pvalue': None,              # zscore pvalue\n",
    "        'in_ivt_qupper': 0.99,                 # upper quantile for standardisation\n",
    "        'in_ivt_qlower': 0.05,                 # lower quantile for standardisation\n",
    "        'in_fmask_flags': 'Valid;Snow;Water',  # fmask flag values\n",
    "        'in_max_cloud': '',                    # max cloud percentage\n",
    "        'in_interpolate': True,                # interpolate missing pixels\n",
    "        'in_add_result_to_map': True,          # add result to map\n",
    "    }\n",
    "    \n",
    "    # set output test string convention\n",
    "    out_nc = os.path.join(output_folder, 't8{}_{}.nc')\n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t8default (test eight default). max cloud cover is 10.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('default', out_func_name), 'in_max_cloud': 10})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)        \n",
    "    \n",
    "    try:\n",
    "        print('\\n\\nRunning t8a (test eight a). max cloud cover is 0.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('a', out_func_name), 'in_max_cloud': 0})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)    \n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t8b (test eight b). max cloud cover is 100.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('b', out_func_name), 'in_max_cloud': 10})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)   \n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t8c (test eight c). max cloud cover is 50.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('c', out_func_name), 'in_max_cloud': 50})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)   \n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t8d (test eight d). max cloud cover is \"10\".')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('d', out_func_name), 'in_max_cloud': \"10\"})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t8e (test eight e). max cloud cover is 150.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('e', out_func_name), 'in_max_cloud': 150})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)   \n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t8f (test eight f). max cloud cover is \"\".')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('f', out_func_name), 'in_max_cloud': \"\"})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)     \n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t8g (test eight g). max cloud cover is None.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('g', out_func_name), 'in_max_cloud': None})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)  \n",
    "    \n",
    "def perform_test_eight(in_nc, temp_nc, nc_corruption_funcs):\n",
    "    \"\"\"iterates corrupt ncs and runs through parameter tests\"\"\"\n",
    "\n",
    "    print('\\nRunning all data corruption functions through test eight.')\n",
    "    print('- ' * 50)\n",
    "\n",
    "    # iterate different dataset corruptors and feed each into test func\n",
    "    for func in nc_corruption_funcs:\n",
    "\n",
    "        # get func name and notify\n",
    "        print('\\nRunning data corruption function: {}'.format(func['func'].__name__))\n",
    "        print('- ' * 50)\n",
    "        help(func['func'])\n",
    "\n",
    "        # make new instance of func dict so we dont destroy keys\n",
    "        func_copy = func.copy()\n",
    "\n",
    "        # get parameter names and entered value\n",
    "        params, inputs = [], []\n",
    "        for k, v in func_copy.items():\n",
    "            if k != 'func':\n",
    "                print('With parameter: {} of value: {}'.format(k, v))\n",
    "\n",
    "        # start the test\n",
    "        print('\\nBeginning test.')\n",
    "\n",
    "        # duplicate raw netcdf for testing\n",
    "        test_funcs.create_temp_nc(in_nc=in_nc, out_nc=temp_nc)\n",
    "\n",
    "        # prepare current function with associated parameters and run\n",
    "        f = func_copy['func']  # get func only\n",
    "        func_copy.pop('func')  # get params only\n",
    "        f(**func_copy)         # run func\n",
    "\n",
    "        # test current corrupted netcdf\n",
    "        _test_eight_max_cloud_cover(temp_nc, out_func_name=str(func['func'].__name__))\n",
    "\n",
    "        # add some space to print out\n",
    "        print('- ' * 50)\n",
    "        print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Eight: Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# run test eight\n",
    "perform_test_eight(in_nc=nc_file, temp_nc=temp_nc, nc_corruption_funcs=nc_corruptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Nine: Interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _test_nine_interpolate(in_nc, out_func_name='default'):\n",
    "    \"\"\"tests different set ups of the interpolate\"\"\"\n",
    "\n",
    "    # set default params for tool\n",
    "    inputs = {\n",
    "        'in_nc': in_nc,                        # input nc (i.e. temp nc)\n",
    "        'out_nc': '',                          # output nc (i.e. t1a nc)\n",
    "        'in_wet_months': '1;2;3',              # wet months \n",
    "        'in_dry_months': '9;10;11',            # dry months \n",
    "        'in_veg_idx': 'MAVI',                  # vege index name\n",
    "        'in_mst_idx': 'NDMI',                  # moisture index name       \n",
    "        'in_zscore_pvalue': None,              # zscore pvalue\n",
    "        'in_ivt_qupper': 0.99,                 # upper quantile for standardisation\n",
    "        'in_ivt_qlower': 0.05,                 # lower quantile for standardisation\n",
    "        'in_fmask_flags': 'Valid;Snow;Water',  # fmask flag values\n",
    "        'in_max_cloud': 10,                    # max cloud percentage\n",
    "        'in_interpolate': '',                # interpolate missing pixels\n",
    "        'in_add_result_to_map': True,          # add result to map\n",
    "    }\n",
    "    \n",
    "    # set output test string convention\n",
    "    out_nc = os.path.join(output_folder, 't9{}_{}.nc')\n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t9default (test nine default). interpolate is True.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('default', out_func_name), 'in_interpolate': True})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)        \n",
    "    \n",
    "    try:\n",
    "        print('\\n\\nRunning t9a (test nine a). interpolate is False.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('a', out_func_name), 'in_interpolate': False})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)    \n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t9b (test nine b). interpolate is None.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('b', out_func_name), 'in_interpolate': None})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)    \n",
    "    \n",
    "def perform_test_nine(in_nc, temp_nc, nc_corruption_funcs):\n",
    "    \"\"\"iterates corrupt ncs and runs through parameter tests\"\"\"\n",
    "\n",
    "    print('\\nRunning all data corruption functions through test nine.')\n",
    "    print('- ' * 50)\n",
    "\n",
    "    # iterate different dataset corruptors and feed each into test func\n",
    "    for func in nc_corruption_funcs:\n",
    "\n",
    "        # get func name and notify\n",
    "        print('\\nRunning data corruption function: {}'.format(func['func'].__name__))\n",
    "        print('- ' * 50)\n",
    "        help(func['func'])\n",
    "\n",
    "        # make new instance of func dict so we dont destroy keys\n",
    "        func_copy = func.copy()\n",
    "\n",
    "        # get parameter names and entered value\n",
    "        params, inputs = [], []\n",
    "        for k, v in func_copy.items():\n",
    "            if k != 'func':\n",
    "                print('With parameter: {} of value: {}'.format(k, v))\n",
    "\n",
    "        # start the test\n",
    "        print('\\nBeginning test.')\n",
    "\n",
    "        # duplicate raw netcdf for testing\n",
    "        test_funcs.create_temp_nc(in_nc=in_nc, out_nc=temp_nc)\n",
    "\n",
    "        # prepare current function with associated parameters and run\n",
    "        f = func_copy['func']  # get func only\n",
    "        func_copy.pop('func')  # get params only\n",
    "        f(**func_copy)         # run func\n",
    "\n",
    "        # test current corrupted netcdf\n",
    "        _test_nine_interpolate(temp_nc, out_func_name=str(func['func'].__name__))\n",
    "\n",
    "        # add some space to print out\n",
    "        print('- ' * 50)\n",
    "        print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Nine: Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# run test nine\n",
    "perform_test_nine(in_nc=nc_file, temp_nc=temp_nc, nc_corruption_funcs=nc_corruptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Ten: Add To Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _test_ten_add_to_map(in_nc, out_func_name='default'):\n",
    "    \"\"\"tests different set ups of the add output to map\"\"\"\n",
    "\n",
    "    # set default params for tool\n",
    "    inputs = {\n",
    "        'in_nc': in_nc,                        # input nc (i.e. temp nc)\n",
    "        'out_nc': '',                          # output nc (i.e. t1a nc)\n",
    "        'in_wet_months': '1;2;3',              # wet months \n",
    "        'in_dry_months': '9;10;11',            # dry months \n",
    "        'in_veg_idx': 'MAVI',                  # vege index name\n",
    "        'in_mst_idx': 'NDMI',                  # moisture index name       \n",
    "        'in_zscore_pvalue': None,              # zscore pvalue\n",
    "        'in_ivt_qupper': 0.99,                 # upper quantile for standardisation\n",
    "        'in_ivt_qlower': 0.05,                 # lower quantile for standardisation\n",
    "        'in_fmask_flags': 'Valid;Snow;Water',  # fmask flag values\n",
    "        'in_max_cloud': 10,                    # max cloud percentage\n",
    "        'in_interpolate': True,                # interpolate missing pixels\n",
    "        'in_add_result_to_map': '',          # add result to map\n",
    "    }\n",
    "    \n",
    "    # set output test string convention\n",
    "    out_nc = os.path.join(output_folder, 't10{}_{}.nc')\n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t10default (test ten default). in_add_result_to_map is True.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('default', out_func_name), 'in_add_result_to_map': True})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)        \n",
    "    \n",
    "    try:\n",
    "        print('\\n\\nRunning t10a (test ten a). add result to_map is False.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('a', out_func_name), 'in_add_result_to_map': False})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)    \n",
    "        \n",
    "    try:\n",
    "        print('\\n\\nRunning t10b (test ten b). add result to_map is None.')\n",
    "        print('- ' * 50)\n",
    "        inputs.update({'out_nc': out_nc.format('b', out_func_name), 'in_add_result_to_map': None})\n",
    "        arcpy.GDVSpectra_Likelihood_toolbox(*list(inputs.values()))\n",
    "    except Exception as e:\n",
    "        print(e)       \n",
    "    \n",
    "def perform_test_ten(in_nc, temp_nc, nc_corruption_funcs):\n",
    "    \"\"\"iterates corrupt ncs and runs through parameter tests\"\"\"\n",
    "\n",
    "    print('\\nRunning all data corruption functions through test ten.')\n",
    "    print('- ' * 50)\n",
    "\n",
    "    # iterate different dataset corruptors and feed each into test func\n",
    "    for func in nc_corruption_funcs:\n",
    "\n",
    "        # get func name and notify\n",
    "        print('\\nRunning data corruption function: {}'.format(func['func'].__name__))\n",
    "        print('- ' * 50)\n",
    "        help(func['func'])\n",
    "\n",
    "        # make new instance of func dict so we dont destroy keys\n",
    "        func_copy = func.copy()\n",
    "\n",
    "        # get parameter names and entered value\n",
    "        params, inputs = [], []\n",
    "        for k, v in func_copy.items():\n",
    "            if k != 'func':\n",
    "                print('With parameter: {} of value: {}'.format(k, v))\n",
    "\n",
    "        # start the test\n",
    "        print('\\nBeginning test.')\n",
    "\n",
    "        # duplicate raw netcdf for testing\n",
    "        test_funcs.create_temp_nc(in_nc=in_nc, out_nc=temp_nc)\n",
    "\n",
    "        # prepare current function with associated parameters and run\n",
    "        f = func_copy['func']  # get func only\n",
    "        func_copy.pop('func')  # get params only\n",
    "        f(**func_copy)         # run func\n",
    "\n",
    "        # test current corrupted netcdf\n",
    "        _test_ten_add_to_map(temp_nc, out_func_name=str(func['func'].__name__))\n",
    "\n",
    "        # add some space to print out\n",
    "        print('- ' * 50)\n",
    "        print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Ten: Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# run test ten\n",
    "perform_test_ten(in_nc=nc_file, temp_nc=temp_nc, nc_corruption_funcs=nc_corruptors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
