{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phenolopy Metrics module tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set globals paths\n",
    "FOLDER_MODULES = r'C:\\Users\\Lewis\\Documents\\GitHub\\tenement-tools\\modules'  \n",
    "FOLDER_SHARED = r'C:\\Users\\Lewis\\Documents\\GitHub\\tenement-tools\\shared'\n",
    "TEST_MODULE = r'C:\\Users\\Lewis\\Documents\\GitHub\\tenement-tools\\tests\\code'\n",
    "GRP_LYR_FILE = r'C:\\Users\\Lewis\\Documents\\GitHub\\tenement-tools\\arc\\lyr\\group_template.lyrx'    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from IPython.utils import io\n",
    "\n",
    "# import testing functions\n",
    "sys.path.append(TEST_MODULE)\n",
    "import test_funcs\n",
    "\n",
    "# import full arcpy toolbox\n",
    "arcpy.ImportToolbox(r\"C:\\Users\\Lewis\\Documents\\GitHub\\tenement-tools\\arc\\toolbox\\tenement-tools-toolbox.pyt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if scripts change, reload\n",
    "from importlib import reload\n",
    "reload(test_funcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set data files and locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup general io. nc ins and outs exist in these folders\n",
    "input_folder = r'E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_likelihood\\inputs'\n",
    "output_folder = r'E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_likelihood\\outputs'\n",
    "\n",
    "# temp nc file for use when breaking ncs\n",
    "temp_nc = os.path.join(input_folder, 'temp_nc.nc')  \n",
    "\n",
    "# setup landsat cubes paths\n",
    "ls_cubes = [\n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_likelihood\\inputs\\yandi_1_ls_90_20_raw_odc.nc\",\n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_likelihood\\inputs\\yandi_2_ls_90_20_raw_odc.nc\", \n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_likelihood\\inputs\\yandi_3_ls_90_20_raw_odc.nc\",\n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_likelihood\\inputs\\yandi_4_ls_90_20_raw_odc.nc\", \n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_likelihood\\inputs\\dwer_1_ls_90_20_raw_odc.nc\",  \n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_likelihood\\inputs\\roy_1_ls_10_20_raw_odc.nc\",\n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_likelihood\\inputs\\roy_3_ls_10_20_raw_odc.nc\",\n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_likelihood\\inputs\\tute_1_ls_10_20_raw_odc.nc\",  \n",
    "]\n",
    "\n",
    "# setup sentinel2 cubes paths\n",
    "s2_cubes = [\n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_likelihood\\inputs\\yandi_2_s2_16_20_raw_odc.nc\", \n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_likelihood\\inputs\\yandi_3_s2_16_20_raw_odc.nc\",\n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_likelihood\\inputs\\yandi_4_s2_16_20_raw_odc.nc\",\n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_likelihood\\inputs\\dwer_1_s2_16_20_raw_odc.nc\",  \n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_likelihood\\inputs\\roy_2_s2_16_21_raw_odc.nc\",   \n",
    "    r\"E:\\Curtin\\GDVII - General\\Work Package 2\\test_data\\gdvspectra_likelihood\\inputs\\tute_1_s2_18_20_raw_odc.nc\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set specific raw netcdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set specific dataset\n",
    "nc_file = ls_cubes[1]\n",
    "#nc_file = s2_cubes[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up function to iterate corruptor and tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_corruptors_through_tests(in_nc, nc_coors, tests, verbose):\n",
    "    \"\"\"\n",
    "    this func takes a path to nc file or raw sat imagery, a list of nc\n",
    "    corruptor funcs and params, a list of test funcs and params. Each \n",
    "    nc corruptor func is iterated through, and for each corrupted nc, \n",
    "    each test in tests is applied to corrupted nc. verbose sets how\n",
    "    much information is printed.\n",
    "    \"\"\"\n",
    "    \n",
    "    for nc_corr in nc_corrs:\n",
    "        corr_name = nc_corr[0].__name__                   # name of current corruptor func\n",
    "        corr_func, corr_params = nc_corr[0], nc_corr[1]   # pointer to corruptor func and dict of params\n",
    "        \n",
    "        # notify\n",
    "        print('Corrupting NetCDF via: {}.\\n'.format(corr_name) + '- ' * 30)\n",
    "\n",
    "        # create temp nc and corrupt it with current corruptor\n",
    "        if not verbose:\n",
    "            with io.capture_output() as cap:\n",
    "                test_funcs.create_temp_nc(in_nc=in_nc, out_nc=temp_nc)\n",
    "        else:\n",
    "            test_funcs.create_temp_nc(in_nc=in_nc, out_nc=temp_nc)\n",
    "\n",
    "        # run current corruptor function\n",
    "        try:\n",
    "            corr_func(**corr_params)\n",
    "        except Exception as e:    \n",
    "            print(e)\n",
    "            print('Corruptor did not have enough data to work with. Skipping.\\n')\n",
    "\n",
    "        # iter each test func and apply to current corrupt nc\n",
    "        for test in tests:\n",
    "            test_nc_name = corr_name + '_' + test[0]    # name of current test nc\n",
    "            test_func, test_params = test[1], test[2]   # pointer to test func and dict of params\n",
    "            test_msg = test[3]\n",
    "            \n",
    "            # notify of test message\n",
    "            print(test_msg)\n",
    "\n",
    "            # create output nc file path and name and update params for in/out paths\n",
    "            out_nc_file = os.path.join(output_folder, test_nc_name)\n",
    "            \n",
    "            # remove output nc if exists\n",
    "            if os.path.exists(out_nc_file):\n",
    "                os.remove(out_nc_file)\n",
    "            \n",
    "            # update params\n",
    "            test_params.update({'in_nc_file': temp_nc, 'out_likelihood_nc_file': out_nc_file})\n",
    "\n",
    "            # perform current test\n",
    "            try:\n",
    "                # notify\n",
    "                print('Performing test: {}.'.format(test_nc_name))\n",
    "\n",
    "                # perform test, provide prints if requested\n",
    "                if not verbose:\n",
    "                    with io.capture_output() as cap:\n",
    "                        test_func(**test_params)\n",
    "                else:\n",
    "                    test_func(**test_params)\n",
    "                    print('\\n')\n",
    "\n",
    "            except Exception as e:    \n",
    "                print(e)\n",
    "\n",
    "        # notify\n",
    "        print('All tests applied to corruptor NetCDF.\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up netcdf corruptor functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are numerous netcdf corruptors. feed a raw nc in, break it, output as temp nc\n",
    "# comment out any that are irrelevant\n",
    "# each of these uncommented will be fed through the tests below\n",
    "def build_nc_corruptors(temp_nc):\n",
    "    \"\"\"\n",
    "    each one of these is a unique netcdf corruptor functions and \n",
    "    associated parameters. \n",
    "    \"\"\"\n",
    "    \n",
    "    # set up list\n",
    "    cs = []\n",
    "\n",
    "    # func: raw default dataset, no changes\n",
    "    cs.append([test_funcs.nc_default, {'in_nc': temp_nc}])\n",
    "\n",
    "    # func: remove x, y, time, spatial_ref coords\n",
    "    #cs.append([test_funcs.remove_coord, {'in_nc': temp_nc, 'coord': 'x'}])\n",
    "    #cs.append([test_funcs.remove_coord, {'in_nc': temp_nc, 'coord': 'y'}])\n",
    "    #cs.append([test_funcs.remove_coord, {'in_nc': temp_nc, 'coord': 'time'}])\n",
    "    #cs.append([test_funcs.remove_coord, {'in_nc': temp_nc, 'coord': 'spatial_ref'}])\n",
    "\n",
    "    #func: remove red and oa_fmask band vars\n",
    "    #cs.append([test_funcs.remove_var, {'in_nc': temp_nc, 'var': 'nbart_red'}])\n",
    "    #cs.append([test_funcs.remove_var, {'in_nc': temp_nc, 'var': 'oa_fmask'}])\n",
    "\n",
    "    #func: limit number of years in various combos\n",
    "    #cs.append([test_funcs.limit_years, {'in_nc': temp_nc, 's_year': 1990, 'e_year': 1990}])\n",
    "    #cs.append([test_funcs.limit_years, {'in_nc': temp_nc, 's_year': 2010, 'e_year': 2010}])\n",
    "    #cs.append([test_funcs.limit_years, {'in_nc': temp_nc, 's_year': 2012, 'e_year': 2012}])\n",
    "    #cs.append([test_funcs.limit_years, {'in_nc': temp_nc, 's_year': 1991, 'e_year': 1992}])\n",
    "    #cs.append([test_funcs.limit_years, {'in_nc': temp_nc, 's_year': 2005, 'e_year': 2006}])\n",
    "    #cs.append([test_funcs.limit_years, {'in_nc': temp_nc, 's_year': 2019, 'e_year': 2020}])\n",
    "    #cs.append([test_funcs.limit_years, {'in_nc': temp_nc, 's_year': 1993, 'e_year': 1995}])\n",
    "    #cs.append([test_funcs.limit_years, {'in_nc': temp_nc, 's_year': 2010, 'e_year': 2012}])\n",
    "    #cs.append([test_funcs.limit_years, {'in_nc': temp_nc, 's_year': 2011, 'e_year': 2013}])\n",
    "\n",
    "    #func: set all vars to nan\n",
    "    #cs.append([test_funcs.set_nc_vars_all_nan, {'in_nc': temp_nc}])\n",
    "\n",
    "    #func: set all vars to zero\n",
    "    #cs.append([test_funcs.set_nc_vars_all_zero, {'in_nc': temp_nc}])\n",
    "\n",
    "    #func: set all vars for 10 rand times to all nan\n",
    "    #cs.append([test_funcs.set_nc_vars_random_all_nan, {'in_nc': temp_nc, 'num': 10}])\n",
    "\n",
    "    #func: strip all attrs from nc    \n",
    "    #cs.append([test_funcs.strip_nc_attributes, {'in_nc': temp_nc}])\n",
    "\n",
    "    #func: set vars in first and last time index to all nan\n",
    "    #cs.append([test_funcs.set_end_times_to_all_nan, {'in_nc': temp_nc}])\n",
    "\n",
    "    #func: reduce whole nc to one random time slice\n",
    "    #cs.append([test_funcs.reduce_to_one_scene, {'in_nc': temp_nc}])\n",
    "\n",
    "    #func: set wet months all nan, all years, for specific months\n",
    "    #cs.append([test_funcs.set_all_specific_season_nan, {'in_nc': temp_nc, 'months': [1]}])\n",
    "    #cs.append([test_funcs.set_all_specific_season_nan, {'in_nc': temp_nc, 'months': [1, 2, 3]}])\n",
    "    #cs.append([test_funcs.set_all_specific_season_nan, {'in_nc': temp_nc, 'months': [7, 8, 9, 10, 11, 12]}])\n",
    "\n",
    "    #func: set wet months all nan, specific years, for specific months\n",
    "    #cs.append([test_funcs.set_specific_years_season_nan, {'in_nc': temp_nc, 'years': [1990], 'months': [1, 2, 3]}])\n",
    "    #cs.append([test_funcs.set_specific_years_season_nan, {'in_nc': temp_nc, 'years': [2005], 'months': [1, 2, 3]}])\n",
    "    #cs.append([test_funcs.set_specific_years_season_nan, {'in_nc': temp_nc, 'years': [2006, 2007], 'months': [1, 2, 3]}])\n",
    "\n",
    "    #func: drop wet months, all years, for specific months\n",
    "    #cs.append([test_funcs.remove_all_specific_season_nan, {'in_nc': temp_nc, 'months': [1]}])\n",
    "    #cs.append([test_funcs.remove_all_specific_season_nan, {'in_nc': temp_nc, 'months': [1, 2, 3]}])\n",
    "    #cs.append([test_funcs.remove_all_specific_season_nan, {'in_nc': temp_nc, 'months': [7, 8, 9, 10, 11, 12]}])\n",
    "\n",
    "    #func: drop wet months, specific years, for specific months (note: seperate tests)\n",
    "    #cs.append([test_funcs.remove_specific_years_season_nan, {'in_nc': temp_nc, 'years': [1990], 'months': [1, 2, 3]}])\n",
    "    #cs.append([test_funcs.remove_specific_years_season_nan, {'in_nc': temp_nc, 'years': [2009], 'months': [1, 2, 3]}])\n",
    "    #cs.append([test_funcs.remove_specific_years_season_nan, {'in_nc': temp_nc, 'years': [2007, 2008], 'months': [1, 2, 3]}])\n",
    "\n",
    "    #func: remove crs attribute\n",
    "    #cs.append([test_funcs.remove_crs_attr, {'in_nc': temp_nc}])\n",
    "\n",
    "    #func: invalidate crs attribute\n",
    "    #cs.append([test_funcs.invalidate_crs_attr, {'in_nc': temp_nc, 'crs_text': 'EPSG:4326'}])\n",
    "    #cs.append([test_funcs.invalidate_crs_attr, {'in_nc': temp_nc, 'crs_text': ''}])\n",
    "\n",
    "    #func: remove nodatavals attribute\n",
    "    #cs.append([test_funcs.remove_nodatavals_attr, {'in_nc': temp_nc}])\n",
    "    \n",
    "    return cs\n",
    "\n",
    "nc_corruptors = build_nc_corruptors(temp_nc=temp_nc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test One: Wet Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_test_one_funcs(in_nc, temp_nc):\n",
    "    \"\"\"sets up test one functions\"\"\"\n",
    "    \n",
    "    # set default params for tool\n",
    "    inputs = {\n",
    "        'in_nc_file': '',                         # input nc (i.e. temp nc)\n",
    "        'out_likelihood_nc_file': '',             # output nc (i.e. t1a nc)\n",
    "        'in_wet_months': '',                      # wet months \n",
    "        'in_dry_months': '9;10;11',               # dry months \n",
    "        'in_veg_idx': 'MAVI',                     # vege index name\n",
    "        'in_mst_idx': 'NDMI',                     # moisture index name       \n",
    "        'in_aggregate': False,                    # aggregate output\n",
    "        'in_zscore_pvalue': None,                 # zscore pvalue\n",
    "        'in_stand_qupper': 0.99,                  # upper quantile for standardisation\n",
    "        'in_stand_qlower': 0.05,                  # lower quantile for standardisation\n",
    "        'in_fmask_flags': 'Valid;Snow;Water',     # fmask flag values\n",
    "        'in_max_cloud': 10,                       # max cloud percentage\n",
    "        'in_interpolate': True,                   # interpolate missing pixels\n",
    "        'in_add_result_to_map': True,             # add result to map\n",
    "    }\n",
    "    \n",
    "    # set up list\n",
    "    ts = []\n",
    "\n",
    "    # func: default wet months 1, 2, 3\n",
    "    msg = 'Running Test One: default wet months.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_wet_months': '1;2;3'})\n",
    "    ts.append(['t_1_def.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "    \n",
    "    # func: no wet months\n",
    "    msg = 'Running Test One: \"\" wet months.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_wet_months': ''})\n",
    "    ts.append(['t_1_a.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "    \n",
    "    # func: one random month\n",
    "    rand = str(random.randint(1, 5))\n",
    "    msg = 'Running Test One: single random wet month ({}).'.format(rand)\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_wet_months': rand})\n",
    "    ts.append(['t_1_b.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])    \n",
    "\n",
    "    # func: three months (2, 3, 4)\n",
    "    msg = 'Running Test One: 2;3;4 as wet months.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_wet_months': '2;3;4'})\n",
    "    ts.append(['t_1_c.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])    \n",
    "    \n",
    "    # func: all dry season months in wet season\n",
    "    msg = 'Running Test One: dry months 5;6;7;8;9 as wet months.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_wet_months': '5;6;7;8;9'})\n",
    "    ts.append(['t_1_d.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg]) \n",
    "\n",
    "    # func: wet season includes a month in dec\n",
    "    msg = 'Running Test One: wet months contain dec (12).'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_wet_months': '12;1;2'})\n",
    "    ts.append(['t_1_e.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg]) \n",
    "    \n",
    "    # func: wet season includes only one month in dec\n",
    "    msg = 'Running Test One: single wet month in dec (12).'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_wet_months': '12'})\n",
    "    ts.append(['t_1_f.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])     \n",
    "        \n",
    "    return ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test One: Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build lsit of nc corruptors and tests to iterate\n",
    "nc_corrs = build_nc_corruptors(temp_nc)\n",
    "tests = build_test_one_funcs(in_nc=nc_file, temp_nc=temp_nc)\n",
    "\n",
    "# run!\n",
    "run_corruptors_through_tests(in_nc=nc_file, nc_coors=nc_corrs, tests=tests, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "scrolled": false
   },
   "source": [
    "### Test Two: Dry Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_test_two_funcs(in_nc, temp_nc):\n",
    "    \"\"\"sets up test two functions\"\"\"\n",
    "    \n",
    "    # set default params for tool\n",
    "    inputs = {\n",
    "        'in_nc_file': '',                         # input nc (i.e. temp nc)\n",
    "        'out_likelihood_nc_file': '',             # output nc (i.e. t1a nc)\n",
    "        'in_wet_months': '1;2;3',                      # wet months \n",
    "        'in_dry_months': '',                      # dry months \n",
    "        'in_veg_idx': 'MAVI',                     # vege index name\n",
    "        'in_mst_idx': 'NDMI',                     # moisture index name       \n",
    "        'in_aggregate': False,                    # aggregate output\n",
    "        'in_zscore_pvalue': None,                 # zscore pvalue\n",
    "        'in_stand_qupper': 0.99,                  # upper quantile for standardisation\n",
    "        'in_stand_qlower': 0.05,                  # lower quantile for standardisation\n",
    "        'in_fmask_flags': 'Valid;Snow;Water',     # fmask flag values\n",
    "        'in_max_cloud': 10,                       # max cloud percentage\n",
    "        'in_interpolate': True,                   # interpolate missing pixels\n",
    "        'in_add_result_to_map': True,             # add result to map\n",
    "    }\n",
    "    \n",
    "    # set up list\n",
    "    ts = []\n",
    "            \n",
    "    # func: default dry months 9, 10, 11\n",
    "    msg = 'Running Test Two: default dry months.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_dry_months': '9;10;11'})\n",
    "    ts.append(['t_2_def.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "    \n",
    "    # func: no dry months\n",
    "    msg = 'Running Test Two: \"\" dry months.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_dry_months': ''})\n",
    "    ts.append(['t_2_a.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "    \n",
    "    # func: one random month\n",
    "    rand = str(random.randint(7, 12))\n",
    "    msg = 'Running Test Two: single random dry month ({}).'.format(rand)\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_dry_months': rand})\n",
    "    ts.append(['t_2_b.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])    \n",
    "\n",
    "    # func: three months (8, 9, 10)\n",
    "    msg = 'Running Test Two: 10;11;12 as dry months.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_dry_months': '10;11;12'})\n",
    "    ts.append(['t_2_c.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])    \n",
    "    \n",
    "    # func: all dry season months in wet season\n",
    "    msg = 'Running Test Two: dry months 1;2;3;4;5 as dry months.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_dry_months': '1;2;3;4;5'})\n",
    "    ts.append(['t_2_d.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg]) \n",
    "\n",
    "    # func: wet season includes a month in dec\n",
    "    msg = 'Running Test Two: dry months contain one month in jan (1).'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_dry_months': '11;12;1'})\n",
    "    ts.append(['t_2_e.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg]) \n",
    "        \n",
    "    return ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Two: Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build lsit of nc corruptors and tests to iterate\n",
    "nc_corrs = build_nc_corruptors(temp_nc)\n",
    "tests = build_test_two_funcs(in_nc=nc_file, temp_nc=temp_nc)\n",
    "\n",
    "# run!\n",
    "run_corruptors_through_tests(in_nc=nc_file, nc_coors=nc_corrs, tests=tests, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Three: Vegetation Idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_test_three_funcs(in_nc, temp_nc):\n",
    "    \"\"\"sets up test three functions\"\"\"\n",
    "    \n",
    "    # set default params for tool\n",
    "    inputs = {\n",
    "        'in_nc_file': '',                         # input nc (i.e. temp nc)\n",
    "        'out_likelihood_nc_file': '',             # output nc (i.e. t1a nc)\n",
    "        'in_wet_months': '1;2;3',                 # wet months \n",
    "        'in_dry_months': '9;10;11',               # dry months \n",
    "        'in_veg_idx': '',                     # vege index name\n",
    "        'in_mst_idx': 'NDMI',                     # moisture index name       \n",
    "        'in_aggregate': False,                    # aggregate output\n",
    "        'in_zscore_pvalue': None,                 # zscore pvalue\n",
    "        'in_stand_qupper': 0.99,                  # upper quantile for standardisation\n",
    "        'in_stand_qlower': 0.05,                  # lower quantile for standardisation\n",
    "        'in_fmask_flags': 'Valid;Snow;Water',     # fmask flag values\n",
    "        'in_max_cloud': 10,                       # max cloud percentage\n",
    "        'in_interpolate': True,                   # interpolate missing pixels\n",
    "        'in_add_result_to_map': True,             # add result to map\n",
    "    }\n",
    "    \n",
    "    # set up list\n",
    "    ts = []\n",
    "            \n",
    "    # func: default veg idx mavi\n",
    "    msg = 'Running Test Three: default veg idx (mavi).'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_veg_idx': 'MAVI'})\n",
    "    ts.append(['t_3_def.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "    \n",
    "    # func: veg idx is ''\n",
    "    msg = 'Running Test Three: veg idx is \"\".'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_veg_idx': ''})\n",
    "    ts.append(['t_3_a.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "    \n",
    "    # func: veg idx is ndvi\n",
    "    msg = 'Running Test Three: veg idx is NDVI.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_veg_idx': 'NDVI'})\n",
    "    ts.append(['t_3_b.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "    \n",
    "    # func: veg idx is evi\n",
    "    msg = 'Running Test Three: veg idx is EVI.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_veg_idx': 'EVI'})\n",
    "    ts.append(['t_3_c.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "    \n",
    "    # func: veg idx is savi\n",
    "    msg = 'Running Test Three: veg idx is SAVI.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_veg_idx': 'SAVI'})\n",
    "    ts.append(['t_3_d.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "    \n",
    "    # func: veg idx is msavi\n",
    "    msg = 'Running Test Three: veg idx is MSAVI.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_veg_idx': 'MSAVI'})\n",
    "    ts.append(['t_3_e.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "\n",
    "    # func: veg idx is slavi\n",
    "    msg = 'Running Test Three: veg idx is SLAVI.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_veg_idx': 'SLAVI'})\n",
    "    ts.append(['t_3_f.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "\n",
    "    # func: veg idx is mavi\n",
    "    msg = 'Running Test Three: veg idx is MAVI.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_veg_idx': 'MAVI'})\n",
    "    ts.append(['t_3_g.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "    \n",
    "    # func: veg idx is kndvi\n",
    "    msg = 'Running Test Three: veg idx is kNDVI.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_veg_idx': 'kNDVI'})\n",
    "    ts.append(['t_3_h.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])    \n",
    "\n",
    "    # func: veg idx is tcg\n",
    "    msg = 'Running Test Three: veg idx is TCG.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_veg_idx': 'TCG'})\n",
    "    ts.append(['t_3_i.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])    \n",
    "    \n",
    "    # func: veg idx is nrva (non-existant)\n",
    "    msg = 'Running Test Three: veg idx is NRVA (non-existant).'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_veg_idx': 'NRVA'})\n",
    "    ts.append(['t_3_j.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])    \n",
    "        \n",
    "    return ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Three: Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build lsit of nc corruptors and tests to iterate\n",
    "nc_corrs = build_nc_corruptors(temp_nc)\n",
    "tests = build_test_three_funcs(in_nc=nc_file, temp_nc=temp_nc)\n",
    "\n",
    "# run!\n",
    "run_corruptors_through_tests(in_nc=nc_file, nc_coors=nc_corrs, tests=tests, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Four: Moisture Idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_test_four_funcs(in_nc, temp_nc):\n",
    "    \"\"\"sets up test four functions\"\"\"\n",
    "    \n",
    "    # set default params for tool\n",
    "    inputs = {\n",
    "        'in_nc_file': '',                         # input nc (i.e. temp nc)\n",
    "        'out_likelihood_nc_file': '',             # output nc (i.e. t1a nc)\n",
    "        'in_wet_months': '1;2;3',                 # wet months \n",
    "        'in_dry_months': '9;10;11',               # dry months \n",
    "        'in_veg_idx': 'MAVI',                     # vege index name\n",
    "        'in_mst_idx': '',                         # moisture index name       \n",
    "        'in_aggregate': False,                    # aggregate output\n",
    "        'in_zscore_pvalue': None,                 # zscore pvalue\n",
    "        'in_stand_qupper': 0.99,                  # upper quantile for standardisation\n",
    "        'in_stand_qlower': 0.05,                  # lower quantile for standardisation\n",
    "        'in_fmask_flags': 'Valid;Snow;Water',     # fmask flag values\n",
    "        'in_max_cloud': 10,                       # max cloud percentage\n",
    "        'in_interpolate': True,                   # interpolate missing pixels\n",
    "        'in_add_result_to_map': True,             # add result to map\n",
    "    }\n",
    "    \n",
    "    # set up list\n",
    "    ts = []\n",
    "            \n",
    "    # func: default mst idx ndmi\n",
    "    msg = 'Running Test Four: default mst idx (ndmi).'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_mst_idx': 'NDMI'})\n",
    "    ts.append(['t_4_def.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "    \n",
    "    # func: mst idx is ''\n",
    "    msg = 'Running Test Four: mst idx is \"\".'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_mst_idx': ''})\n",
    "    ts.append(['t_4_a.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "    \n",
    "    # func: mst idx is gvmi\n",
    "    msg = 'Running Test Four: mst idx is gvmi.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_mst_idx': 'GVMI'})\n",
    "    ts.append(['t_4_b.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "    \n",
    "    # func: mst idx is rasd\n",
    "    msg = 'Running Test Four: mst idx is rasd.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_mst_idx': 'RASD'})\n",
    "    ts.append(['t_4_c.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])   \n",
    "            \n",
    "    return ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Four: Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build lsit of nc corruptors and tests to iterate\n",
    "nc_corrs = build_nc_corruptors(temp_nc)\n",
    "tests = build_test_four_funcs(in_nc=nc_file, temp_nc=temp_nc)\n",
    "\n",
    "# run!\n",
    "run_corruptors_through_tests(in_nc=nc_file, nc_coors=nc_corrs, tests=tests, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Five: Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_test_five_funcs(in_nc, temp_nc):\n",
    "    \"\"\"sets up test five functions\"\"\"\n",
    "    \n",
    "    # set default params for tool\n",
    "    inputs = {\n",
    "        'in_nc_file': '',                         # input nc (i.e. temp nc)\n",
    "        'out_likelihood_nc_file': '',             # output nc (i.e. t1a nc)\n",
    "        'in_wet_months': '1;2;3',                 # wet months \n",
    "        'in_dry_months': '9;10;11',               # dry months \n",
    "        'in_veg_idx': 'MAVI',                     # vege index name\n",
    "        'in_mst_idx': '',                         # moisture index name       \n",
    "        'in_aggregate': True,                     # aggregate output\n",
    "        'in_zscore_pvalue': None,                 # zscore pvalue\n",
    "        'in_stand_qupper': 0.99,                  # upper quantile for standardisation\n",
    "        'in_stand_qlower': 0.05,                  # lower quantile for standardisation\n",
    "        'in_fmask_flags': 'Valid;Snow;Water',     # fmask flag values\n",
    "        'in_max_cloud': 10,                       # max cloud percentage\n",
    "        'in_interpolate': True,                   # interpolate missing pixels\n",
    "        'in_add_result_to_map': True,             # add result to map\n",
    "    }\n",
    "    \n",
    "    # set up list\n",
    "    ts = []\n",
    "            \n",
    "    # func: default aggregate (True)\n",
    "    msg = 'Running Test Five: default aggregate (True).'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_aggregate': True})\n",
    "    ts.append(['t_5_def.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "    \n",
    "    # func: aggregate is False\n",
    "    msg = 'Running Test Five: aggregate is False'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_aggregate': False})\n",
    "    ts.append(['t_5_a.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])    \n",
    "\n",
    "    # func: aggregate is None\n",
    "    msg = 'Running Test Five: aggregate is None'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_aggregate': None})\n",
    "    ts.append(['t_5_b.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])     \n",
    "            \n",
    "    return ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Five: Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build lsit of nc corruptors and tests to iterate\n",
    "nc_corrs = build_nc_corruptors(temp_nc)\n",
    "tests = build_test_five_funcs(in_nc=nc_file, temp_nc=temp_nc)\n",
    "\n",
    "# run!\n",
    "run_corruptors_through_tests(in_nc=nc_file, nc_coors=nc_corrs, tests=tests, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Six: Outlier Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_test_six_funcs(in_nc, temp_nc):\n",
    "    \"\"\"sets up test six functions\"\"\"\n",
    "    \n",
    "    # set default params for tool\n",
    "    inputs = {\n",
    "        'in_nc_file': '',                         # input nc (i.e. temp nc)\n",
    "        'out_likelihood_nc_file': '',             # output nc (i.e. t1a nc)\n",
    "        'in_wet_months': '1;2;3',                 # wet months \n",
    "        'in_dry_months': '9;10;11',               # dry months \n",
    "        'in_veg_idx': 'MAVI',                     # vege index name\n",
    "        'in_mst_idx': 'NDMI',                     # moisture index name       \n",
    "        'in_aggregate': False,                    # aggregate output\n",
    "        'in_zscore_pvalue': None,                 # zscore pvalue\n",
    "        'in_stand_qupper': 0.99,                  # upper quantile for standardisation\n",
    "        'in_stand_qlower': 0.05,                  # lower quantile for standardisation\n",
    "        'in_fmask_flags': 'Valid;Snow;Water',     # fmask flag values\n",
    "        'in_max_cloud': 10,                       # max cloud percentage\n",
    "        'in_interpolate': True,                   # interpolate missing pixels\n",
    "        'in_add_result_to_map': True,             # add result to map\n",
    "    }\n",
    "    \n",
    "    # set up list\n",
    "    ts = []\n",
    "            \n",
    "    # func: default zscore p-value\n",
    "    msg = 'Running Test Five: default zscore p-value (None).'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_zscore_pvalue': None})\n",
    "    ts.append(['t_6_def.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "    \n",
    "    # func: zscore p-value is ''\n",
    "    msg = 'Running Test Five: zscore p-value is \"\".'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_zscore_pvalue': ''})\n",
    "    ts.append(['t_6_a.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    " \n",
    "    # func: zscore p-value is string of 0.01\n",
    "    msg = 'Running Test Five: zscore p-value is 0.01.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_zscore_pvalue': 0.01})\n",
    "    ts.append(['t_6_b.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "\n",
    "    # func: zscore p-value is string of 0.05\n",
    "    msg = 'Running Test Five: zscore p-value is 0.05.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_zscore_pvalue': 0.05})\n",
    "    ts.append(['t_6_c.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "\n",
    "    # func: zscore p-value is string of 0.1\n",
    "    msg = 'Running Test Five: zscore p-value is 0.1.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_zscore_pvalue': 0.1})\n",
    "    ts.append(['t_6_d.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "    \n",
    "    # func: zscore p-value is string of 1.0 (not supported)\n",
    "    msg = 'Running Test Five: zscore p-value is 1.0 (not supported).'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_zscore_pvalue': 1.0})\n",
    "    ts.append(['t_6_e.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])    \n",
    "\n",
    "    # func: zscore p-value is float of 0.01\n",
    "    msg = 'Running Test Five: zscore p-value is \"0.01\".'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_zscore_pvalue': '0.01'})\n",
    "    ts.append(['t_6_f.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg]) \n",
    "    \n",
    "    # func: zscore p-value is int of 1 (not supported)\n",
    "    msg = 'Running Test Five: zscore p-value is \"1\" (not supported).'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_zscore_pvalue': '1'})\n",
    "    ts.append(['t_6_g.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])     \n",
    "            \n",
    "    return ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Six: Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build lsit of nc corruptors and tests to iterate\n",
    "nc_corrs = build_nc_corruptors(temp_nc)\n",
    "tests = build_test_six_funcs(in_nc=nc_file, temp_nc=temp_nc)\n",
    "\n",
    "# run!\n",
    "run_corruptors_through_tests(in_nc=nc_file, nc_coors=nc_corrs, tests=tests, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Seven: IVT Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_test_seven_funcs(in_nc, temp_nc):\n",
    "    \"\"\"sets up test seven functions\"\"\"\n",
    "    \n",
    "    # set default params for tool\n",
    "    inputs = {\n",
    "        'in_nc_file': '',                         # input nc (i.e. temp nc)\n",
    "        'out_likelihood_nc_file': '',             # output nc (i.e. t1a nc)\n",
    "        'in_wet_months': '1;2;3',                 # wet months \n",
    "        'in_dry_months': '9;10;11',               # dry months \n",
    "        'in_veg_idx': 'MAVI',                     # vege index name\n",
    "        'in_mst_idx': 'NDMI',                     # moisture index name       \n",
    "        'in_aggregate': False,                    # aggregate output\n",
    "        'in_zscore_pvalue': None,                 # zscore pvalue\n",
    "        'in_stand_qupper': None,                  # upper quantile for standardisation\n",
    "        'in_stand_qlower': None,                  # lower quantile for standardisation\n",
    "        'in_fmask_flags': 'Valid;Snow;Water',     # fmask flag values\n",
    "        'in_max_cloud': 10,                       # max cloud percentage\n",
    "        'in_interpolate': True,                   # interpolate missing pixels\n",
    "        'in_add_result_to_map': True,             # add result to map\n",
    "    }\n",
    "    \n",
    "    # set up list\n",
    "    ts = []\n",
    "            \n",
    "    # func: default ivt lower and upper\n",
    "    msg = 'Running Test Seven: default IVT lower, upper input (0.05, 0.99).'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_stand_qlower': 0.05, 'in_stand_qupper': 0.99})\n",
    "    ts.append(['t_7_def.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "\n",
    "    # func: ivt lower and upper \"\", \"\"\n",
    "    msg = 'Running Test Seven: IVT lower, upper input \"\", \"\".'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_stand_qlower': '', 'in_stand_qupper': ''})\n",
    "    ts.append(['t_7_a.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "    \n",
    "    # func: ivt lower and upper 0.0, 0.99\n",
    "    msg = 'Running Test Seven: IVT lower, upper input 0.0, 0.99.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_stand_qlower': 0.0, 'in_stand_qupper': 0.99})\n",
    "    ts.append(['t_7_b.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg]) \n",
    "    \n",
    "    # func: ivt lower and upper 0.05, 0.0\n",
    "    msg = 'Running Test Seven: IVT lower, upper input 0.05, 0.0.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_stand_qlower': 0.05, 'in_stand_qupper': 0.0})\n",
    "    ts.append(['t_7_c.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])    \n",
    "\n",
    "    # func: ivt lower and upper 0.25, 0.80\n",
    "    msg = 'Running Test Seven: IVT lower, upper input 0.25, 0.80.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_stand_qlower': 0.25, 'in_stand_qupper': 0.80})\n",
    "    ts.append(['t_7_d.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])    \n",
    "\n",
    "    # func: ivt lower and upper 0.4, 0.6\n",
    "    msg = 'Running Test Seven: IVT lower, upper input 0.4, 0.6.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_stand_qlower': 0.4, 'in_stand_qupper': 0.6})\n",
    "    ts.append(['t_7_e.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg]) \n",
    "    \n",
    "    # func: ivt lower and upper 0.5, 0.5\n",
    "    msg = 'Running Test Seven: IVT lower, upper input 0.5, 0.5.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_stand_qlower': 0.5, 'in_stand_qupper': 0.5})\n",
    "    ts.append(['t_7_f.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])     \n",
    "\n",
    "    # func: ivt lower and upper \"0.2\", \"0.8\"\n",
    "    msg = 'Running Test Seven: IVT lower, upper input \"0.2\", \"0.8\".'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_stand_qlower': \"0.2\", 'in_stand_qupper': \"0.8\"})\n",
    "    ts.append(['t_7_g.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg]) \n",
    "    \n",
    "    return ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Seven: Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build lsit of nc corruptors and tests to iterate\n",
    "nc_corrs = build_nc_corruptors(temp_nc)\n",
    "tests = build_test_seven_funcs(in_nc=nc_file, temp_nc=temp_nc)\n",
    "\n",
    "# run!\n",
    "run_corruptors_through_tests(in_nc=nc_file, nc_coors=nc_corrs, tests=tests, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Eight: Pixel Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_test_eight_funcs(in_nc, temp_nc):\n",
    "    \"\"\"sets up test eight functions\"\"\"\n",
    "    \n",
    "    # set default params for tool\n",
    "    inputs = {\n",
    "        'in_nc_file': '',                         # input nc (i.e. temp nc)\n",
    "        'out_likelihood_nc_file': '',             # output nc (i.e. t1a nc)\n",
    "        'in_wet_months': '1;2;3',                 # wet months \n",
    "        'in_dry_months': '9;10;11',               # dry months \n",
    "        'in_veg_idx': 'MAVI',                     # vege index name\n",
    "        'in_mst_idx': 'NDMI',                     # moisture index name       \n",
    "        'in_aggregate': False,                    # aggregate output\n",
    "        'in_zscore_pvalue': None,                 # zscore pvalue\n",
    "        'in_stand_qupper': 0.99,                  # upper quantile for standardisation\n",
    "        'in_stand_qlower': 0.05,                  # lower quantile for standardisation\n",
    "        'in_fmask_flags': '',                     # fmask flag values\n",
    "        'in_max_cloud': 10,                       # max cloud percentage\n",
    "        'in_interpolate': True,                   # interpolate missing pixels\n",
    "        'in_add_result_to_map': True,             # add result to map\n",
    "    }\n",
    "    \n",
    "    # set up list\n",
    "    ts = []\n",
    "            \n",
    "    # func: default pixel flags\n",
    "    msg = 'Running Test Eight: pixel flags Valid;Snow;Water.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_fmask_flags': 'Valid;Snow;Water'})\n",
    "    ts.append(['t_8_def.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "\n",
    "    # func: pixel flags is ''\n",
    "    msg = 'Running Test Eight: pixel flags is \"\".'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_fmask_flags': ''})\n",
    "    ts.append(['t_8_a.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "    \n",
    "    # func: pixel flags is NoData;Valid;Cloud;Shadow;Snow;Water\n",
    "    msg = 'Running Test Eight: pixel flags is NoData;Valid;Cloud;Shadow;Snow;Water.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_fmask_flags': 'NoData;Valid;Cloud;Shadow;Snow;Water'})\n",
    "    ts.append(['t_8_b.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])    \n",
    "\n",
    "    # func: pixel flags is Valid\n",
    "    msg = 'Running Test Eight: pixel flags is Valid.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_fmask_flags': 'Valid'})\n",
    "    ts.append(['t_8_c.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg]) \n",
    "\n",
    "    # func: pixel flags is Cloud;Shadow\n",
    "    msg = 'Running Test Eight: pixel flags is Cloud;Shadow.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_fmask_flags': 'Cloud;Shadow'})\n",
    "    ts.append(['t_8_d.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg]) \n",
    "\n",
    "    # func: pixel flags is Water\n",
    "    msg = 'Running Test Eight: pixel flags is Water.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_fmask_flags': 'Water'})\n",
    "    ts.append(['t_8_e.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg]) \n",
    "\n",
    "    # func: pixel flags is Water;Water\n",
    "    msg = 'Running Test Eight: pixel flags is Water;Water.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_fmask_flags': 'Water;Water'})\n",
    "    ts.append(['t_8_f.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg]) \n",
    "\n",
    "    # func: pixel flags is water\n",
    "    msg = 'Running Test Eight: pixel flags is water.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_fmask_flags': 'water'})\n",
    "    ts.append(['t_8_g.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg]) \n",
    "    \n",
    "    return ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Eight: Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build lsit of nc corruptors and tests to iterate\n",
    "nc_corrs = build_nc_corruptors(temp_nc)\n",
    "tests = build_test_eight_funcs(in_nc=nc_file, temp_nc=temp_nc)\n",
    "\n",
    "# run!\n",
    "run_corruptors_through_tests(in_nc=nc_file, nc_coors=nc_corrs, tests=tests, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Nine: Max Cloud Cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_test_nine_funcs(in_nc, temp_nc):\n",
    "    \"\"\"sets up test nine functions\"\"\"\n",
    "    \n",
    "    # set default params for tool\n",
    "    inputs = {\n",
    "        'in_nc_file': '',                         # input nc (i.e. temp nc)\n",
    "        'out_likelihood_nc_file': '',             # output nc (i.e. t1a nc)\n",
    "        'in_wet_months': '1;2;3',                 # wet months \n",
    "        'in_dry_months': '9;10;11',               # dry months \n",
    "        'in_veg_idx': 'MAVI',                     # vege index name\n",
    "        'in_mst_idx': 'NDMI',                     # moisture index name       \n",
    "        'in_aggregate': False,                    # aggregate output\n",
    "        'in_zscore_pvalue': None,                 # zscore pvalue\n",
    "        'in_stand_qupper': 0.99,                  # upper quantile for standardisation\n",
    "        'in_stand_qlower': 0.05,                  # lower quantile for standardisation\n",
    "        'in_fmask_flags': 'Valid;Snow;Water',     # fmask flag values\n",
    "        'in_max_cloud': '',                       # max cloud percentage\n",
    "        'in_interpolate': True,                   # interpolate missing pixels\n",
    "        'in_add_result_to_map': True,             # add result to map\n",
    "    }\n",
    "    \n",
    "    # set up list\n",
    "    ts = []\n",
    "            \n",
    "    # func: default max cloud cover\n",
    "    msg = 'Running Test Nine: default max cloud (10).'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_max_cloud': 10})\n",
    "    ts.append(['t_9_def.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "\n",
    "    # func: max cloud cover is 0\n",
    "    msg = 'Running Test Nine: max cloud cover is 0.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_max_cloud': 0})\n",
    "    ts.append(['t_9_a.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "    \n",
    "    # func: max cloud cover is 50\n",
    "    msg = 'Running Test Nine: max cloud cover is 50.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_max_cloud': 50})\n",
    "    ts.append(['t_9_b.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])    \n",
    "\n",
    "    # func: max cloud cover is \"10\"\n",
    "    msg = 'Running Test Nine: max cloud cover is \"10\".'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_max_cloud': \"10\"})\n",
    "    ts.append(['t_9_c.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])  \n",
    "    \n",
    "    # func: max cloud cover is \"10\"\n",
    "    msg = 'Running Test Nine: max cloud cover is \"10\".'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_max_cloud': \"10\"})\n",
    "    ts.append(['t_9_d.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])      \n",
    "\n",
    "    # func: max cloud cover is 150\n",
    "    msg = 'Running Test Nine: max cloud cover is 150.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_max_cloud': 150})\n",
    "    ts.append(['t_9_e.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])   \n",
    "    \n",
    "    # func: max cloud cover is ''\n",
    "    msg = 'Running Test Nine: max cloud cover is \"\".'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_max_cloud': ''})\n",
    "    ts.append(['t_9_f.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])   \n",
    "\n",
    "    # func: max cloud cover is None\n",
    "    msg = 'Running Test Nine: max cloud cover is None.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_max_cloud': None})\n",
    "    ts.append(['t_9_g.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])  \n",
    "    \n",
    "    return ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Nine: Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build lsit of nc corruptors and tests to iterate\n",
    "nc_corrs = build_nc_corruptors(temp_nc)\n",
    "tests = build_test_nine_funcs(in_nc=nc_file, temp_nc=temp_nc)\n",
    "\n",
    "# run!\n",
    "run_corruptors_through_tests(in_nc=nc_file, nc_coors=nc_corrs, tests=tests, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Ten: Interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_test_ten_funcs(in_nc, temp_nc):\n",
    "    \"\"\"sets up test ten functions\"\"\"\n",
    "    \n",
    "    # set default params for tool\n",
    "    inputs = {\n",
    "        'in_nc_file': '',                         # input nc (i.e. temp nc)\n",
    "        'out_likelihood_nc_file': '',             # output nc (i.e. t1a nc)\n",
    "        'in_wet_months': '1;2;3',                 # wet months \n",
    "        'in_dry_months': '9;10;11',               # dry months \n",
    "        'in_veg_idx': 'MAVI',                     # vege index name\n",
    "        'in_mst_idx': 'NDMI',                     # moisture index name       \n",
    "        'in_aggregate': False,                    # aggregate output\n",
    "        'in_zscore_pvalue': None,                 # zscore pvalue\n",
    "        'in_stand_qupper': 0.99,                  # upper quantile for standardisation\n",
    "        'in_stand_qlower': 0.05,                  # lower quantile for standardisation\n",
    "        'in_fmask_flags': 'Valid;Snow;Water',     # fmask flag values\n",
    "        'in_max_cloud': 10,                       # max cloud percentage\n",
    "        'in_interpolate': '',                     # interpolate missing pixels\n",
    "        'in_add_result_to_map': True,             # add result to map\n",
    "    }\n",
    "    \n",
    "    # set up list\n",
    "    ts = []\n",
    "            \n",
    "    # func: default interpolate (True)\n",
    "    msg = 'Running Test Ten: default interpolate (True).'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_interpolate': True})\n",
    "    ts.append(['t_10_def.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "    \n",
    "    # func: interpolate is False\n",
    "    msg = 'Running Test Ten: interpolate is False.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_interpolate': False})\n",
    "    ts.append(['t_10_a.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])    \n",
    "\n",
    "    # func: interpolate is None\n",
    "    msg = 'Running Test Ten: interpolate is None (default to True).'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_interpolate': None})\n",
    "    ts.append(['t_10_b.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])       \n",
    "            \n",
    "    return ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Ten: Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build lsit of nc corruptors and tests to iterate\n",
    "nc_corrs = build_nc_corruptors(temp_nc)\n",
    "tests = build_test_ten_funcs(in_nc=nc_file, temp_nc=temp_nc)\n",
    "\n",
    "# run!\n",
    "run_corruptors_through_tests(in_nc=nc_file, nc_coors=nc_corrs, tests=tests, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Eleven: Add To Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_test_eleven_funcs(in_nc, temp_nc):\n",
    "    \"\"\"sets up test eleven functions\"\"\"\n",
    "    \n",
    "    # set default params for tool\n",
    "    inputs = {\n",
    "        'in_nc_file': '',                         # input nc (i.e. temp nc)\n",
    "        'out_likelihood_nc_file': '',             # output nc (i.e. t1a nc)\n",
    "        'in_wet_months': '1;2;3',                 # wet months \n",
    "        'in_dry_months': '9;10;11',               # dry months \n",
    "        'in_veg_idx': 'MAVI',                     # vege index name\n",
    "        'in_mst_idx': 'NDMI',                     # moisture index name       \n",
    "        'in_aggregate': False,                    # aggregate output\n",
    "        'in_zscore_pvalue': None,                 # zscore pvalue\n",
    "        'in_stand_qupper': 0.99,                  # upper quantile for standardisation\n",
    "        'in_stand_qlower': 0.05,                  # lower quantile for standardisation\n",
    "        'in_fmask_flags': 'Valid;Snow;Water',     # fmask flag values\n",
    "        'in_max_cloud': 10,                       # max cloud percentage\n",
    "        'in_interpolate': True,                   # interpolate missing pixels\n",
    "        'in_add_result_to_map': '',             # add result to map\n",
    "    }\n",
    "    \n",
    "    # set up list\n",
    "    ts = []\n",
    "            \n",
    "    # func: default add result to map (True)\n",
    "    msg = 'Running Test Eleven: default add result to map (True).'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_add_result_to_map': True})\n",
    "    ts.append(['t_11_def.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])\n",
    "    \n",
    "    # func: add result to map is False\n",
    "    msg = 'Running Test Eleven: add result to map is False.'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_add_result_to_map': False})\n",
    "    ts.append(['t_11_a.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])    \n",
    "\n",
    "    # func: add result to map is None\n",
    "    msg = 'Running Test Eleven: add result to map is None (default to True).'\n",
    "    params = inputs.copy()\n",
    "    params.update({'in_add_result_to_map': None})\n",
    "    ts.append(['t_11_b.nc', arcpy.GDVSpectra_Likelihood_toolbox , params, msg])       \n",
    "            \n",
    "    return ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Eleven: Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build lsit of nc corruptors and tests to iterate\n",
    "nc_corrs = build_nc_corruptors(temp_nc)\n",
    "tests = build_test_eleven_funcs(in_nc=nc_file, temp_nc=temp_nc)\n",
    "\n",
    "# run!\n",
    "run_corruptors_through_tests(in_nc=nc_file, nc_coors=nc_corrs, tests=tests, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(self, parameters, messages):\n",
    "    \"\"\"\n",
    "    Executes the GDV Spectra Likelihood module.\n",
    "    \"\"\"\n",
    "\n",
    "    # safe imports\n",
    "    import os, sys        # arcgis comes with these\n",
    "    import datetime       # arcgis comes with these\n",
    "    import numpy as np    # arcgis comes with these\n",
    "\n",
    "    # risky imports (not native to arcgis)\n",
    "    try:\n",
    "        import xarray as xr\n",
    "        import dask\n",
    "    except Exception as e:\n",
    "        arcpy.AddError('Python libraries xarray and dask not installed.')\n",
    "        arcpy.AddMessage(str(e))\n",
    "        return\n",
    "\n",
    "    # import tools\n",
    "    try:\n",
    "        # shared folder\n",
    "        sys.path.append(FOLDER_SHARED)\n",
    "        import arc, satfetcher, tools\n",
    "\n",
    "        # module folder\n",
    "        sys.path.append(FOLDER_MODULES)\n",
    "        import gdvspectra, cog \n",
    "    except Exception as e:\n",
    "        arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')\n",
    "        arcpy.AddMessage(str(e))\n",
    "        return\n",
    "\n",
    "    # disable future warnings\n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "    warnings.simplefilter(action='ignore', category=dask.array.core.PerformanceWarning)\n",
    "\n",
    "    # grab parameter values \n",
    "    in_nc = parameters[0].valueAsText            # raw input satellite netcdf\n",
    "    out_nc = parameters[1].valueAsText           # output gdv likelihood netcdf\n",
    "    in_wet_months = parameters[2].valueAsText    # wet months \n",
    "    in_dry_months = parameters[3].valueAsText    # dry months \n",
    "    in_veg_idx = parameters[4].value             # vege index name\n",
    "    in_mst_idx = parameters[5].value             # moisture index name       \n",
    "    in_aggregate = parameters[6].value           # aggregate output\n",
    "    in_zscore_pvalue = parameters[7].value       # zscore pvalue\n",
    "    in_ivt_qupper = parameters[8].value          # upper quantile for standardisation\n",
    "    in_ivt_qlower = parameters[9].value          # lower quantile for standardisation\n",
    "    in_fmask_flags = parameters[10].valueAsText  # fmask flag values\n",
    "    in_max_cloud = parameters[11].value          # max cloud percentage\n",
    "    in_interpolate = parameters[12].value        # interpolate missing pixels\n",
    "    in_add_result_to_map = parameters[13].value  # add result to map\n",
    "\n",
    "\n",
    "\n",
    "    # # # # #\n",
    "    # notify user and set up progress bar\n",
    "    arcpy.AddMessage('Beginning GDVSpectra Likelihood.')\n",
    "    arcpy.SetProgressor(type='step', \n",
    "                        message='Preparing parameters...',\n",
    "                        min_range=0, max_range=19)\n",
    "\n",
    "\n",
    "\n",
    "    # # # # #\n",
    "    # notify and increment progress bar\n",
    "    arcpy.SetProgressorLabel('Loading and checking netcdf...')\n",
    "    arcpy.SetProgressorPosition(1)\n",
    "\n",
    "    try:\n",
    "        # do quick lazy load of netcdf for checking\n",
    "        ds = xr.open_dataset(in_nc)\n",
    "    except Exception as e:\n",
    "        arcpy.AddError('Could not quick load input satellite NetCDF data.')\n",
    "        arcpy.AddMessage(str(e))\n",
    "        return\n",
    "\n",
    "    # check xr type, vars, coords, dims, attrs\n",
    "    if not isinstance(ds, xr.Dataset):\n",
    "        arcpy.AddError('Input NetCDF must be a xr dataset.')\n",
    "        return\n",
    "    elif len(ds) == 0:\n",
    "        arcpy.AddError('Input NetCDF has no data/variables/bands.')\n",
    "        return\n",
    "    elif 'x' not in ds.dims or 'y' not in ds.dims or 'time' not in ds.dims:\n",
    "        arcpy.AddError('Input NetCDF must have x, y and time dimensions.')\n",
    "        return\n",
    "    elif 'x' not in ds.coords or 'y' not in ds.coords or 'time' not in ds.coords:\n",
    "        arcpy.AddError('Input NetCDF must have x, y and time coords.')\n",
    "        return\n",
    "    elif 'spatial_ref' not in ds.coords:\n",
    "        arcpy.AddError('Input NetCDF must have a spatial_ref coord.')\n",
    "        return\n",
    "    elif len(ds['x']) == 0 or len(ds['y']) == 0 or len(ds['time']) == 0:\n",
    "        arcpy.AddError('Input NetCDF must have all at least one x, y and time index.')\n",
    "        return\n",
    "    elif 'oa_fmask' not in ds and 'fmask' not in ds:\n",
    "        arcpy.AddError('Expected cloud mask band not found in NetCDF.')\n",
    "        return\n",
    "    elif not hasattr(ds, 'time.year') or not hasattr(ds, 'time.month'):\n",
    "        arcpy.AddError('Input NetCDF must have time with year and month component.')\n",
    "        return\n",
    "    elif len(ds.groupby('time.year')) < 3:\n",
    "        arcpy.AddError('Input NetCDF must have >= 3 years of data.')\n",
    "        return\n",
    "    elif ds.attrs == {}:\n",
    "        arcpy.AddError('NetCDF must have attributes.')\n",
    "        return\n",
    "    elif not hasattr(ds, 'crs'):\n",
    "        arcpy.AddError('NetCDF CRS attribute not found. CRS required.')\n",
    "        return\n",
    "    elif ds.crs != 'EPSG:3577':\n",
    "        arcpy.AddError('NetCDF CRS is not in GDA94 Albers (EPSG:3577).')            \n",
    "        return \n",
    "    elif not hasattr(ds, 'nodatavals'):\n",
    "        arcpy.AddError('NetCDF nodatavals attribute not found.')            \n",
    "        return \n",
    "\n",
    "    # efficient: if all nan, 0 at first var, assume rest same, so abort\n",
    "    if ds[list(ds)[0]].isnull().all() or (ds[list(ds)[0]] == 0).all():\n",
    "        arcpy.AddError('NetCDF has empty variables. Please download again.')            \n",
    "        return \n",
    "\n",
    "    try:\n",
    "        # now, do proper open of netcdf properly (and set nodata to nan)\n",
    "        ds = satfetcher.load_local_nc(nc_path=in_nc, \n",
    "                                      use_dask=True, \n",
    "                                      conform_nodata_to=np.nan)\n",
    "    except Exception as e:\n",
    "        arcpy.AddError('Could not properly load input satellite NetCDF data.')\n",
    "        arcpy.AddMessage(str(e))\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    # # # # #\n",
    "    # notify and increment progress bar\n",
    "    arcpy.SetProgressorLabel('Getting NetCDF attributes...')\n",
    "    arcpy.SetProgressorPosition(2)\n",
    "\n",
    "    # get attributes from dataset\n",
    "    ds_attrs = ds.attrs\n",
    "    ds_band_attrs = ds[list(ds)[0]].attrs\n",
    "    ds_spatial_ref_attrs = ds['spatial_ref'].attrs\n",
    "\n",
    "    # remove potential datetime duplicates\n",
    "    ds = satfetcher.group_dupe_times(ds)\n",
    "\n",
    "\n",
    "\n",
    "    # # # # #\n",
    "    # notify and increment progress bar\n",
    "    arcpy.SetProgressorLabel('Removing invalid pixels and empty dates...')\n",
    "    arcpy.SetProgressorPosition(3)  \n",
    "\n",
    "    # convert fmask as text to numeric code equivalents\n",
    "    in_fmask_flags = [e for e in in_fmask_flags.split(';')]\n",
    "    in_fmask_flags = arc.convert_fmask_codes(in_fmask_flags)\n",
    "\n",
    "    # check if flags selected, if not, select all \n",
    "    if len(in_fmask_flags) == 0:\n",
    "        arcpy.AddWarning('No flags selected, using default.')\n",
    "        in_fmask_flags = [1, 4, 5]\n",
    "\n",
    "    # check numeric flags are valid \n",
    "    for flag in in_fmask_flags:\n",
    "        if flag not in [0, 1, 2, 3, 4, 5]:\n",
    "            arcpy.AddError('Pixel flag not supported.')\n",
    "            return\n",
    "\n",
    "    # check for duplicate flags \n",
    "    u, c = np.unique(in_fmask_flags, return_counts=True)\n",
    "    if len(u[c > 1]) > 0:\n",
    "        arcpy.AddError('Duplicate pixel flags detected.')\n",
    "        return\n",
    "\n",
    "    # get name of mask band\n",
    "    mask_band = arc.get_name_of_mask_band(list(ds))\n",
    "\n",
    "    try:\n",
    "        # remove invalid pixels and empty scenes\n",
    "        ds = cog.remove_fmask_dates(ds=ds, \n",
    "                                    valid_class=in_fmask_flags, \n",
    "                                    max_invalid=in_max_cloud, \n",
    "                                    mask_band=mask_band, \n",
    "                                    nodata_value=np.nan, \n",
    "                                    drop_fmask=True)\n",
    "    except Exception as e:\n",
    "        arcpy.AddError('Could not mask pixels.')\n",
    "        arcpy.AddMessage(str(e))\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    # # # # #\n",
    "    # notify and increment progress bar\n",
    "    arcpy.SetProgressorLabel('Conforming satellite band names...')\n",
    "    arcpy.SetProgressorPosition(4)\n",
    "\n",
    "    try:\n",
    "        # get platform name from attributes, error if no attributes\n",
    "        in_platform = arc.get_platform_from_dea_attrs(ds_attrs)\n",
    "\n",
    "        # conform dea aws band names based on platform\n",
    "        ds = satfetcher.conform_dea_ard_band_names(ds=ds, \n",
    "                                                   platform=in_platform.lower())   \n",
    "    except Exception as e: \n",
    "        arcpy.AddError('Could not get platform from attributes.')\n",
    "        arcpy.AddMessage(str(e))\n",
    "        return\n",
    "\n",
    "    # check if all expected bands are in dataset \n",
    "    for band in ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']:\n",
    "        if band not in ds:\n",
    "            arcpy.AddError('NetCDF is missing band: {}. Need all bands.'.format(band))\n",
    "            return\n",
    "\n",
    "\n",
    "\n",
    "    # # # # #\n",
    "    # notify and increment progress bar\n",
    "    arcpy.SetProgressorLabel('Reducing dataset to wet and dry months...')\n",
    "    arcpy.SetProgressorPosition(5)   \n",
    "\n",
    "    # prepare wet, dry season lists\n",
    "    if in_wet_months == '' or in_dry_months == '':\n",
    "        arcpy.AddError('Must include at least one wet and dry month.')\n",
    "        return\n",
    "\n",
    "    # unpack months\n",
    "    wet_month = [int(e) for e in in_wet_months.split(';')]\n",
    "    dry_month = [int(e) for e in in_dry_months.split(';')]\n",
    "\n",
    "    # check if same months in wet and dry\n",
    "    for v in wet_month:\n",
    "        if v in dry_month:\n",
    "            arcpy.AddError('Cannot use same month in wet and dry months.')\n",
    "            return\n",
    "\n",
    "    try:\n",
    "        # reduce xr dataset into only wet, dry months (force rechunk via time)\n",
    "        ds = gdvspectra.subset_months(ds=ds.chunk({'time': -1}), \n",
    "                                      month=wet_month + dry_month,\n",
    "                                      inplace=True)\n",
    "    except Exception as e: \n",
    "        arcpy.AddError('Could not subset dataset into wet and dry months.')\n",
    "        arcpy.AddMessage(str(e))\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    # # # # #\n",
    "    # notify and increment progress bar\n",
    "    arcpy.SetProgressorLabel('Calculating vegetation and moisture indices...')\n",
    "    arcpy.SetProgressorPosition(6) \n",
    "\n",
    "    # check if veg idx supported \n",
    "    if in_veg_idx.lower() not in ['ndvi', 'evi', 'savi', 'msavi', 'slavi', 'mavi', 'kndvi', 'tcg']:\n",
    "        arcpy.AddError('Vegetation index not supported.')\n",
    "        return \n",
    "    elif in_mst_idx.lower() not in ['ndmi', 'gvmi']:\n",
    "        arcpy.AddError('Moisture index not supported.')\n",
    "        return \n",
    "\n",
    "    try:\n",
    "        # calculate vegetation and moisture index\n",
    "        ds = tools.calculate_indices(ds=ds, \n",
    "                                     index=[in_veg_idx.lower(), in_mst_idx.lower()], \n",
    "                                     custom_name=['veg_idx', 'mst_idx'], \n",
    "                                     rescale=True, \n",
    "                                     drop=True)\n",
    "\n",
    "        # add band attrs back on\n",
    "        ds['veg_idx'].attrs = ds_band_attrs   \n",
    "        ds['mst_idx'].attrs = ds_band_attrs\n",
    "\n",
    "    except Exception as e: \n",
    "        arcpy.AddError('Could not calculate indices.')\n",
    "        arcpy.AddMessage(str(e))\n",
    "        return\n",
    "\n",
    "    # check once more: if all nan, 0, abort\n",
    "    if ds['veg_idx'].isnull().all() or ds['mst_idx'].isnull().all():\n",
    "        arcpy.AddError('NetCDF has empty variables. Please download again.')            \n",
    "        return \n",
    "\n",
    "\n",
    "\n",
    "    # # # # #\n",
    "    # notify and increment progress bar\n",
    "    arcpy.SetProgressorLabel('Interpolating dataset, if requested...')\n",
    "    arcpy.SetProgressorPosition(7) \n",
    "\n",
    "    # if requested...\n",
    "    if in_interpolate:\n",
    "        try:\n",
    "            # interpolate along time dimension (linear)\n",
    "            ds = tools.perform_interp(ds=ds, method='full')\n",
    "        except Exception as e: \n",
    "            arcpy.AddError('Could not interpolate dataset.')\n",
    "            arcpy.AddMessage(str(e))\n",
    "            return\n",
    "\n",
    "\n",
    "\n",
    "    # # # # #\n",
    "    # notify and increment progress bar\n",
    "    arcpy.SetProgressorLabel('Computing data into memory, please wait...')\n",
    "    arcpy.SetProgressorPosition(8)\n",
    "\n",
    "    # compute! \n",
    "    ds = ds.compute()\n",
    "\n",
    "    # check if all nan again\n",
    "    if ds.to_array().isnull().all():\n",
    "        arcpy.AddError('NetCDF is empty. Please download again.')            \n",
    "        return \n",
    "\n",
    "\n",
    "\n",
    "    # # # # #\n",
    "    # notify and increment progress bar\n",
    "    arcpy.SetProgressorLabel('Resampling dataset to annual wet/dry medians...')\n",
    "    arcpy.SetProgressorPosition(9)     \n",
    "\n",
    "    # extract datetimes for wet and dry seasons \n",
    "    dts_wet = ds['time'].where(ds['time.month'].isin(wet_month), drop=True)\n",
    "    dts_dry = ds['time'].where(ds['time.month'].isin(dry_month), drop=True)\n",
    "\n",
    "    # check if wet/dry months exist in the dataset, arent all empty\n",
    "    if len(dts_wet) == 0 or len(dts_dry) == 0:\n",
    "        arcpy.AddError('No wet and/or dry months captured in NetCDF.')\n",
    "        return\n",
    "    elif ds.sel(time=dts_wet).to_array().isnull().all():\n",
    "        arcpy.AddError('Entire wet season is devoid of values in NetCDF.')\n",
    "        return\n",
    "    elif ds.sel(time=dts_dry).to_array().isnull().all():\n",
    "        arcpy.AddError('Entire dry season is devoid of values in NetCDF.')\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # resample data to annual seasons\n",
    "        ds = gdvspectra.resample_to_wet_dry_medians(ds=ds, \n",
    "                                                    wet_month=wet_month, \n",
    "                                                    dry_month=dry_month,\n",
    "                                                    inplace=True)\n",
    "    except Exception as e: \n",
    "            arcpy.AddError('Could not resample annualised wet and dry seasons.')\n",
    "            arcpy.AddMessage(str(e))\n",
    "            return\n",
    "\n",
    "\n",
    "\n",
    "    # # # # #\n",
    "    # notify and increment progress bar\n",
    "    arcpy.SetProgressorLabel('Removing outliers, if requested...')\n",
    "    arcpy.SetProgressorPosition(10)\n",
    "\n",
    "    # prepare zscore selection\n",
    "    if in_zscore_pvalue not in [0.01, 0.05, 0.1, None]:\n",
    "        arcpy.AddWarning('Z-score value not supported. Setting to default.')\n",
    "        in_zscore_pvalue = None\n",
    "\n",
    "    # if requested...\n",
    "    if in_zscore_pvalue is not None:\n",
    "        try:\n",
    "            # remove outliers\n",
    "            ds = gdvspectra.nullify_wet_dry_outliers(ds=ds, \n",
    "                                                     wet_month=wet_month, \n",
    "                                                     dry_month=dry_month, \n",
    "                                                     p_value=in_zscore_pvalue,\n",
    "                                                     inplace=True)     \n",
    "        except Exception as e: \n",
    "            arcpy.AddError('Could not remove outliers.')\n",
    "            arcpy.AddMessage(str(e))\n",
    "            return\n",
    "\n",
    "\n",
    "\n",
    "    # # # # #\n",
    "    # notify and increment progress bar\n",
    "    arcpy.SetProgressorLabel('Cleaning years with insufficient seasonality...')\n",
    "    arcpy.SetProgressorPosition(11) \n",
    "\n",
    "    try:\n",
    "        # remove any years missing wet, dry season \n",
    "        ds = gdvspectra.drop_incomplete_wet_dry_years(ds=ds)\n",
    "    except Exception as e: \n",
    "        arcpy.AddError('Could not drop years with insufficient seasons.')\n",
    "        arcpy.AddMessage(str(e))\n",
    "        return\n",
    "\n",
    "    # check if we still have sufficient number of years \n",
    "    if len(ds.groupby('time.year')) < 3:\n",
    "        arcpy.AddError('Input NetCDF needs more years. Expand time range in NetCDF.')\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # fill any empty first, last years using manual back/forward fill\n",
    "        ds = gdvspectra.fill_empty_wet_dry_edges(ds=ds,\n",
    "                                                 wet_month=wet_month, \n",
    "                                                 dry_month=dry_month,\n",
    "                                                 inplace=True)\n",
    "    except Exception as e: \n",
    "        arcpy.AddError('Could not fill empty wet and dry edge dates.')\n",
    "        arcpy.AddMessage(str(e))\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # interpolate missing values \n",
    "        ds = gdvspectra.interp_empty_wet_dry(ds=ds,\n",
    "                                             wet_month=wet_month,\n",
    "                                             dry_month=dry_month,\n",
    "                                             method='full',\n",
    "                                             inplace=True)\n",
    "    except Exception as e: \n",
    "        arcpy.AddError('Could not interpolate empty wet and dry edge dates.')\n",
    "        arcpy.AddMessage(str(e))\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    # # # # #\n",
    "    # notify and increment progress bar\n",
    "    arcpy.SetProgressorLabel('Standardising data to dry season invariant targets...')\n",
    "    arcpy.SetProgressorPosition(12)\n",
    "\n",
    "    # check upper quantile\n",
    "    if in_ivt_qlower < 0 or in_ivt_qlower >= 0.5:\n",
    "        arcpy.AddMessage('Lower quantile must be between 0, 0.5. Setting to default.')\n",
    "        in_ivt_qlower = 0.05\n",
    "\n",
    "    # do same for upper quantile\n",
    "    if in_ivt_qupper <= 0.5 or in_ivt_qupper > 1.0:\n",
    "        arcpy.AddMessage('Upper quantile must be between 0.5, 1.0. Setting to default.')\n",
    "        in_ivt_qlower = 0.99 \n",
    "\n",
    "    # check if upper <= lower \n",
    "    if in_ivt_qupper <= in_ivt_qlower:\n",
    "        arcpy.AddError('Upper quantile must be > than lower quantile value.')\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # standardise data to invariant targets derived from dry times\n",
    "        ds = gdvspectra.standardise_to_dry_targets(ds=ds, \n",
    "                                                   dry_month=dry_month, \n",
    "                                                   q_upper=in_ivt_qupper,\n",
    "                                                   q_lower=in_ivt_qlower,\n",
    "                                                   inplace=True)\n",
    "    except Exception as e: \n",
    "        arcpy.AddError('Could not standardise data to invariant targets.')\n",
    "        arcpy.AddMessage(str(e))\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    # # # # #\n",
    "    # notify and increment progress bar\n",
    "    arcpy.SetProgressorLabel('Calculating seasonal similarity...')\n",
    "    arcpy.SetProgressorPosition(13)  \n",
    "\n",
    "    try:\n",
    "        # calculate seasonal similarity\n",
    "        ds_similarity = gdvspectra.calc_seasonal_similarity(ds=ds,\n",
    "                                                            wet_month=wet_month,\n",
    "                                                            dry_month=dry_month,\n",
    "                                                            q_mask=0.9,\n",
    "                                                            inplace=True)\n",
    "    except Exception as e: \n",
    "        arcpy.AddError('Could not generate similarity.')\n",
    "        arcpy.AddMessage(str(e))\n",
    "        return\n",
    "\n",
    "    # check similarity dataset is not empty \n",
    "    if ds_similarity.to_array().isnull().all():\n",
    "        arcpy.AddError('Similarity modelling returned no data.')\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    # # # # #\n",
    "    # notify and increment progress bar\n",
    "    arcpy.SetProgressorLabel('Calculating GDV Likelihood...')\n",
    "    arcpy.SetProgressorPosition(14)  \n",
    "\n",
    "    try:\n",
    "        # calculate gdv likelihood\n",
    "        ds = gdvspectra.calc_likelihood(ds=ds, \n",
    "                                        ds_similarity=ds_similarity,\n",
    "                                        wet_month=wet_month, \n",
    "                                        dry_month=dry_month)\n",
    "\n",
    "        # convert dataset back to float32\n",
    "        ds = ds.astype('float32')\n",
    "\n",
    "    except Exception as e: \n",
    "        arcpy.AddError('Could not generate likelihood data.')\n",
    "        arcpy.AddMessage(str(e))\n",
    "        return\n",
    "\n",
    "    # check likelihood dataset is not empty \n",
    "    if ds.to_array().isnull().all():\n",
    "        arcpy.AddError('Likelihood modelling returned no data.')\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    # # # # #\n",
    "    # notify and increment progress bar\n",
    "    arcpy.SetProgressorLabel('Aggreating dataset, if requested...')\n",
    "    arcpy.SetProgressorPosition(15) \n",
    "\n",
    "    # if requested...\n",
    "    if in_aggregate is True:\n",
    "        try:\n",
    "            # reducing full dataset down to one median image without time \n",
    "            ds = ds.median('time')\n",
    "        except Exception as e: \n",
    "            arcpy.AddError('Could not aggregate dataset.')\n",
    "            arcpy.AddMessage(str(e))\n",
    "            return\n",
    "\n",
    "\n",
    "\n",
    "    # # # # #\n",
    "    # notify and increment progess bar\n",
    "    arcpy.SetProgressorLabel('Appending attributes back on to dataset...')\n",
    "    arcpy.SetProgressorPosition(16)\n",
    "\n",
    "    # append attrbutes on to dataset and bands\n",
    "    ds.attrs = ds_attrs\n",
    "    ds['spatial_ref'].attrs = ds_spatial_ref_attrs\n",
    "    for var in ds:\n",
    "        ds[var].attrs = ds_band_attrs\n",
    "\n",
    "\n",
    "\n",
    "    # # # # #\n",
    "    # notify and increment progess bar\n",
    "    arcpy.SetProgressorLabel('Exporting NetCDF file...')\n",
    "    arcpy.SetProgressorPosition(17)   \n",
    "\n",
    "    try:\n",
    "        # export netcdf file\n",
    "        tools.export_xr_as_nc(ds=ds, filename=out_nc)\n",
    "    except Exception as e: \n",
    "            arcpy.AddError('Could not export dataset.')\n",
    "            arcpy.AddMessage(str(e))\n",
    "            return\n",
    "\n",
    "\n",
    "\n",
    "    # # # # #\n",
    "    # notify and increment progress bar\n",
    "    arcpy.SetProgressorLabel('Adding output to map, if requested...')\n",
    "    arcpy.SetProgressorPosition(18)\n",
    "\n",
    "    # if requested...\n",
    "    if in_add_result_to_map:\n",
    "        try:\n",
    "            # for current project, open current map\n",
    "            aprx = arcpy.mp.ArcGISProject('CURRENT')\n",
    "            m = aprx.activeMap\n",
    "\n",
    "            # remove likelihood layer if already exists\n",
    "            for layer in m.listLayers():\n",
    "                if layer.name == 'likelihood.crf':\n",
    "                    m.removeLayer(layer)\n",
    "\n",
    "            # create output folder using datetime as name\n",
    "            dt = datetime.datetime.now().strftime('%d%m%Y%H%M%S')\n",
    "            out_folder = os.path.join(os.path.dirname(out_nc), 'likelihood' + '_' + dt)\n",
    "            os.makedirs(out_folder)\n",
    "\n",
    "            # disable visualise on map temporarily\n",
    "            arcpy.env.addOutputsToMap = False\n",
    "\n",
    "            # create crf filename and copy it\n",
    "            out_file = os.path.join(out_folder, 'likelihood.crf')\n",
    "            crf = arcpy.CopyRaster_management(in_raster=out_nc, \n",
    "                                              out_rasterdataset=out_file)\n",
    "\n",
    "            # add to map                  \n",
    "            m.addDataFromPath(crf)   \n",
    "\n",
    "        except Exception as e:\n",
    "            arcpy.AddWarning('Could not visualise output, aborting visualisation.')\n",
    "            arcpy.AddMessage(str(e))\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            # get symbology, update it\n",
    "            layer = m.listLayers('likelihood.crf')[0]\n",
    "            sym = layer.symbology\n",
    "\n",
    "            # if layer has stretch coloriser, apply color\n",
    "            if hasattr(sym, 'colorizer'):\n",
    "                if sym.colorizer.type == 'RasterStretchColorizer':\n",
    "\n",
    "                    # apply percent clip type\n",
    "                    sym.colorizer.stretchType = 'PercentClip'\n",
    "                    sym.colorizer.minPercent = 0.01\n",
    "                    sym.colorizer.maxPercent = 0.99\n",
    "\n",
    "                    # apply color map\n",
    "                    cmap = aprx.listColorRamps('Bathymetric Scale')[0]\n",
    "                    sym.colorizer.colorRamp = cmap\n",
    "\n",
    "                    # apply other basic options\n",
    "                    sym.colorizer.invertColorRamp = False\n",
    "                    sym.colorizer.gamma = 1.0\n",
    "\n",
    "                    # update symbology\n",
    "                    layer.symbology = sym\n",
    "\n",
    "        except Exception as e:\n",
    "            arcpy.AddWarning('Could not colorise output, aborting colorisation.')\n",
    "            arcpy.AddMessage(str(e))\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "    # # # # #\n",
    "    # clean up variables\n",
    "    arcpy.SetProgressorLabel('Finalising process...')\n",
    "    arcpy.SetProgressorPosition(19)\n",
    "\n",
    "    # close main dataset and del datasets\n",
    "    ds.close()\n",
    "    del ds \n",
    "\n",
    "    # close similarity dataset\n",
    "    ds_similarity.close()\n",
    "    del ds_similarity\n",
    "\n",
    "    # notify user\n",
    "    arcpy.AddMessage('Generated GDV Likelihood successfully.')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(self, parameters, messages):       \n",
    "        \"\"\"\n",
    "        Executes the GDV Spectra Threshold module.\n",
    "        \"\"\"\n",
    "        \n",
    "        # safe imports\n",
    "        import os, sys                           # arcgis comes with these\n",
    "        import datetime                          # arcgis comes with this\n",
    "        import numpy as np                       # arcgis comes with this\n",
    "        import pandas as pd                      # arcgis comes with this\n",
    "        import arcpy                             # arcgis comes with this\n",
    "        \n",
    "        # risky imports (not native to arcgis)\n",
    "        try:\n",
    "            import xarray as xr\n",
    "            import dask\n",
    "        except Exception as e:\n",
    "            arcpy.AddError('Python libraries xarray and dask not installed.')\n",
    "            arcpy.AddMessage(str(e))\n",
    "            return\n",
    "                \n",
    "        # import tools\n",
    "        try:\n",
    "            # shared folder\n",
    "            sys.path.append(FOLDER_SHARED)\n",
    "            import arc, satfetcher, tools\n",
    "        \n",
    "            # module folder\n",
    "            sys.path.append(FOLDER_MODULES)\n",
    "            import gdvspectra \n",
    "        except Exception as e:\n",
    "            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')\n",
    "            arcpy.AddMessage(str(e))\n",
    "            return\n",
    "            \n",
    "        # disable future warnings\n",
    "        import warnings\n",
    "        warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "        warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "        warnings.simplefilter(action='ignore', category=dask.array.core.PerformanceWarning)\n",
    "        \n",
    "        # grab parameter values \n",
    "        in_nc = parameters[0].valueAsText                # likelihood netcdf\n",
    "        out_nc = parameters[1].valueAsText               # output netcdf\n",
    "        in_aggregate = parameters[2].value               # aggregate dates\n",
    "        in_specific_years = parameters[3].valueAsText    # set specific year \n",
    "        in_type = parameters[4].value                    # threshold type\n",
    "        in_std_dev = parameters[5].value                 # std dev threshold value \n",
    "        in_occurrence_feat = parameters[6]               # occurrence shp path \n",
    "        in_pa_column = parameters[7].value               # occurrence shp pres/abse col \n",
    "        in_remove_stray = parameters[8].value            # apply salt n pepper -- requires sa\n",
    "        in_convert_binary = parameters[9].value          # convert thresh to binary 1, nan\n",
    "        in_add_result_to_map = parameters[10].value      # add result to map\n",
    "\n",
    "\n",
    "        \n",
    "        # # # # #\n",
    "        # notify user and set up progress bar\n",
    "        arcpy.AddMessage('Beginning GDVSpectra Threshold.')\n",
    "        arcpy.SetProgressor(type='step', \n",
    "                            message='Preparing parameters...',\n",
    "                            min_range=0, max_range=12)\n",
    "\n",
    "\n",
    "\n",
    "        # # # # #\n",
    "        # notify and increment progress bar\n",
    "        arcpy.SetProgressorLabel('Loading and checking netcdf...')\n",
    "        arcpy.SetProgressorPosition(1)\n",
    "        \n",
    "        try:\n",
    "            # do quick lazy load of netcdf for checking\n",
    "            ds = xr.open_dataset(in_nc)\n",
    "        except Exception as e:\n",
    "            arcpy.AddWarning('Could not quick load input likelihood NetCDF data.')\n",
    "            arcpy.AddMessage(str(e))\n",
    "            return\n",
    "\n",
    "        # check xr type, vars, coords, dims, attrs\n",
    "        if not isinstance(ds, xr.Dataset):\n",
    "            arcpy.AddError('Input NetCDF must be a xr dataset.')\n",
    "            return\n",
    "        elif len(ds) == 0:\n",
    "            arcpy.AddError('Input NetCDF has no data/variables/bands.')\n",
    "            return\n",
    "        elif 'x' not in ds.dims or 'y' not in ds.dims:\n",
    "            arcpy.AddError('Input NetCDF must have x, y dimensions.')\n",
    "            return        \n",
    "        elif 'x' not in ds.coords or 'y' not in ds.coords:\n",
    "            arcpy.AddError('Input NetCDF must have x, y coords.')\n",
    "            return\n",
    "        elif 'spatial_ref' not in ds.coords:\n",
    "            arcpy.AddError('Input NetCDF must have a spatial_ref coord.')\n",
    "            return\n",
    "        elif len(ds['x']) == 0 or len(ds['y']) == 0:\n",
    "            arcpy.AddError('Input NetCDF must have at least one x, y index.')\n",
    "            return\n",
    "        elif 'like' not in ds:\n",
    "            arcpy.AddError('Input NetCDF must have a \"like\" variable. Run GDVSpectra Likelihood.')\n",
    "            return\n",
    "        elif 'time' in ds and (not hasattr(ds, 'time.year') or not hasattr(ds, 'time.month')):\n",
    "            arcpy.AddError('Input NetCDF must have time with year and month component.')\n",
    "            return\n",
    "        elif 'time' in ds.dims and 'time' not in ds.coords:\n",
    "            arcpy.AddError('Input NetCDF has time dimension but not coordinate.')\n",
    "            return\n",
    "        elif ds.attrs == {}:\n",
    "            arcpy.AddError('NetCDF attributes not found. NetCDF must have attributes.')\n",
    "            return\n",
    "        elif not hasattr(ds, 'crs'):\n",
    "            arcpy.AddError('NetCDF CRS attribute not found. CRS required.')\n",
    "            return\n",
    "        elif ds.crs != 'EPSG:3577':\n",
    "            arcpy.AddError('NetCDF CRS is not EPSG:3577. EPSG:3577 required.')            \n",
    "            return \n",
    "        elif not hasattr(ds, 'nodatavals'):\n",
    "            arcpy.AddError('NetCDF nodatavals attribute not found.')            \n",
    "            return \n",
    "\n",
    "        # check if variables (should only be like) are empty\n",
    "        if ds['like'].isnull().all() or (ds['like'] == 0).all():\n",
    "            arcpy.AddError('NetCDF \"like\" variable is empty. Please download again.')            \n",
    "            return \n",
    " \n",
    "        try:\n",
    "            # now, do proper open of netcdf (set nodata to nan)\n",
    "            ds = satfetcher.load_local_nc(nc_path=in_nc, \n",
    "                                          use_dask=True, \n",
    "                                          conform_nodata_to=np.nan)\n",
    "        except Exception as e:\n",
    "            arcpy.AddError('Could not properly load input likelihood NetCDF data.')\n",
    "            arcpy.AddMessage(str(e))\n",
    "            return\n",
    "\n",
    "\n",
    "\n",
    "        # # # # #\n",
    "        # notify and increment progress bar\n",
    "        arcpy.SetProgressorLabel('Getting NetCDF attributes...')\n",
    "        arcpy.SetProgressorPosition(2)\n",
    "            \n",
    "        # get attributes from dataset\n",
    "        ds_attrs = ds.attrs\n",
    "        ds_band_attrs = ds[list(ds)[0]].attrs\n",
    "        ds_spatial_ref_attrs = ds['spatial_ref'].attrs\n",
    "        \n",
    "        # remove potential datetime duplicates, if time exists\n",
    "        if 'time' in ds:\n",
    "            ds = satfetcher.group_dupe_times(ds)\n",
    "\n",
    "\n",
    "\n",
    "        # # # # #\n",
    "        # notify and increment progress bar\n",
    "        arcpy.SetProgressorLabel('Reducing dataset based on time, if requested...')\n",
    "        arcpy.SetProgressorPosition(3)\n",
    "                \n",
    "        # if time is in dataset...\n",
    "        if 'time' in ds:\n",
    "        \n",
    "            # check aggregate and specified year(s) is valid\n",
    "            if in_aggregate is None:\n",
    "                arcpy.AddError('Did not specify aggregate parameter.')\n",
    "                return\n",
    "            elif in_aggregate is False and in_specific_years is None:\n",
    "                arcpy.AddError('Did not provide a specific year.')\n",
    "                return\n",
    "                \n",
    "            # if specific years set...\n",
    "            if in_aggregate is False:\n",
    "                in_specific_years = [int(e) for e in in_specific_years.split(';')]\n",
    "               \n",
    "            # aggregate depending on user choice \n",
    "            if in_aggregate is True:\n",
    "                ds = ds.median('time')\n",
    "            else:\n",
    "                ds = ds.where(ds['time.year'].isin(in_specific_years), drop=True)\n",
    "                ds = ds.median('time')\n",
    "\n",
    "\n",
    "\n",
    "        # # # # #\n",
    "        # notify and increment progress bar\n",
    "        arcpy.SetProgressorLabel('Computing data into memory, please wait...')\n",
    "        arcpy.SetProgressorPosition(4)\n",
    "\n",
    "        # compute! \n",
    "        ds = ds.compute()\n",
    "                \n",
    "        # check if all nan again\n",
    "        if ds.to_array().isnull().all():\n",
    "            arcpy.AddError('NetCDF is empty. Please download again.')            \n",
    "            return \n",
    "            \n",
    "            \n",
    "            \n",
    "        # # # # #\n",
    "        # notify and increment progress bar\n",
    "        arcpy.SetProgressorLabel('Preparing occurrence points, if provided...')\n",
    "        arcpy.SetProgressorPosition(5)\n",
    "\n",
    "        # we need nodataval attr, so ensure it exists \n",
    "        ds.attrs = ds_attrs        \n",
    "        \n",
    "        # if requested...\n",
    "        if in_type == 'Occurrence Points':\n",
    "        \n",
    "            # check if both shapefile and field provided \n",
    "            if in_occurrence_feat.value is None or in_pa_column is None:\n",
    "                arcpy.AddError('No occurrence feature and/or field provided.')\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                # get path to feature instead of map layer \n",
    "                desc = arcpy.Describe(in_occurrence_feat)\n",
    "                in_occurrence_feat = os.path.join(desc.path, desc.name)\n",
    "            \n",
    "                # read shapefile via arcpy, convert feat into dataframe of x, y, actual\n",
    "                df_records = arc.read_shp_for_threshold(in_occurrence_feat=in_occurrence_feat, \n",
    "                                                        in_pa_column=in_pa_column)\n",
    "\n",
    "                # intersect points with dataset and extract likelihood values\n",
    "                df_records = tools.intersect_records_with_xr(ds=ds, \n",
    "                                                             df_records=df_records, \n",
    "                                                             extract=True, \n",
    "                                                             res_factor=3, \n",
    "                                                             if_nodata='any')    \n",
    "\n",
    "                # rename column to predicted and check\n",
    "                df_records = df_records.rename(columns={'like': 'predicted'})\n",
    "                \n",
    "                # check if any records intersected dataset \n",
    "                if len(df_records.index) == 0:\n",
    "                    arcpy.AddError('No shapefile points intersect GDV likelihood dataset.')\n",
    "                    return\n",
    "                    \n",
    "                # remove any records where vars contain nodata\n",
    "                df_records = tools.remove_nodata_records(df_records, \n",
    "                                                         nodata_value=ds.nodatavals)\n",
    "                                                         \n",
    "                # check again if any records exist\n",
    "                if len(df_records.index) == 0:\n",
    "                    arcpy.AddError('No shapefile points remain after empty values removed.')\n",
    "                    return\n",
    "                    \n",
    "            except Exception as e:\n",
    "                arcpy.AddError('Could not read shapefile, see messages for details.')\n",
    "                arcpy.AddMessage(str(e))\n",
    "                return\n",
    "                \n",
    "            # check if some 1s and 0s exist \n",
    "            unq = df_records['actual'].unique()\n",
    "            if not np.any(unq == 1) or not np.any(unq == 0):\n",
    "                arcpy.AddError('Insufficient presence/absence points within NetCDF bounds.')\n",
    "                return\n",
    "\n",
    "\n",
    "\n",
    "        # # # # #\n",
    "        # notify and increment progress bar\n",
    "        arcpy.SetProgressorLabel('Thresholding GDV Likelihood...')\n",
    "        arcpy.SetProgressorPosition(6)      \n",
    "        \n",
    "        try:\n",
    "            # perform thresholding using either shapefile points or std dev\n",
    "            if in_type == 'Occurrence Points' and df_records is not None:\n",
    "                ds = gdvspectra.threshold_likelihood(ds=ds,\n",
    "                                                     df=df_records, \n",
    "                                                     res_factor=3, \n",
    "                                                     if_nodata='any')\n",
    "            else:\n",
    "                ds = gdvspectra.threshold_likelihood(ds=ds,\n",
    "                                                     num_stdevs=in_std_dev, \n",
    "                                                     res_factor=3, \n",
    "                                                     if_nodata='any')\n",
    "                                                     \n",
    "            # rename var, convert to float32\n",
    "            ds = ds.rename({'like': 'thresh'}).astype('float32')\n",
    "\n",
    "        except Exception as e:\n",
    "            arcpy.AddError('Could not threshold data.')\n",
    "            arcpy.AddMessage(str(e))\n",
    "            #print(str(e))\n",
    "            return\n",
    "            \n",
    "        # check if any data was returned after threshold\n",
    "        if ds.to_array().isnull().all():\n",
    "            arcpy.AddError('Threshold returned no values, try modifying threshold.')\n",
    "            return\n",
    "\n",
    "\n",
    "\n",
    "        # # # # #\n",
    "        # notify and increment progress bar\n",
    "        arcpy.SetProgressorLabel('Removing stray pixels, if requested...')\n",
    "        arcpy.SetProgressorPosition(7) \n",
    "        \n",
    "        # if requested...\n",
    "        if in_remove_stray:\n",
    "            try:\n",
    "                # remove salt n pepper \n",
    "                ds = gdvspectra.remove_salt_pepper(ds, iterations=1)\n",
    "            except Exception as e:\n",
    "                arcpy.AddError('Could not remove stray pixels.')\n",
    "                arcpy.AddMessage(str(e))\n",
    "                return\n",
    "\n",
    "\n",
    "\n",
    "        # # # # #\n",
    "        # notify and increment progess bar\n",
    "        arcpy.SetProgressorLabel('Binarising values, if requested...')\n",
    "        arcpy.SetProgressorPosition(8)\n",
    "        \n",
    "        # if requested...\n",
    "        if in_convert_binary:\n",
    "\n",
    "            # set all threshold non-nan values to 1\n",
    "            ds = ds.where(ds.isnull(), 1)\n",
    "\n",
    "\n",
    "\n",
    "        # # # # #\n",
    "        # notify and increment progess bar\n",
    "        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')\n",
    "        arcpy.SetProgressorPosition(9)\n",
    "        \n",
    "        # append attrbutes on to dataset and bands\n",
    "        ds.attrs = ds_attrs\n",
    "        ds['spatial_ref'].attrs = ds_spatial_ref_attrs\n",
    "        for var in ds:\n",
    "            ds[var].attrs = ds_band_attrs\n",
    "\n",
    "\n",
    "\n",
    "        # # # # #\n",
    "        # notify and increment progess bar\n",
    "        arcpy.SetProgressorLabel('Exporting NetCDF file...')\n",
    "        arcpy.SetProgressorPosition(10)   \n",
    "\n",
    "        try:\n",
    "            # export netcdf file\n",
    "            tools.export_xr_as_nc(ds=ds, filename=out_nc)\n",
    "        except Exception as e:\n",
    "            arcpy.AddError('Could not export dataset.')\n",
    "            arcpy.AddMessage(str(e))\n",
    "            return\n",
    "\n",
    "\n",
    "\n",
    "        # # # # #\n",
    "        # notify and increment progess bar\n",
    "        arcpy.SetProgressorLabel('Adding output to map, if requested...')\n",
    "        arcpy.SetProgressorPosition(11)\n",
    "        \n",
    "        # if requested...\n",
    "        if in_add_result_to_map:\n",
    "            try:\n",
    "                # for current project, open current map\n",
    "                aprx = arcpy.mp.ArcGISProject('CURRENT')\n",
    "                m = aprx.activeMap\n",
    "                \n",
    "                # remove threshold layer if already exists \n",
    "                for layer in m.listLayers():\n",
    "                    if layer.name == 'likelihood_threshold.crf':\n",
    "                        m.removeLayer(layer)\n",
    "                \n",
    "                # create output folder using datetime as name\n",
    "                dt = datetime.datetime.now().strftime('%d%m%Y%H%M%S')\n",
    "                out_folder = os.path.join(os.path.dirname(out_nc), 'likelihood_threshold' + '_' + dt)\n",
    "                os.makedirs(out_folder)\n",
    "            \n",
    "                # disable visualise on map temporarily\n",
    "                arcpy.env.addOutputsToMap = False\n",
    "                \n",
    "                # create crf filename and copy it\n",
    "                out_file = os.path.join(out_folder, 'likelihood_threshold.crf')\n",
    "                crf = arcpy.CopyRaster_management(in_raster=out_nc, \n",
    "                                                  out_rasterdataset=out_file)\n",
    "                                    \n",
    "                # add to map                  \n",
    "                m.addDataFromPath(crf)  \n",
    "\n",
    "            except Exception as e:\n",
    "                arcpy.AddWarning('Could not visualise output, aborting visualisation.')\n",
    "                arcpy.AddMessage(str(e))\n",
    "                pass\n",
    "                \n",
    "            try:           \n",
    "                # get symbology, update it\n",
    "                layer = m.listLayers('likelihood_threshold.crf')[0]\n",
    "                sym = layer.symbology\n",
    "                \n",
    "                # if layer has stretch coloriser, apply color\n",
    "                if hasattr(sym, 'colorizer'):\n",
    "                    \n",
    "                    # apply percent clip type\n",
    "                    sym.colorizer.stretchType = 'PercentClip'\n",
    "                    sym.colorizer.minPercent = 0.01\n",
    "                    sym.colorizer.maxPercent = 0.99\n",
    "                \n",
    "                    # colorise deopending on binary or continious\n",
    "                    if in_convert_binary is True:\n",
    "                        cmap = aprx.listColorRamps('Yellow to Red')[0]\n",
    "                    else:\n",
    "                        cmap = aprx.listColorRamps('Bathymetric Scale')[0]\n",
    "                    \n",
    "                    # apply colormap\n",
    "                    sym.colorizer.colorRamp = cmap\n",
    "\n",
    "                    # apply other basic options\n",
    "                    sym.colorizer.invertColorRamp = False\n",
    "                    sym.colorizer.gamma = 1.0\n",
    "                        \n",
    "                    # update symbology\n",
    "                    layer.symbology = sym  \n",
    "       \n",
    "            \n",
    "            except Exception as e:\n",
    "                arcpy.AddWarning('Could not colorise output, aborting colorisation.')\n",
    "                arcpy.AddMessage(str(e))\n",
    "                pass\n",
    "\n",
    "\n",
    "        # # # # #\n",
    "        # notify and increment progress bar\n",
    "        arcpy.SetProgressorLabel('Finalising process...')\n",
    "        arcpy.SetProgressorPosition(12)\n",
    "        \n",
    "        # close main dataset\n",
    "        ds.close()\n",
    "        del ds \n",
    "        \n",
    "        # notify user\n",
    "        arcpy.AddMessage('Generated GDV Threshold successfully.')  \n",
    "                \n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
