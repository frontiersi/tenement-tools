# -*- coding: utf-8 -*-# importsimport osimport uuidimport certifiimport arcpy# set default gdal and ertifi envs (dev)#os.environ['GDAL_DATA']  = r'C:\Program Files\ArcGIS\Pro\Resources\pedata\gdaldata'#os.environ.setdefault("CURL_CA_BUNDLE", certifi.where())# set default gdal and ertifi envs (non-dev)try:    install_dir = arcpy.GetInstallInfo().get('InstallDir')  # get arcgis install dir    os.environ['GDAL_DATA'] = os.path.join(install_dir, 'Resources\pedata\gdaldata')  # join to gdal install    os.environ.setdefault("CURL_CA_BUNDLE", certifi.where())  # set certifiexcept:    arcpy.AddError('Could not get install directory for ArcGIS Pro or set certifi.')    raise# get location of tenement-tool toolboxtbx_filename = os.path.realpath(__file__)tbx_folder = os.path.dirname(tbx_filename)folder = os.path.dirname(tbx_folder)# globals (non-dev)#FOLDER_MODULES = os.path.join(folder, 'modules')#FOLDER_SHARED = os.path.join(folder, 'shared')#GRP_LYR_FILE = os.path.join(folder, r'arc\lyr\group_template.lyrx')#MON_LYR_FILE = os.path.join(folder, r'arc\lyr\monitoring_zones_template.lyrx')# globals (dev)FOLDER_MODULES = r'C:\Users\Lewis\Documents\GitHub\tenement-tools\modules'  FOLDER_SHARED = r'C:\Users\Lewis\Documents\GitHub\tenement-tools\shared'GRP_LYR_FILE = r'C:\Users\Lewis\Documents\GitHub\tenement-tools\arc\lyr\group_template.lyrx'MON_LYR_FILE = r'C:\Users\Lewis\Documents\GitHub\tenement-tools\arc\lyr\monitoring_zones_template.lyrx'# globals (dea aws)STAC_ENDPOINT = 'https://explorer.sandbox.dea.ga.gov.au/stac/search'STAC_ENDPOINT_ODC = 'https://explorer.sandbox.dea.ga.gov.au/stac'AWS_KEY = ''AWS_SECRET = ''RESULT_LIMIT = 250# globalsGDVSPECTRA_THRESHOLD = {}             # persist gdv threshold valuesGDVSPECTRA_TREND = {}                 # persist gdv trend valuesGDVSPECTRA_CVA = {}                   # persist gdv cva valuesENSEMBLE_SIGMOIDS = {}                # persist ensemble signoidal valuesENSEMBLE_MASKER = {}                  # persist ensemble masker valuesNRT_MOD_MON_AREAS_LAST_AREA = Noneclass Toolbox(object):    def __init__(self):        """Define the toolbox (the name of the toolbox is the name of the        .pyt file)."""           self.label = "Toolbox"        self.alias = "toolbox"        # list of tool classes associated with this toolbox        self.tools = [            COG_Fetch,             COG_Fetch_ODC,            COG_Explore,             GDVSpectra_Likelihood,             GDVSpectra_Threshold,             GDVSpectra_Trend,            GDVSpectra_CVA,            Phenolopy_Metrics,            Nicher_SDM,            Nicher_Masker,            VegFrax_Fractional_Cover,            Ensemble_Sigmoider,            Ensemble_Model,            Ensemble_Masker,            NRT_Create_Project,            NRT_Create_Monitoring_Areas,            NRT_Modify_Monitoring_Areas,            NRT_Delete_Monitoring_Areas,            NRT_Monitor_Areas,            NRT_Build_Graphs,            Test            ]class COG_Fetch(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = "COG Fetch"        self.description = "COG contains functions that " \                           "allow for efficient download of " \                           "analysis ready data (ARD) Landsat " \                           "5, 7, 8 or Sentinel 2A, 2B images " \                           "from the Digital Earth Australia " \                           "(DEA) public AWS server."        self.canRunInBackground = False    def getParameterInfo(self):                # input study area shapefile        par_studyarea_feat = arcpy.Parameter(                                displayName="Input study area feature",                                name="in_studyarea_feat",                                datatype="GPFeatureLayer",                                parameterType="Required",                                direction="Input"                                )                                        # set study area to be polygon only        par_studyarea_feat.filter.list = ['Polygon']                                        # output file location        par_out_nc_path = arcpy.Parameter(                                displayName="Output NetCDF file",                                name="out_nc_path",                                datatype="DEFile",                                parameterType="Required",                                direction="Output"                                )                                        # set file type to be netcdf only        par_out_nc_path.filter.list = ['nc']        # in_platform        par_platform = arcpy.Parameter(                            displayName="Satellite platform",                            name="in_platform",                            datatype="GPString",                            parameterType="Required",                            direction="Input",                            multiValue=False                            )                                    # set default platform        par_platform.values = 'Landsat'        par_platform.filter.list = ['Landsat', 'Sentinel']  # 'Sentinel 2A', 'Sentinel 2B'                # in_from_date        par_date_from = arcpy.Parameter(                            displayName="Date from",                            name="in_from_date",                            datatype="GPDate",                            parameterType="Required",                            direction="Input",                            multiValue=False                            )                                    # set in_from_date value        par_date_from.values = '2015/01/01'                # in_to_date        par_date_to = arcpy.Parameter(                        displayName="Date to",                        name="in_to_date",                        datatype="GPDate",                        parameterType="Required",                        direction="Input",                        multiValue=False                        )        # set in_from_date value        par_date_to.values = '2020/12/31'        # set bands        par_bands = arcpy.Parameter(                        displayName="Bands",                        name="in_bands",                        datatype="GPString",                        parameterType="Required",                        direction="Input",                        category='Satellite Bands',                        multiValue=True                        )                         # set landsat bands        bands = [            'Blue',             'Green',             'Red',             'NIR',             'SWIR1',             'SWIR2',             'OA_Mask'            ]                # set default bands        par_bands.filter.type = "ValueList"                par_bands.filter.list = bands        par_bands.values = bands                # set slc-off        par_slc_off = arcpy.Parameter(                        displayName="SLC Off",                        name="in_slc_off",                        datatype="GPBoolean",                        parameterType="Required",                        direction="Input",                        multiValue=False                        )                # set slc-off value        par_slc_off.value = False                       # set output datatype        par_output_dtype = arcpy.Parameter(                            displayName="Output data type",                            name="in_output_dtype",                            datatype="GPString",                            parameterType="Required",                            direction="Input",                            category='Warping Options',                            multiValue=False                            )                                    # set default platform        par_output_dtype.filter.list = ['int8', 'int16', 'float32', 'float64']        par_output_dtype.values = 'int16'                # todo make this changeh when sent/landsat changed        # set output resolution        par_output_res = arcpy.Parameter(                            displayName="Output pixel resolution",                            name="in_output_res",                            datatype="GPLong",                            parameterType="Required",                            direction="Input",                            category='Warping Options',                            multiValue=False                            )                                    # set default platform        par_output_res.filter.type = 'Range'        par_output_res.filter.list = [0, 1000]        par_output_res.value = 30         # todo allow this to handle np.nan        # set output nodata value        par_output_fill_value = arcpy.Parameter(                                    displayName="Output NoData value",                                    name="in_output_fill_value",                                    datatype="GPString",                                    parameterType="Required",                                    direction="Input",                                    category='Warping Options',                                    multiValue=False                                    )                                    # set default nodata value        par_output_fill_value.value = "-999"                # set output epsg        par_output_epsg = arcpy.Parameter(                            displayName="Output EPSG",                            name="in_output_epsg",                            datatype="GPLong",                            parameterType="Required",                            direction="Input",                            category='Warping Options',                            multiValue=False                            )                                    # set default epsg        par_output_epsg.filter.list = [3577]        par_output_epsg.values = 3577                # set resampling type        par_output_resampling = arcpy.Parameter(                            displayName="Resampling type",                            name="in_output_resampling",                            datatype="GPString",                            parameterType="Required",                            direction="Input",                            category='Warping Options',                            multiValue=False                            )                                    # set default resampling        par_output_resampling.filter.list = ['Nearest', 'Bilinear']        par_output_resampling.values = 'Nearest'                # set snap boundary        par_output_snap = arcpy.Parameter(                            displayName="Snap boundaries",                            name="in_snap_bounds",                            datatype="GPBoolean",                            parameterType="Required",                            direction="Input",                            category='Warping Options',                            multiValue=False                            )                # set snap boundary value        par_output_snap.value = True                # set rescale        par_output_rescale = arcpy.Parameter(                        displayName="Rescale",                        name="in_rescale",                        datatype="GPBoolean",                        parameterType="Required",                        direction="Input",                        category='Warping Options',                        multiValue=False                        )                # set rescale value        par_output_rescale.value = True                # set cell alignment        par_output_cell_align = arcpy.Parameter(                            displayName="Cell alignment",                            name="in_output_cell_align",                            datatype="GPString",                            parameterType="Required",                            direction="Input",                            category='Warping Options',                            multiValue=False                            )                                    # set default cell align        par_output_cell_align.filter.list = ['Top-left', 'Center']        par_output_cell_align.values = 'Top-left'                # set chunks        par_output_chunk_size = arcpy.Parameter(                            displayName="Chunk size",                            name="in_output_chunk_size",                            datatype="GPLong",                            parameterType="Required",                            direction="Input",                            category='Parallelisation',                            multiValue=False                            )                                    # set default chunksize        par_output_chunk_size.value = -1                # set dea aws stac url        par_output_stac_url = arcpy.Parameter(                                displayName="Digital Earth Australia STAC URL",                                name="in_output_stac_url",                                datatype="GPString",                                parameterType="Required",                                direction="Input",                                category='STAC Options',                                multiValue=False                                )                 # set default dea aws stac url        par_output_stac_url.value = STAC_ENDPOINT        # set dea aws key        par_output_aws_key = arcpy.Parameter(                                displayName="Digital Earth Australia AWS Key",                                name="in_output_aws_key",                                datatype="GPString",                                parameterType="Optional",                                direction="Input",                                category='STAC Options',                                multiValue=False                                )                 # set default dea aws key value        par_output_aws_key.value = AWS_KEY         # set dea aws secret        par_output_aws_secret = arcpy.Parameter(                                displayName="Digital Earth Australia AWS Secret Key",                                name="in_output_aws_secret",                                datatype="GPString",                                parameterType="Optional",                                direction="Input",                                category='STAC Options',                                multiValue=False                                )                 # set default dea aws secret value        par_output_aws_secret.value = AWS_SECRET                # combine parameters        parameters = [            par_studyarea_feat,            par_out_nc_path,            par_platform,            par_date_from,            par_date_to,            par_bands,            par_slc_off,            par_output_dtype,            par_output_res,            par_output_fill_value,            par_output_epsg,            par_output_resampling,            par_output_snap,            par_output_rescale,            par_output_cell_align,            par_output_chunk_size,            par_output_stac_url,            par_output_aws_key,            par_output_aws_secret        ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # modify bands list when platform changed        if parameters[2].value == 'Landsat' and not parameters[2].hasBeenValidated:                    # enable slc-off control            parameters[6].enabled = True                        # update landsat band list            bands = [                'Blue',                 'Green',                 'Red',                 'NIR',                 'SWIR1',                 'SWIR2',                 'OA_Mask'                ]                        # update bands and set to default resolution            parameters[5].filter.list = bands            parameters[5].values = bands            parameters[8].value = 30        elif 'Sentinel' in parameters[2].value and not parameters[2].hasBeenValidated:                    # disable slc-off control            parameters[6].enabled = False                        # update sentinel band list            bands = [                'Blue',                 'Green',                 'Red',                 'NIR1',                 'SWIR2',                 'SWIR3',                 'OA_Mask'                ]                        # update values in control            parameters[5].filter.list = bands            parameters[5].values = bands                        # set resolution to original 10x10m            parameters[8].value = 10        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the GDV Spectra Likelihood module.        """                # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)                                # safe imports        import os, sys        import io        import time        import numpy as np        import arcpy                # risky imports (not native to arcgis)        try:            import xarray as xr            import dask            import dask.array as da        except:            arcpy.AddError('Python libraries xarray, dask not installed.')            return        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, tools, satfetcher                    # module folder            sys.path.append(FOLDER_MODULES)            import cog        except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return                # notify         arcpy.AddMessage('Performing COG Fetch.')                                                    # grab parameter values         in_studyarea_feat = parameters[0].value         # study area feat         out_nc = parameters[1].valueAsText              # output nc         in_platform = parameters[2].value               # platform name        in_from_date = parameters[3].value              # from date        in_to_date = parameters[4].value                # to date        in_bands = parameters[5].valueAsText            # bands        in_slc_off = parameters[6].value                # slc off         in_dtype = parameters[7].value                  # output pixel dtype        in_res = parameters[8].value                    # output pixel resolution        in_fill_value = parameters[9].value             # todo processing string to int, float or np.nan        in_epsg = parameters[10].value                  # output epsg         in_resampling = parameters[11].value            # output resampler method         in_snap = parameters[12].value                  # output snap alignment         in_rescale = parameters[13].value               # output rescale        in_cell_align = parameters[14].value            # output cell alignmnent             in_chunk_size = parameters[15].value            # chunk size        in_stac_endpoint = parameters[16].value         # stac endpoint        in_aws_key = parameters[17].value               # dea aws key        in_aws_secret = parameters[18].value            # dea aws secret                # let user know that aws key and secret not yet implemented        if in_aws_key is not None or in_aws_secret is not None:            arcpy.AddWarning('AWS Credentials not yet supported. Using DEA AWS.')                # set up progess bar        arcpy.SetProgressor(type='default', message='Preparing query parameters...')                        # get minimum bounding geom from input         bbox = arc.get_selected_layer_extent(in_studyarea_feat)                # get collections based on platform         collections = arc.prepare_collections_list(in_platform)                    # prepare start and end date times        in_from_date = arc.datetime_to_string(in_from_date)        in_to_date = arc.datetime_to_string(in_to_date)                # fetch stac data         arcpy.SetProgressorLabel('Performing STAC query...')        feats = cog.fetch_stac_data(stac_endpoint=in_stac_endpoint,                                     collections=collections,                                     start_dt=in_from_date,                                     end_dt=in_to_date,                                     bbox=bbox,                                    slc_off=in_slc_off,                                    limit=RESULT_LIMIT)                # count number of items        arcpy.AddMessage('Found {} {} scenes.'.format(len(feats), in_platform))        # prepare band (i.e. stac assets) names        assets = arc.prepare_band_names(in_bands=in_bands,                                         in_platform=in_platform)                                                            # convert raw stac into dict with coord reproject, etc.        arcpy.SetProgressorLabel('Converting STAC data into useable format...')        meta, asset_table = cog.prepare_data(feats,                                              assets=assets,                                             bounds_latlon=bbox,                                              bounds=None,                                              epsg=in_epsg,                                              resolution=in_res,                                              snap_bounds=in_snap,                                             force_dea_http=True)                                                     # prepare resample and fill value types        resampling = in_resampling.lower()        fill_value = arc.prepare_fill_value_type(in_fill_value)                                                                                                  # convert assets to dask array        arcpy.SetProgressorLabel('Parallelising data...')        darray = cog.convert_to_dask(meta=meta,                                      asset_table=asset_table,                                      chunksize=in_chunk_size,                                     resampling=resampling,                                      dtype=in_dtype,                                      fill_value=fill_value,                                      rescale=in_rescale)                                             # prepare alignment type        cell_align = arc.prepare_cell_align_type(in_cell_align)        # generate coordinates and dimensions from metadata        arcpy.SetProgressorLabel('Building dataset metadata...')        coords, dims = cog.build_coords(feats=feats,                                        assets=assets,                                         meta=meta,                                        pix_loc=cell_align)                # build final xarray data array        arcpy.SetProgressorLabel('Finalising dataset...')        ds_name = 'stac-' + dask.base.tokenize(darray)        ds = xr.DataArray(darray,                          coords=coords,                          dims=dims,                          name=ds_name                          )                                 # comvert to cleaner xarray dataset        ds = ds.to_dataset(dim='band')                # append attributes onto dataset        ds = cog.build_attributes(ds=ds,                                  meta=meta,                                   collections=collections,                                   bands=assets,                                  slc_off=in_slc_off,                                   bbox=bbox,                                  dtype=in_dtype,                                  snap_bounds=in_snap,                                  fill_value=fill_value,                                   rescale=in_rescale,                                  cell_align=in_cell_align,                                  resampling=in_resampling)                                             # set up proper progress bar        arcpy.SetProgressor(type='step',                             message='Preparing data download...',                             min_range=0,                             max_range=len(ds.data_vars) + 1)        # get list of dataset vars and iterate compute on each        for counter, data_var in enumerate(list(ds.data_vars), start=1):                    # start clock            start = time.time()                    # update progress bar            arcpy.SetProgressorLabel('Downloading band: {}...'.format(data_var))            arcpy.SetProgressorPosition(counter)                    # compute!            ds[data_var] = ds[data_var].compute()                        # notify time             duration = round(time.time() - start, 2)            arcpy.AddMessage('Band: {} took: {}s to download.'.format(data_var, duration))                                          # wrap up         arcpy.SetProgressorLabel('Exporting NetCDF...')        arcpy.SetProgressorPosition(counter + 1)                        # export netcdf to output folder        tools.export_xr_as_nc(ds=ds, filename=out_nc)        # notify finish        arcpy.AddMessage('COG Fetch completed successfully.')                return# implement resamplingclass COG_Fetch_ODC(object):    def __init__(self):        """        Initialise tool.        """            # set tool name, description, options        self.label = "COG Fetch ODC"        self.description = "COG contains functions that " \                           "allow for efficient download of " \                           "analysis ready data (ARD) Landsat " \                           "5, 7, 8 or Sentinel 2A, 2B images " \                           "from the Digital Earth Australia " \                           "(DEA) public AWS server."        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """                # input study area shapefile        par_studyarea_feat = arcpy.Parameter(                               displayName='Input study area feature',                               name='in_studyarea_feat',                               datatype='GPFeatureLayer',                               parameterType='Required',                               direction='Input')        par_studyarea_feat.filter.list = ['Polygon']                                        # output file location        par_out_nc_path = arcpy.Parameter(                            displayName='Output NetCDF file',                            name='out_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_path.filter.list = ['nc']        # satellite platform        par_platform = arcpy.Parameter(                         displayName='Satellite platform',                         name='in_platform',                         datatype='GPString',                         parameterType='Required',                         direction='Input',                         multiValue=False)        par_platform.filter.list = ['Landsat', 'Sentinel']  # 'Sentinel 2A', 'Sentinel 2B'        par_platform.values = 'Landsat'                # start date        par_date_from = arcpy.Parameter(                          displayName='Date from',                          name='in_from_date',                          datatype='GPDate',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_date_from.values = '2015/01/01'                # end date        par_date_to = arcpy.Parameter(                        displayName='Date to',                        name='in_to_date',                        datatype='GPDate',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_date_to.values = '2020/12/31'        # satellite bands        par_bands = arcpy.Parameter(                      displayName='Bands',                      name='in_bands',                      datatype='GPString',                      parameterType='Required',                      direction='Input',                      category='Satellite bands',                      multiValue=True)        bands = [            'Blue',             'Green',             'Red',             'NIR',             'SWIR1',             'SWIR2',             'OA_Mask'            ]        par_bands.filter.type = 'ValueList'                par_bands.filter.list = bands        par_bands.values = bands                # slc-off        par_slc_off = arcpy.Parameter(                        displayName='SLC Off',                        name='in_slc_off',                        datatype='GPBoolean',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_slc_off.value = False                       # output datatype        par_output_dtype = arcpy.Parameter(                             displayName='Output data type',                             name='in_output_dtype',                             datatype='GPString',                             parameterType='Required',                             direction='Input',                             category='Warping Options',                             multiValue=False)        par_output_dtype.filter.list = ['int16', 'float32']        par_output_dtype.values = 'int16'                # output resolution        par_output_res = arcpy.Parameter(                           displayName='Output pixel resolution',                           name='in_output_res',                           datatype='GPLong',                           parameterType='Required',                           direction='Input',                           category='Warping Options',                           multiValue=False)        par_output_res.filter.type = 'Range'        par_output_res.filter.list = [1, 1000]        par_output_res.value = 30         # output nodata value        par_output_fill_value = arcpy.Parameter(                                  displayName='Output NoData value',                                  name='in_output_fill_value',                                  datatype='GPString',                                  parameterType='Required',                                  direction='Input',                                  category='Warping Options',                                  multiValue=False)        par_output_fill_value.value = '-999'                # output epsg        par_output_epsg = arcpy.Parameter(                            displayName='Output EPSG',                            name='in_output_epsg',                            datatype='GPLong',                            parameterType='Required',                            direction='Input',                            category='Warping Options',                            multiValue=False)        par_output_epsg.filter.list = [3577]        par_output_epsg.values = 3577                # output resampling type        par_output_resampling = arcpy.Parameter(                            displayName='Resampling type',                            name='in_output_resampling',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Warping Options',                            multiValue=False)        par_output_resampling.filter.list = ['Nearest', 'Bilinear']        par_output_resampling.values = 'Nearest'                # output snap boundary        #par_output_snap = arcpy.Parameter(                            #displayName='Snap boundaries',                            #name='in_snap_bounds',                            #datatype='GPBoolean',                            #parameterType='Required',                            #direction='Input',                            #category='Warping Options',                            #multiValue=False)        #par_output_snap.value = True                # output rescale        #par_output_rescale = arcpy.Parameter(                        #displayName='Rescale',                        #name='in_rescale',                        #datatype='GPBoolean',                        #parameterType='Required',                        #direction='Input',                        #category='Warping Options',                        #multiValue=False)        #par_output_rescale.value = True                # output cell alignment        #par_output_cell_align = arcpy.Parameter(                                  #displayName='Cell alignment',                                  #name='in_output_cell_align',                                  #datatype='GPString',                                  #parameterType='Required',                                  #direction='Input',                                  #category='Warping Options',                                  #multiValue=False)        #par_output_cell_align.filter.list = ['Top-left', 'Center']        #par_output_cell_align.values = 'Top-left'                # dask chunks        par_output_chunk_size = arcpy.Parameter(                                  displayName='Chunk size',                                  name='in_output_chunk_size',                                  datatype='GPLong',                                  parameterType='Required',                                  direction='Input',                                  category='Parallelisation',                                  multiValue=False)        par_output_chunk_size.value = -1                # dea aws stac url        par_output_stac_url = arcpy.Parameter(                                displayName='Digital Earth Australia STAC URL',                                name='in_output_stac_url',                                datatype='GPString',                                parameterType='Required',                                direction='Input',                                category='STAC Options',                                multiValue=False)        par_output_stac_url.value = STAC_ENDPOINT_ODC        # dea aws key        par_output_aws_key = arcpy.Parameter(                                displayName='Digital Earth Australia AWS Key',                                name='in_output_aws_key',                                datatype='GPString',                                parameterType='Optional',                                direction='Input',                                category='STAC Options',                                multiValue=False)        par_output_aws_key.value = AWS_KEY         # dea aws secret        par_output_aws_secret = arcpy.Parameter(                                displayName='Digital Earth Australia AWS Secret Key',                                name='in_output_aws_secret',                                datatype='GPString',                                parameterType='Optional',                                direction='Input',                                category='STAC Options',                                multiValue=False)        par_output_aws_secret.value = AWS_SECRET                # output cell alignment        par_save_type = arcpy.Parameter(                          displayName='Save type',                          name='in_save_type',                          datatype='GPString',                          parameterType='Required',                          direction='Input',                          category='Output Options',                          multiValue=False)        par_save_type.filter.list = ['Memory', 'Local']        par_save_type.values = 'Memory'                # combine parameters        parameters = [            par_studyarea_feat,            par_out_nc_path,            par_platform,            par_date_from,            par_date_to,            par_bands,            par_slc_off,            par_output_dtype,            par_output_res,            par_output_fill_value,            par_output_epsg,            par_output_resampling,            #par_output_snap,            #par_output_rescale,            #par_output_cell_align,            par_output_chunk_size,            par_output_stac_url,            par_output_aws_key,            par_output_aws_secret,            par_save_type        ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # modify bands list when platform changed        if parameters[2].value == 'Landsat' and not parameters[2].hasBeenValidated:                    # enable slc-off control            parameters[6].enabled = True                        # update landsat band list            bands = [                'Blue',                 'Green',                 'Red',                 'NIR',                 'SWIR1',                 'SWIR2',                 'OA_Mask'                ]                        # update bands and set to default resolution            parameters[5].filter.list = bands            parameters[5].values = bands            parameters[8].value = 30        elif 'Sentinel' in parameters[2].value and not parameters[2].hasBeenValidated:                    # disable slc-off control            parameters[6].enabled = False                        # update sentinel band list            bands = [                'Blue',                 'Green',                 'Red',                 'NIR1',                 'SWIR2',                 'SWIR3',                 'OA_Mask'                ]                        # update bands and set to default resolution            parameters[5].filter.list = bands            parameters[5].values = bands            parameters[8].value = 10        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the COG Fetch (ODC) module.        """                # set gdal global environ        import os        os.environ['GDAL_DISABLE_READDIR_ON_OPEN'] = 'EMPTY_DIR'        os.environ['CPL_VSIL_CURL_ALLOWED_EXTENSIONS '] = 'tif'        os.environ['VSI_CACHE '] = 'TRUE'        os.environ['GDAL_HTTP_MULTIRANGE '] = 'YES'        os.environ['GDAL_HTTP_MERGE_CONSECUTIVE_RANGES '] = 'YES'                # also set rasterio env variables        rasterio_env = {            'GDAL_DISABLE_READDIR_ON_OPEN': 'EMPTY_DIR',            'CPL_VSIL_CURL_ALLOWED_EXTENSIONS':'tif',            'VSI_CACHE': True,            'GDAL_HTTP_MULTIRANGE': 'YES',            'GDAL_HTTP_MERGE_CONSECUTIVE_RANGES': 'YES'        }                # safe imports        import sys                      # arcgis comes with these        import datetime                 # arcgis comes with these        import numpy as np              # arcgis comes with these        import arcpy                    # arcgis comes with these        from datetime import datetime   # arcgis comes with these                # risky imports (not native to arcgis)        try:            import xarray as xr            import dask            import rasterio            import pystac_client            from odc import stac        except:            arcpy.AddError('Python libraries xarray, dask, rasterio, pystac, or odc not installed.')            return                        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                    # module folder            sys.path.append(FOLDER_MODULES)            import cog_odc        except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return                    # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        # grab parameter values        in_studyarea_feat = parameters[0].value         # study area feat         out_nc = parameters[1].valueAsText              # output nc         in_platform = parameters[2].value               # platform name        in_from_date = parameters[3].value              # from date        in_to_date = parameters[4].value                # to date        in_bands = parameters[5].valueAsText            # bands        in_slc_off = parameters[6].value                # slc off         in_dtype = parameters[7].value                  # output pixel dtype        in_res = parameters[8].value                    # output pixel resolution        in_fill_value = parameters[9].value             # todo processing string to int, float or np.nan        in_epsg = parameters[10].value                  # output epsg         in_resampling = parameters[11].value            # output resampler method         #in_snap = parameters[12].value                 # output snap alignment         #in_rescale = parameters[13].value              # output rescale        #in_cell_align = parameters[14].value           # output cell alignmnent             in_chunk_size = parameters[12].value            # chunk size        in_stac_endpoint = parameters[13].value         # stac endpoint        in_aws_key = parameters[14].value               # dea aws key        in_aws_secret = parameters[15].value            # dea aws secret        in_save_type = parameters[16].value             # save type                        # # # # #        # notify user and set proressbar        arcpy.SetProgressor(type='default',                             message='Preparing parameters...')                # aws keys not implemented yet        if in_aws_key is not None or in_aws_secret is not None:            arcpy.AddWarning('AWS Credentials not yet supported. Using DEA AWS.')                # get minimum bounding geom from input         bbox = arc.get_selected_layer_extent(in_studyarea_feat)                # get collections based on platform         collections = arc.prepare_collections_list(in_platform)                    # prepare start and end date times        in_from_date = arc.datetime_to_string(in_from_date)        in_to_date = arc.datetime_to_string(in_to_date)                # prepare band (i.e. stac assets) names        bands = arc.prepare_band_names(in_bands=in_bands,                                        in_platform=in_platform)                                               # prepare resample and fill value types        resampling = in_resampling.lower()        fill_value = arc.prepare_fill_value_type(in_fill_value)        # # # # #        # notify user and set progress        arcpy.SetProgressor(type='default',                             message='Performing STAC query and obtaining items...')                try:                   # fetch stac items            items = cog_odc.fetch_stac_items_odc(stac_endpoint=in_stac_endpoint,                                                  collections=collections,                                                  start_dt=in_from_date,                                                  end_dt=in_to_date,                                                  bbox=bbox,                                                 slc_off=in_slc_off,                                                 limit=RESULT_LIMIT)                        # count number of items            arcpy.AddMessage('Found {} {} scenes.'.format(len(items), in_platform))                    except:            arcpy.AddError('Could not obtain DEA AWS STAC items.')            return                # # # # #        # notify user and set progress        arcpy.SetProgressor(type='default',                             message='Replacing s3 url prefix with https...')                # replace s3 prefix with https for each band - arcgis doesnt like s3        items = cog_odc.replace_items_s3_to_https(items=items,                                                   from_prefix='s3://dea-public-data',                                                   to_prefix='https://data.dea.ga.gov.au')        # # # # #        # notify user and set progress        arcpy.SetProgressor(type='default',                             message='Converting STAC data into xr dataset via odc-stac...')        # construct an xr of items (lazy)        try:            ds = cog_odc.build_xr_odc(items=items,                                      bbox=bbox,                                      bands=bands,                                      crs=in_epsg,                                      resolution=in_res,                                      group_by='solar_day',                                      skip_broken_datasets=True,                                      like=None,                                      chunks={})                    except:            arcpy.AddError('Could not build xr dataset via odc-stac.')            return                                 # # # # #        # notify user and set progress        arcpy.SetProgressor(type='default',                             message='Preparing xr dataset for ArcGIS compatibility...')                                    # convert whole dataset from uint16 to int16 (to handle -999 nodata)        ds = cog_odc.convert_type(ds=ds, to_type=in_dtype)                # change nodata value from 0 to -999 to align with original cog method        ds = cog_odc.change_nodata_odc(ds=ds,                                       orig_value=0,                                        fill_value=fill_value)                # correct xr dataset datetimes for arcgis compatiability        ds = cog_odc.fix_xr_time_for_arc_cog(ds)                        # # # # #        # notify user and set progress        arcpy.SetProgressor(type='default',                             message='Appending original query attributes on to dataset...')                      # append query attributes on        ds = cog_odc.append_query_attrs_odc(ds=ds,                                            bbox=bbox,                                            collections=collections,                                             bands=bands,                                             resolution=in_res,                                            dtype=in_dtype,                                             fill_value=fill_value,                                            slc_off=in_slc_off,                                             resampling=resampling)          # # # # #        # download, compute / save netcdf!        if in_save_type == 'Memory':                        # set up proper progress bar            arcpy.SetProgressor(type='step',                                 message='Beginning to download data cube... This can take awhile.',                                 min_range=0,                                 max_range=len(ds.data_vars) + 1)            try:                for counter, data_var in enumerate(list(ds.data_vars), start=1):                                    # start clock                    start = time.time()                                    # update progress bar                    arcpy.SetProgressorLabel('Downloading band: {}...'.format(data_var))                    arcpy.SetProgressorPosition(counter)                                    # compute!                    with rasterio.Env(**rasterio_env):                        ds[data_var] = ds[data_var].compute()                                        # notify time                     duration = round((time.time() - start) / 60, 2)                    arcpy.AddMessage('Band: {} took: {} min to download.'.format(data_var, duration))                                # wrap up                 arcpy.SetProgressorLabel('Exporting NetCDF...')                arcpy.SetProgressorPosition(counter + 1)                                    # export netcdf to output folder                tools.export_xr_as_nc(ds=ds, filename=out_nc)                        except Exception as e:                arcpy.AddError(e)                return                    else:            # notify user and set progress            arcpy.SetProgressor(type='default',                                 message='Beginning to download data cube... This can take awhile.')                                            # start clock            start = time.time()                        try:                # export netcdf to output folder                with rasterio.Env(**rasterio_env):                    tools.export_xr_as_nc(ds=ds, filename=out_nc)                        except Exception as e:                arcpy.AddError(e)                return            # notify time             duration = round((time.time() - start) / 60, 2)            arcpy.AddMessage('NetCDF took: {} min to download.'.format(duration))                    # notify finish        arcpy.AddMessage('COG Fetch completed successfully.')                return# deprecatedclass COG_Sync(object):    def __init__(self):            # set tool name        self.label = "COG Sync"                # set tool description        self.description = "Sync COG to update cube with latest " \                           "data."                                   # set false for pro        self.canRunInBackground = False    def getParameterInfo(self):            # input netcdf file        par_nc_file = arcpy.Parameter(                        displayName="Input NetCDF file",                        name="in_nc_file",                        datatype="DEFile",                        parameterType="Required",                        direction="Input"                        )                                        # set options        par_nc_file.filter.list = ['nc']                # combine parameters        parameters = [            par_nc_file            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):                # imports        import os, sys        #import io        #import time        import pandas as pd        import numpy as np        import xarray as xr        import arcpy        # import tools        sys.path.append(FOLDER_SHARED)        import arc, tools, satfetcher                # import gdvspectra module        sys.path.append(FOLDER_MODULES)        import cog                # globals         AWS_KEY = ''        AWS_SECRET = ''        STAC_ENDPOINT = 'https://explorer.sandbox.dea.ga.gov.au/stac/search'        RESULT_LIMIT = 250                # notify         arcpy.AddMessage('Performing COG Sync.')                                                    # grab parameter values         in_nc = parameters[0].valueAsText      # raw netcdf        # set up progess bar        arcpy.SetProgressor(type='default', message='Loading and checking netcdf...')                # load netcdf file as xr        ds = satfetcher.load_local_nc(nc_path=in_nc,                                       use_dask=True,                                       conform_nodata_to=np.nan)  #nodatavals?                # checks        if 'time' not in ds:            arcpy.AddError('No time dimension detected.')                #tod other checks                 # get original query attributes         arcpy.SetProgressorLabel('Getting original query parameters...')        # check attributes        in_bands = list(ds.data_vars)        collections = list(ds.orig_collections)        bbox = list(ds.orig_bbox)        in_res = ds.res # use get xr res method        crs = ds.crs        in_slc_off = ds.orig_slc_off        resampling = ds.orig_resample        nodatavals = ds.nodatavals        # need to do        in_epsg = int(crs.split(':')[1])        in_platform = 'Landsat'        dtype = 'int16'        fill_value = -999        in_snap = True        rescale = True        cell_align = 'Top-left'        chunk_size = -1                # get datetimes        arcpy.SetProgressorLabel('Assessing dates...')        # get now, earliest, latest datetimes in dataset        dt_now = np.datetime64('now')        dt_first = ds['time'].isel(time=0).values        dt_last = ds['time'].isel(time=-1).values        # conver to stac format        in_from_date = arc.datetime_to_string(pd.Timestamp(dt_last))        in_to_date = arc.datetime_to_string(pd.Timestamp(dt_now))        # check if xr dt less than now (will be for now, but not if override)        if dt_last < dt_now:                        # fetch cog            arcpy.SetProgressorLabel('Performing STAC query...')            feats = cog.fetch_stac_data(stac_endpoint=STAC_ENDPOINT,                                         collections=collections,                                         start_dt=in_from_date,                                         end_dt=in_to_date,                                         bbox=bbox,                                        slc_off=in_slc_off,                                        limit=RESULT_LIMIT)                                                    # count number of items            arcpy.AddMessage('Found {} {} scenes.'.format(len(feats), in_platform))                                # prepare band (i.e. stac assets) names            assets = in_bands            #assets = arc.prepare_band_names(in_bands=in_bands,                                             #in_platform=in_platform)                            # convert raw stac into dict with coord reproject, etc.            arcpy.SetProgressorLabel('Converting STAC data into useable format...')            meta, asset_table = cog.prepare_data(feats,                                                  assets=assets,                                                 bounds_latlon=bbox,                                                  bounds=None,                                                  epsg=in_epsg,                                                  resolution=in_res,                                                  snap_bounds=in_snap,                                                 force_dea_http=True)                                                                                    else:            arcpy.AddMessage('No new scenes available. No sync required.')            returnclass COG_Explore(object):    def __init__(self):        self.label = "COG Explore"        self.description = "Explore an existing multidimensional " \                            "raster layer downloaded using COG " \                            "Fetcher."        self.canRunInBackground = False    def getParameterInfo(self):            # input netcdf file        par_nc_file = arcpy.Parameter(                        displayName="Input NetCDF file",                        name="in_nc_file",                        datatype="DEFile",                        parameterType="Required",                        direction="Input"                        )                                        # set options        par_nc_file.filter.list = ['nc']        # input vegetation index         par_veg_idx = arcpy.Parameter(                        displayName="Set vegetation index",                        name="in_veg_idx",                        datatype="GPString",                        parameterType="Required",                        direction="Input",                        multiValue=False                        )                                # set options        idxs = [            'NDVI',            'EVI',             'SAVI',            'MSAVI',            'SLAVI',            'MAVI',            'kNDVI',            'TCG']        par_veg_idx.filter.type = 'ValueList'        par_veg_idx.filter.list = idxs        par_veg_idx.value = 'MAVI'                               # input interpolate        par_interpolate = arcpy.Parameter(                            displayName="Interpolate missing pixels",                            name="in_interpolate",                            datatype="GPBoolean",                            parameterType="Required",                            direction="Input",                            multiValue=False                            )        par_interpolate.value = True        # set oa class values        par_fmask_flags = arcpy.Parameter(displayName="Include flags",                                          name="in_fmask_flags",                                          datatype="GPString",                                          parameterType="Required",                                          direction="Input",                                          category='Quality Options',                                          multiValue=True                                          )        fmask_flags = [            'NoData',             'Valid',             'Cloud',             'Shadow',             'Snow',             'Water']        par_fmask_flags.filter.type = "ValueList"                par_fmask_flags.filter.list = fmask_flags        par_fmask_flags.values = ['Valid', 'Snow', 'Water']                # max cloud cover        par_max_cloud = arcpy.Parameter(                            displayName="Maximum cloud cover",                            name="in_max_cloud",                            datatype="GPDouble",                            parameterType="Optional",                            direction="Input",                            category='Quality Options',                            multiValue=False                            )        par_max_cloud.filter.type = 'Range'        par_max_cloud.filter.list = [0.0, 100.0]        par_max_cloud.value = 10.0        # combine parameters        parameters = [            par_nc_file,            par_veg_idx,            par_interpolate,            par_fmask_flags,            par_max_cloud        ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):                            # safe imports        import os, sys        import numpy as np        import arcpy        from datetime import datetime        from tempfile import NamedTemporaryFile                # risky imports (not native to arcgis)        try:            import xarray as xr        except:            arcpy.AddError('Python libraries xarray not installed.')            return                        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, tools, satfetcher                    # shared modules            sys.path.append(FOLDER_MODULES)            import cog        except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return            # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)                    # notify         arcpy.AddMessage('Opening COG Explore.')                                                    # grab parameter values         in_nc = parameters[0].valueAsText            # raw netcdf        in_veg_idx = parameters[1].value             # vege index name        in_interpolate = parameters[2].value         # interpolate missing pixels        in_fmask_flags = parameters[3].valueAsText   # fmask flag values        in_max_cloud = parameters[4].value           # max cloud percentage        # set up progess bar        arcpy.SetProgressor(type='default', message='Loading and checking netcdf...')                # load netcdf file as xr        ds = satfetcher.load_local_nc(nc_path=in_nc,                                       use_dask=True,                                       conform_nodata_to=np.nan)                # tcheck if vars/bands exist        if len(ds.data_vars) == 0:            arcpy.AddError('No satellite bands detected in NetCDF.')            return                        # obtain band attributes for any band        band_attrs = ds[list(ds.data_vars)[0]].attrs                # get platform of current netcdf, expects dea aws labels        collections = ds.attrs.get('orig_collections')                # todo: put in func. grab collections whether string or tuple/list        if isinstance(collections, (list, tuple)) and len(collections) > 0:            in_platform = collections[0]        elif isinstance(collections, str):            in_platform = collections        else:            arcpy.AddError('Input NetCDF missing DEA AWS metadata.')            return                    # parse dea aws platform code from collections attirbute        if in_platform[:5] == 'ga_ls':            in_platform = 'Landsat'        elif in_platform[:2] == 's2':            in_platform = 'Sentinel'        else:            arcpy.AddError('Platform in NetCDF is not supported.')            return        # set name of mask band depending on platform        if in_platform == 'Landsat':            mask_band = 'oa_fmask'        elif in_platform == 'Sentinel':            mask_band = 'fmask'        else:            arcpy.AddError('No DEA AWS compatible mask band detected.')            return                # todo : func to do all this (convert value to num etc)        flags = [e for e in in_fmask_flags.split(';')]                # unpack flags as indexes. todo: move to arc function        in_fmask_flags = []        for flag in flags:            if flag == 'NoData':                in_fmask_flags.append(0)            elif flag == 'Valid':                in_fmask_flags.append(1)            elif flag == 'Cloud':                in_fmask_flags.append(2)            elif flag == 'Shadow':                in_fmask_flags.append(3)            elif flag == 'Snow':                in_fmask_flags.append(4)            elif flag == 'Water':                in_fmask_flags.append(5)                                        # remove clouded pixels and empty dates        arcpy.SetProgressorLabel('Removing invalid pixels and empty dates...')        ds = cog.remove_fmask_dates(ds=ds,                                     valid_class=in_fmask_flags,                                     max_invalid=in_max_cloud,                                     mask_band=mask_band,                                     nodata_value=np.nan,                                     drop_fmask=True)                  # generate mavi and drop bands         arcpy.SetProgressorLabel('Conforming band names...')        ds = satfetcher.conform_dea_ard_band_names(ds=ds,                                                    platform=in_platform.lower())                                                                                                                                                 # generate mavi and drop bands         arcpy.SetProgressorLabel('Calculating {}...'.format(in_veg_idx))        ds = tools.calculate_indices(ds=ds,                                      index=in_veg_idx.lower(),                                      custom_name=in_veg_idx.lower(),                                      rescale=False,                                      drop=True)                                          # add band attrs back on        ds[in_veg_idx.lower()].attrs = band_attrs                   # group duplicate times if exist and rechunk        ds = satfetcher.group_dupe_times(ds)        ds = ds.chunk({'time': -1})                # interpolate         if in_interpolate:            arcpy.SetProgressorLabel('Interpolating missing pixels...')            ds = ds.interpolate_na(dim='time', method='nearest')                # create temp netcdf with clean data         with NamedTemporaryFile() as tmp:            fn = tmp.name + '.nc'            ds.to_netcdf(fn)                    # temp disable auto add of outputs to map        arcpy.env.addOutputsToMap = False                # prepare output filename and folder        in_folder = os.path.dirname(in_nc)        dt = datetime.now().strftime("%d%m%Y%H%M%S")        out_crf = os.path.join(in_folder, 'mdr' + '_' + dt + '.crf')                # export new multidim raster raster for visualise        mdr = arcpy.CopyRaster_management(in_raster=fn,                                           out_rasterdataset=out_crf)                                                  # todo make this safe        aprx = arcpy.mp.ArcGISProject('CURRENT')        m = aprx.activeMap        m.addDataFromPath(mdr)                    # re-enable auto add to map and apply cmap        arcpy.env.addOutputsToMap = True        lyr = arc.apply_cmap(aprx=aprx,                              lyr_name='mdr' + '_' + dt + '.crf',                              cmap_name='Precipitation',                              cutoff_pct=0.5)                                     # close and del dataset        ds.close()        del ds                returnclass GDVSpectra_Likelihood(object):    def __init__(self):        """        Initialise tool.        """            # set tool name, description, options        self.label = 'GDVSpectra Likelihood'        self.description = 'GDVSpectra Likelihood derives potential groundwater ' \                           'dependent vegetation (GDV) areas from three or more years of ' \                           'Landsat or Sentinel NetCDF data. This functions results in ' \                           'a NetCDF of GDV likelihood with values ranging ' \                           'from 0 to 1, with 1 being highest probability of GDV.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """                # input netcdf data file        par_nc_file = arcpy.Parameter(                        displayName='Input satellite NetCDF file',                        name='in_nc_file',                        datatype='DEFile',                        parameterType='Required',                        direction='Input')        par_nc_file.filter.list = ['nc']                # output netcdf location        par_out_nc_file = arcpy.Parameter(                            displayName='Output GDV Likelihood NetCDF file',                            name='out_likelihood_nc_file',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_file.filter.list = ['nc']                # input wet month(s)         par_wet_months = arcpy.Parameter(                           displayName='Wet month(s)',                           name='in_wet_months',                           datatype='GPLong',                           parameterType='Required',                           direction='Input',                           category='Wet Period',                           multiValue=True)        par_wet_months.filter.type = 'ValueList'        par_wet_months.filter.list = [m for m in range(1, 13)]        par_wet_months.value = [1, 2, 3]                        # input dry month(s)        par_dry_months = arcpy.Parameter(                           displayName='Dry month(s)',                           name='in_dry_months',                           datatype='GPLong',                           parameterType='Required',                           direction='Input',                           category='Dry Period',                           multiValue=True)        par_dry_months.filter.type = 'ValueList'        par_dry_months.filter.list = [m for m in range(1, 13)]        par_dry_months.value = [9, 10, 11]                # input vegetation index         par_veg_idx = arcpy.Parameter(                        displayName='Vegetation index',                        name='in_veg_idx',                        datatype='GPString',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_veg_idx.filter.type = 'ValueList'        par_veg_idx.filter.list = [            'NDVI',            'EVI',             'SAVI',            'MSAVI',            'SLAVI',            'MAVI',            'kNDVI',            'TCG'            ]        par_veg_idx.value = 'MAVI'                # input moisture index         par_mst_idx = arcpy.Parameter(                        displayName='Moisture index',                        name='in_mst_idx',                        datatype='GPString',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_mst_idx.filter.type = 'ValueList'        par_mst_idx.filter.list = ['NDMI', 'GVMI']        par_mst_idx.value = 'NDMI'                # aggregate likelihood layers        par_aggregate = arcpy.Parameter(                          displayName='Combine outputs',                          name='in_aggregate',                          datatype='GPBoolean',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_aggregate.value = True                # pvalue for zscore        par_zscore_pvalue = arcpy.Parameter(                              displayName='Z-Score p-value',                              name='in_zscore_pvalue',                              datatype='GPDouble',                              parameterType='Optional',                              direction='Input',                              category='Outlier Correction',                              multiValue=False)        par_zscore_pvalue.filter.type = 'ValueList'        par_zscore_pvalue.filter.list = [0.01, 0.05, 0.1]        par_zscore_pvalue.value = None                       # q upper for standardisation        par_ivt_qupper = arcpy.Parameter(                           displayName='Upper percentile',                           name='in_stand_qupper',                           datatype='GPDouble',                           parameterType='Required',                           direction='Input',                           category='Invariant Standardisation',                           multiValue=False)        par_ivt_qupper.filter.type = 'Range'        par_ivt_qupper.filter.list = [0.0, 1.0]        par_ivt_qupper.value = 0.99                         # q lower for standardisation        par_ivt_qlower = arcpy.Parameter(                           displayName='Lower percentile',                           name='in_stand_qlower',                           datatype='GPDouble',                           parameterType='Required',                           direction='Input',                           category='Invariant Standardisation',                           multiValue=False)        par_ivt_qlower.filter.type = 'Range'        par_ivt_qlower.filter.list = [0.0, 1.0]        par_ivt_qlower.value = 0.05                                              # mask flags        par_fmask_flags = arcpy.Parameter(displayName='Include pixels flags',                            name='in_fmask_flags',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Satellite Quality Options',                            multiValue=True)        flags = [            'NoData',             'Valid',             'Cloud',             'Shadow',             'Snow',             'Water'            ]        par_fmask_flags.filter.type = 'ValueList'                par_fmask_flags.filter.list = flags        par_fmask_flags.values = ['Valid', 'Snow', 'Water']                # max cloud cover        par_max_cloud = arcpy.Parameter(                          displayName='Maximum cloud cover',                          name='in_max_cloud',                          datatype='GPDouble',                          parameterType='Required',                          direction='Input',                          category='Satellite Quality Options',                          multiValue=False)        par_max_cloud.filter.type = 'Range'        par_max_cloud.filter.list = [0.0, 100.0]        par_max_cloud.value = 10.0                # input interpolate        par_interpolate = arcpy.Parameter(                            displayName='Interpolate NoData pixels',                            name='in_interpolate',                            datatype='GPBoolean',                            parameterType='Required',                            direction='Input',                            category='Satellite Quality Options',                            multiValue=False)        par_interpolate.value = True                # input add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True        # combine parameters        parameters = [            par_nc_file,            par_out_nc_file,            par_wet_months,             par_dry_months,             par_veg_idx,             par_mst_idx,             par_aggregate,            par_zscore_pvalue,            par_ivt_qupper,            par_ivt_qlower,            par_fmask_flags,            par_max_cloud,            par_interpolate,            par_add_result_to_map            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""                        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the GDV Spectra Likelihood module.        """        # safe imports        import os, sys        # arcgis comes with these        import datetime       # arcgis comes with these        import numpy as np    # arcgis comes with these        # risky imports (not native to arcgis)        try:            import xarray as xr            import dask        except Exception as e:            arcpy.AddError('Python libraries xarray and dask not installed.')            arcpy.AddMessage(str(e))            return        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                    # module folder            sys.path.append(FOLDER_MODULES)            import gdvspectra, cog         except Exception as e:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            arcpy.AddMessage(str(e))            return                    # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        warnings.simplefilter(action='ignore', category=dask.array.core.PerformanceWarning)                 # grab parameter values         in_nc = parameters[0].valueAsText            # raw input satellite netcdf        out_nc = parameters[1].valueAsText           # output gdv likelihood netcdf        in_wet_months = parameters[2].valueAsText    # wet months         in_dry_months = parameters[3].valueAsText    # dry months         in_veg_idx = parameters[4].value             # vege index name        in_mst_idx = parameters[5].value             # moisture index name               in_aggregate = parameters[6].value           # aggregate output        in_zscore_pvalue = parameters[7].value       # zscore pvalue        in_ivt_qupper = parameters[8].value          # upper quantile for standardisation        in_ivt_qlower = parameters[9].value          # lower quantile for standardisation        in_fmask_flags = parameters[10].valueAsText  # fmask flag values        in_max_cloud = parameters[11].value          # max cloud percentage        in_interpolate = parameters[12].value        # interpolate missing pixels        in_add_result_to_map = parameters[13].value  # add result to map        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning GDVSpectra Likelihood.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=19)        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading and checking netcdf...')        arcpy.SetProgressorPosition(1)                try:            # do quick lazy load of netcdf for checking            ds = xr.open_dataset(in_nc)        except Exception as e:            arcpy.AddError('Could not quick load input satellite NetCDF data.')            arcpy.AddMessage(str(e))            return        # check xr type, vars, coords, dims, attrs        if not isinstance(ds, xr.Dataset):            arcpy.AddError('Input NetCDF must be a xr dataset.')            return        elif len(ds) == 0:            arcpy.AddError('Input NetCDF has no data/variables/bands.')            return        elif 'x' not in ds.dims or 'y' not in ds.dims or 'time' not in ds.dims:            arcpy.AddError('Input NetCDF must have x, y and time dimensions.')            return        elif 'x' not in ds.coords or 'y' not in ds.coords or 'time' not in ds.coords:            arcpy.AddError('Input NetCDF must have x, y and time coords.')            return        elif 'spatial_ref' not in ds.coords:            arcpy.AddError('Input NetCDF must have a spatial_ref coord.')            return        elif len(ds['x']) == 0 or len(ds['y']) == 0 or len(ds['time']) == 0:            arcpy.AddError('Input NetCDF must have all at least one x, y and time index.')            return        elif 'oa_fmask' not in ds and 'fmask' not in ds:            arcpy.AddError('Expected cloud mask band not found in NetCDF.')            return        elif not hasattr(ds, 'time.year') or not hasattr(ds, 'time.month'):            arcpy.AddError('Input NetCDF must have time with year and month component.')            return        elif len(ds.groupby('time.year')) < 3:            arcpy.AddError('Input NetCDF must have >= 3 years of data.')            return        elif ds.attrs == {}:            arcpy.AddError('NetCDF must have attributes.')            return        elif not hasattr(ds, 'crs'):            arcpy.AddError('NetCDF CRS attribute not found. CRS required.')            return        elif ds.crs != 'EPSG:3577':            arcpy.AddError('NetCDF CRS is not in GDA94 Albers (EPSG:3577).')                        return         elif not hasattr(ds, 'nodatavals'):            arcpy.AddError('NetCDF nodatavals attribute not found.')                        return                     # efficient: if all nan, 0 at first var, assume rest same, so abort        if ds[list(ds)[0]].isnull().all() or (ds[list(ds)[0]] == 0).all():            arcpy.AddError('NetCDF has empty variables. Please download again.')                        return         try:            # now, do proper open of netcdf properly (and set nodata to nan)            ds = satfetcher.load_local_nc(nc_path=in_nc,                                           use_dask=True,                                           conform_nodata_to=np.nan)        except Exception as e:            arcpy.AddError('Could not properly load input satellite NetCDF data.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Getting NetCDF attributes...')        arcpy.SetProgressorPosition(2)                    # get attributes from dataset        ds_attrs = ds.attrs        ds_band_attrs = ds[list(ds)[0]].attrs        ds_spatial_ref_attrs = ds['spatial_ref'].attrs                # remove potential datetime duplicates        ds = satfetcher.group_dupe_times(ds)                                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Removing invalid pixels and empty dates...')        arcpy.SetProgressorPosition(3)                  # convert fmask as text to numeric code equivalents        in_fmask_flags = [e for e in in_fmask_flags.split(';')]        in_fmask_flags = arc.convert_fmask_codes(in_fmask_flags)                # check if flags selected, if not, select all         if len(in_fmask_flags) == 0:            arcpy.AddWarning('No flags selected, using default.')            in_fmask_flags = [1, 4, 5]                # check numeric flags are valid         for flag in in_fmask_flags:            if flag not in [0, 1, 2, 3, 4, 5]:                arcpy.AddError('Pixel flag not supported.')                return                # check for duplicate flags         u, c = np.unique(in_fmask_flags, return_counts=True)        if len(u[c > 1]) > 0:            arcpy.AddError('Duplicate pixel flags detected.')            return         # get name of mask band        mask_band = arc.get_name_of_mask_band(list(ds))        try:            # remove invalid pixels and empty scenes            ds = cog.remove_fmask_dates(ds=ds,                                         valid_class=in_fmask_flags,                                         max_invalid=in_max_cloud,                                         mask_band=mask_band,                                         nodata_value=np.nan,                                         drop_fmask=True)        except Exception as e:            arcpy.AddError('Could not mask pixels.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Conforming satellite band names...')        arcpy.SetProgressorPosition(4)                try:            # get platform name from attributes, error if no attributes            in_platform = arc.get_platform_from_dea_attrs(ds_attrs)                        # conform dea aws band names based on platform            ds = satfetcher.conform_dea_ard_band_names(ds=ds,                                                        platform=in_platform.lower())           except Exception as e:             arcpy.AddError('Could not get platform from attributes.')            arcpy.AddMessage(str(e))            return        # check if all expected bands are in dataset         for band in ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']:            if band not in ds:                arcpy.AddError('NetCDF is missing band: {}. Need all bands.'.format(band))                return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Reducing dataset to wet and dry months...')        arcpy.SetProgressorPosition(5)                   # prepare wet, dry season lists        if in_wet_months == '' or in_dry_months == '':            arcpy.AddError('Must include at least one wet and dry month.')            return                # unpack months        wet_month = [int(e) for e in in_wet_months.split(';')]        dry_month = [int(e) for e in in_dry_months.split(';')]                # check if same months in wet and dry        for v in wet_month:            if v in dry_month:                arcpy.AddError('Cannot use same month in wet and dry months.')                return                        try:            # reduce xr dataset into only wet, dry months (force rechunk via time)            ds = gdvspectra.subset_months(ds=ds.chunk({'time': -1}),                                           month=wet_month + dry_month,                                          inplace=True)        except Exception as e:             arcpy.AddError('Could not subset dataset into wet and dry months.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Calculating vegetation and moisture indices...')        arcpy.SetProgressorPosition(6)                 # check if veg idx supported         if in_veg_idx.lower() not in ['ndvi', 'evi', 'savi', 'msavi', 'slavi', 'mavi', 'kndvi', 'tcg']:            arcpy.AddError('Vegetation index not supported.')            return         elif in_mst_idx.lower() not in ['ndmi', 'gvmi']:            arcpy.AddError('Moisture index not supported.')            return         try:            # calculate vegetation and moisture index            ds = tools.calculate_indices(ds=ds,                                          index=[in_veg_idx.lower(), in_mst_idx.lower()],                                          custom_name=['veg_idx', 'mst_idx'],                                          rescale=True,                                          drop=True)                                                     # add band attrs back on            ds['veg_idx'].attrs = ds_band_attrs               ds['mst_idx'].attrs = ds_band_attrs                    except Exception as e:             arcpy.AddError('Could not calculate indices.')            arcpy.AddMessage(str(e))            return                    # check once more: if all nan, 0, abort        if ds['veg_idx'].isnull().all() or ds['mst_idx'].isnull().all():            arcpy.AddError('NetCDF has empty variables. Please download again.')                        return         # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Interpolating dataset, if requested...')        arcpy.SetProgressorPosition(7)                    # if requested...        if in_interpolate:            try:                # interpolate along time dimension (linear)                ds = tools.perform_interp(ds=ds, method='full')            except Exception as e:                 arcpy.AddError('Could not interpolate dataset.')                arcpy.AddMessage(str(e))                return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Computing data into memory, please wait...')        arcpy.SetProgressorPosition(8)                # compute!         ds = ds.compute()                # check if all nan again        if ds.to_array().isnull().all():            arcpy.AddError('NetCDF is empty. Please download again.')                        return         # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Resampling dataset to annual wet/dry medians...')        arcpy.SetProgressorPosition(9)             # extract datetimes for wet and dry seasons         dts_wet = ds['time'].where(ds['time.month'].isin(wet_month), drop=True)        dts_dry = ds['time'].where(ds['time.month'].isin(dry_month), drop=True)                # check if wet/dry months exist in the dataset, arent all empty        if len(dts_wet) == 0 or len(dts_dry) == 0:            arcpy.AddError('No wet and/or dry months captured in NetCDF.')            return        elif ds.sel(time=dts_wet).to_array().isnull().all():            arcpy.AddError('Entire wet season is devoid of values in NetCDF.')            return        elif ds.sel(time=dts_dry).to_array().isnull().all():            arcpy.AddError('Entire dry season is devoid of values in NetCDF.')            return        try:            # resample data to annual seasons            ds = gdvspectra.resample_to_wet_dry_medians(ds=ds,                                                         wet_month=wet_month,                                                         dry_month=dry_month,                                                        inplace=True)        except Exception as e:                 arcpy.AddError('Could not resample annualised wet and dry seasons.')                arcpy.AddMessage(str(e))                return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Removing outliers, if requested...')        arcpy.SetProgressorPosition(10)                # prepare zscore selection        if in_zscore_pvalue not in [0.01, 0.05, 0.1, None]:            arcpy.AddWarning('Z-score value not supported. Setting to default.')            in_zscore_pvalue = None                # if requested...        if in_zscore_pvalue is not None:            try:                # remove outliers                ds = gdvspectra.nullify_wet_dry_outliers(ds=ds,                                                          wet_month=wet_month,                                                          dry_month=dry_month,                                                          p_value=in_zscore_pvalue,                                                         inplace=True)                 except Exception as e:                 arcpy.AddError('Could not remove outliers.')                arcpy.AddMessage(str(e))                return                                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Cleaning years with insufficient seasonality...')        arcpy.SetProgressorPosition(11)         try:            # remove any years missing wet, dry season             ds = gdvspectra.drop_incomplete_wet_dry_years(ds=ds)        except Exception as e:             arcpy.AddError('Could not drop years with insufficient seasons.')            arcpy.AddMessage(str(e))            return                    # check if we still have sufficient number of years         if len(ds.groupby('time.year')) < 3:            arcpy.AddError('Input NetCDF needs more years. Expand time range in NetCDF.')            return                    try:            # fill any empty first, last years using manual back/forward fill            ds = gdvspectra.fill_empty_wet_dry_edges(ds=ds,                                                     wet_month=wet_month,                                                      dry_month=dry_month,                                                     inplace=True)        except Exception as e:             arcpy.AddError('Could not fill empty wet and dry edge dates.')            arcpy.AddMessage(str(e))            return                    try:            # interpolate missing values             ds = gdvspectra.interp_empty_wet_dry(ds=ds,                                                 wet_month=wet_month,                                                 dry_month=dry_month,                                                 method='full',                                                 inplace=True)        except Exception as e:             arcpy.AddError('Could not interpolate empty wet and dry edge dates.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Standardising data to dry season invariant targets...')        arcpy.SetProgressorPosition(12)                # check upper quantile        if in_ivt_qlower < 0 or in_ivt_qlower >= 0.5:            arcpy.AddMessage('Lower quantile must be between 0, 0.5. Setting to default.')            in_ivt_qlower = 0.05                # do same for upper quantile        if in_ivt_qupper <= 0.5 or in_ivt_qupper > 1.0:            arcpy.AddMessage('Upper quantile must be between 0.5, 1.0. Setting to default.')            in_ivt_qlower = 0.99                 # check if upper <= lower         if in_ivt_qupper <= in_ivt_qlower:            arcpy.AddError('Upper quantile must be > than lower quantile value.')            return                    try:            # standardise data to invariant targets derived from dry times            ds = gdvspectra.standardise_to_dry_targets(ds=ds,                                                        dry_month=dry_month,                                                        q_upper=in_ivt_qupper,                                                       q_lower=in_ivt_qlower,                                                       inplace=True)        except Exception as e:             arcpy.AddError('Could not standardise data to invariant targets.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Calculating seasonal similarity...')        arcpy.SetProgressorPosition(13)          try:            # calculate seasonal similarity            ds_similarity = gdvspectra.calc_seasonal_similarity(ds=ds,                                                                wet_month=wet_month,                                                                dry_month=dry_month,                                                                q_mask=0.9,                                                                inplace=True)        except Exception as e:             arcpy.AddError('Could not generate similarity.')            arcpy.AddMessage(str(e))            return                    # check similarity dataset is not empty         if ds_similarity.to_array().isnull().all():            arcpy.AddError('Similarity modelling returned no data.')            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Calculating GDV Likelihood...')        arcpy.SetProgressorPosition(14)          try:            # calculate gdv likelihood            ds = gdvspectra.calc_likelihood(ds=ds,                                             ds_similarity=ds_similarity,                                            wet_month=wet_month,                                             dry_month=dry_month)                                                        # convert dataset back to float32            ds = ds.astype('float32')                    except Exception as e:             arcpy.AddError('Could not generate likelihood data.')            arcpy.AddMessage(str(e))            return                    # check likelihood dataset is not empty         if ds.to_array().isnull().all():            arcpy.AddError('Likelihood modelling returned no data.')            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Aggreating dataset, if requested...')        arcpy.SetProgressorPosition(15)                 # if requested...        if in_aggregate is True:            try:                # reducing full dataset down to one median image without time                 ds = ds.median('time')            except Exception as e:                 arcpy.AddError('Could not aggregate dataset.')                arcpy.AddMessage(str(e))                return        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(16)                # append attrbutes on to dataset and bands        ds.attrs = ds_attrs        ds['spatial_ref'].attrs = ds_spatial_ref_attrs        for var in ds:            ds[var].attrs = ds_band_attrs                                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(17)           try:            # export netcdf file            tools.export_xr_as_nc(ds=ds, filename=out_nc)        except Exception as e:                 arcpy.AddError('Could not export dataset.')                arcpy.AddMessage(str(e))                return                                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Adding output to map, if requested...')        arcpy.SetProgressorPosition(18)                # if requested...        if in_add_result_to_map:            try:                # for current project, open current map                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap                                # remove likelihood layer if already exists                for layer in m.listLayers():                    if layer.name == 'likelihood.crf':                        m.removeLayer(layer)                            # create output folder using datetime as name                dt = datetime.datetime.now().strftime('%d%m%Y%H%M%S')                out_folder = os.path.join(os.path.dirname(out_nc), 'likelihood' + '_' + dt)                os.makedirs(out_folder)                            # disable visualise on map temporarily                arcpy.env.addOutputsToMap = False                                # create crf filename and copy it                out_file = os.path.join(out_folder, 'likelihood.crf')                crf = arcpy.CopyRaster_management(in_raster=out_nc,                                                   out_rasterdataset=out_file)                                                    # add to map                                  m.addDataFromPath(crf)                               except Exception as e:                arcpy.AddWarning('Could not visualise output, aborting visualisation.')                arcpy.AddMessage(str(e))                pass                            try:                # get symbology, update it                layer = m.listLayers('likelihood.crf')[0]                sym = layer.symbology                                # if layer has stretch coloriser, apply color                if hasattr(sym, 'colorizer'):                    if sym.colorizer.type == 'RasterStretchColorizer':                        # apply percent clip type                        sym.colorizer.stretchType = 'PercentClip'                        sym.colorizer.minPercent = 0.01                        sym.colorizer.maxPercent = 0.99                        # apply color map                        cmap = aprx.listColorRamps('Bathymetric Scale')[0]                        sym.colorizer.colorRamp = cmap                        # apply other basic options                        sym.colorizer.invertColorRamp = False                        sym.colorizer.gamma = 1.0                        # update symbology                        layer.symbology = sym            except Exception as e:                arcpy.AddWarning('Could not colorise output, aborting colorisation.')                arcpy.AddMessage(str(e))                pass                        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(19)                # close main dataset and del datasets        ds.close()        del ds                 # close similarity dataset        ds_similarity.close()        del ds_similarity        # notify user        arcpy.AddMessage('Generated GDV Likelihood successfully.')        returnclass GDVSpectra_Threshold(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = 'GDVSpectra Threshold'        self.description = 'Threshold an existing GDV Likelihood NetCDF using a ' \                           'shapefile of points or standard deviation. The output ' \                           'is a layer representing areas of high potential GDV ' \                           'Likelihood only.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """            # input netcdf data file        par_nc_file = arcpy.Parameter(                        displayName='Input GDV Likelihood NetCDF file',                        name='in_nc_file',                        datatype='DEFile',                        parameterType='Required',                        direction='Input')        par_nc_file.filter.list = ['nc']                # output netcdf location        par_out_nc_file = arcpy.Parameter(                            displayName='Output GDV Threshold NetCDF file',                            name='out_nc_file',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_file.filter.list = ['nc']                # aggregate all dates        par_aggregate = arcpy.Parameter(                          displayName='Combine all input years',                          name='in_aggregate',                          datatype='GPBoolean',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_aggregate.value = True                # set specific years        par_specific_years = arcpy.Parameter(                               displayName='Specific year(s) to threshold',                               name='in_specific_years',                               datatype='GPLong',                               parameterType='Optional',                               direction='Input',                               multiValue=True)        par_specific_years.filter.type = 'ValueList'        par_specific_years.filter.list = []                # threshold type        par_type = arcpy.Parameter(                     displayName='Threshold type',                     name='in_type',                     datatype='GPString',                     parameterType='Required',                     direction='Input',                     multiValue=False)        par_type.filter.type = 'ValueList'        par_type.filter.list = ['Standard Deviation', 'Occurrence Points']        par_type.value = 'Standard Deviation'                # standard dev        par_std_dev = arcpy.Parameter(                        displayName='Standard deviation of threshold',                        name='in_std_dev',                        datatype='GPDouble',                        parameterType='Optional',                        direction='Input',                        multiValue=False)        par_std_dev.filter.type = 'Range'        par_std_dev.filter.list = [0.0, 10.0]        par_std_dev.value = 2.0                # occurrence points        par_occurrence_feat = arcpy.Parameter(                                displayName='Occurrence point feature',                                name='in_occurrence_feat',                                datatype='GPFeatureLayer',                                parameterType='Optional',                                direction='Input')        par_occurrence_feat.filter.list = ['Point']                # field of presence/absence values        par_pa_column = arcpy.Parameter(                          displayName='Field with presence and absence labels',                          name='in_pa_column',                          datatype='GPString',                          parameterType='Optional',                          direction='Input',                          multiValue=False)        par_pa_column.filter.type = 'ValueList'        par_pa_column.filter.list = []                        # remove stray pixels        par_remove_stray = arcpy.Parameter(                             displayName='Remove stray pixels',                             name='in_remove_stray',                             datatype='GPBoolean',                             parameterType='Required',                             direction='Input',                             category='Additional Options',                             multiValue=False)        par_remove_stray.value = True                # binarise checkbox        par_convert_binary = arcpy.Parameter(                               displayName='Binarise result',                               name='in_convert_binary',                               datatype='GPBoolean',                               parameterType='Required',                               direction='Input',                               category='Additional Options',                               multiValue=False)        par_convert_binary.value = True                # add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True                # combine parameters        parameters = [            par_nc_file,            par_out_nc_file,            par_aggregate,            par_specific_years,            par_type,            par_std_dev,            par_occurrence_feat,            par_pa_column,            par_remove_stray,            par_convert_binary,            par_add_result_to_map            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""                return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # imports        try:            import numpy as np            import xarray as xr                   except:            arcpy.AddError('Python libraries xarray not installed.')            return        # globals         global GDVSPECTRA_THRESHOLD                # unpack global parameter values         curr_file = GDVSPECTRA_THRESHOLD.get('in_file')        curr_feat = GDVSPECTRA_THRESHOLD.get('in_feat')                        # if input file added, run        if parameters[0].value is not None:                        # if global has no matching file (or first run), reload all            if curr_file != parameters[0].valueAsText:                try:                    ds = xr.open_dataset(parameters[0].valueAsText)                    dts = np.unique(ds['time.year']).tolist()                    ds.close()                except:                    dts = []                                    # populate year list with new years, reset selection                parameters[3].filter.list = dts                parameters[3].value = None        # if occurrence point feat added, run         if parameters[6].value is not None:                        # if global has no matching feat (or first run), reload all            if curr_feat != parameters[6].valueAsText:                try:                    shp_path = parameters[6].valueAsText                    cols = [f.name for f in arcpy.ListFields(shp_path)]                except:                    cols = []                # populate field list with new names, reset selection                parameters[7].filter.list = cols                parameters[7].value = None        # update global values        GDVSPECTRA_THRESHOLD = {            'in_file': parameters[0].valueAsText,            'in_feat': parameters[6].valueAsText,        }        # enable specifc years based on combine checkbox         if parameters[2].value is False:            parameters[3].enabled = True        else:            parameters[3].enabled = False                # enable std dev or shapefile and field based on drop down        if parameters[4].value == 'Standard Deviation':            parameters[5].enabled = True             parameters[6].enabled = False            parameters[7].enabled = False         else:            parameters[5].enabled = False             parameters[6].enabled = True            parameters[7].enabled = True         return            def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""                return    def execute(self, parameters, messages):               """        Executes the GDV Spectra Threshold module.        """                # safe imports        import os, sys                           # arcgis comes with these        import datetime                          # arcgis comes with this        import numpy as np                       # arcgis comes with this        import pandas as pd                      # arcgis comes with this        import arcpy                             # arcgis comes with this                # risky imports (not native to arcgis)        try:            import xarray as xr            import dask        except Exception as e:            arcpy.AddError('Python libraries xarray and dask not installed.')            arcpy.AddMessage(str(e))            return                        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                    # module folder            sys.path.append(FOLDER_MODULES)            import gdvspectra         except Exception as e:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            arcpy.AddMessage(str(e))            return                    # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        warnings.simplefilter(action='ignore', category=dask.array.core.PerformanceWarning)                # grab parameter values         in_nc = parameters[0].valueAsText                # likelihood netcdf        out_nc = parameters[1].valueAsText               # output netcdf        in_aggregate = parameters[2].value               # aggregate dates        in_specific_years = parameters[3].valueAsText    # set specific year         in_type = parameters[4].value                    # threshold type        in_std_dev = parameters[5].value                 # std dev threshold value         in_occurrence_feat = parameters[6]               # occurrence shp path         in_pa_column = parameters[7].value               # occurrence shp pres/abse col         in_remove_stray = parameters[8].value            # apply salt n pepper -- requires sa        in_convert_binary = parameters[9].value          # convert thresh to binary 1, nan        in_add_result_to_map = parameters[10].value      # add result to map                # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning GDVSpectra Threshold.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=12)        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading and checking netcdf...')        arcpy.SetProgressorPosition(1)                try:            # do quick lazy load of netcdf for checking            ds = xr.open_dataset(in_nc)        except Exception as e:            arcpy.AddWarning('Could not quick load input likelihood NetCDF data.')            arcpy.AddMessage(str(e))            return        # check xr type, vars, coords, dims, attrs        if not isinstance(ds, xr.Dataset):            arcpy.AddError('Input NetCDF must be a xr dataset.')            return        elif len(ds) == 0:            arcpy.AddError('Input NetCDF has no data/variables/bands.')            return        elif 'x' not in ds.dims or 'y' not in ds.dims:            arcpy.AddError('Input NetCDF must have x, y dimensions.')            return                elif 'x' not in ds.coords or 'y' not in ds.coords:            arcpy.AddError('Input NetCDF must have x, y coords.')            return        elif 'spatial_ref' not in ds.coords:            arcpy.AddError('Input NetCDF must have a spatial_ref coord.')            return        elif len(ds['x']) == 0 or len(ds['y']) == 0:            arcpy.AddError('Input NetCDF must have at least one x, y index.')            return        elif 'like' not in ds:            arcpy.AddError('Input NetCDF must have a "like" variable. Run GDVSpectra Likelihood.')            return        elif 'time' in ds and (not hasattr(ds, 'time.year') or not hasattr(ds, 'time.month')):            arcpy.AddError('Input NetCDF must have time with year and month component.')            return        elif ds.attrs == {}:            arcpy.AddError('NetCDF attributes not found. NetCDF must have attributes.')            return        elif not hasattr(ds, 'crs'):            arcpy.AddError('NetCDF CRS attribute not found. CRS required.')            return        elif ds.crs != 'EPSG:3577':            arcpy.AddError('NetCDF CRS is not EPSG:3577. EPSG:3577 required.')                        return         elif not hasattr(ds, 'nodatavals'):            arcpy.AddError('NetCDF nodatavals attribute not found.')                        return         # check if variables (should only be like) are empty        if ds['like'].isnull().all() or (ds['like'] == 0).all():            arcpy.AddError('NetCDF "like" variable is empty. Please download again.')                        return          try:            # now, do proper open of netcdf (set nodata to nan)            ds = satfetcher.load_local_nc(nc_path=in_nc,                                           use_dask=True,                                           conform_nodata_to=np.nan)        except Exception as e:            arcpy.AddError('Could not properly load input likelihood NetCDF data.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Getting NetCDF attributes...')        arcpy.SetProgressorPosition(2)                    # get attributes from dataset        ds_attrs = ds.attrs        ds_band_attrs = ds[list(ds)[0]].attrs        ds_spatial_ref_attrs = ds['spatial_ref'].attrs                # remove potential datetime duplicates, if time exists        if 'time' in ds:            ds = satfetcher.group_dupe_times(ds)        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Reducing dataset based on time, if requested...')        arcpy.SetProgressorPosition(3)                        # if time is in dataset...        if 'time' in ds:                    # check aggregate and specified year(s) is valid            if in_aggregate is None:                arcpy.AddError('Did not specify aggregate parameter.')                return            elif in_aggregate is False and in_specific_years is None:                arcpy.AddError('Did not provide a specific year.')                return                            # if specific years set...            if in_aggregate is False:                in_specific_years = [int(e) for e in in_specific_years.split(';')]                           # aggregate depending on user choice             if in_aggregate is True:                ds = ds.median('time')            else:                ds = ds.where(ds['time.year'].isin(in_specific_years), drop=True)                ds = ds.median('time')        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Computing data into memory, please wait...')        arcpy.SetProgressorPosition(4)        # compute!         ds = ds.compute()                        # check if all nan again        if ds.to_array().isnull().all():            arcpy.AddError('NetCDF is empty. Please download again.')                        return                                             # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Preparing occurrence points, if provided...')        arcpy.SetProgressorPosition(5)        # we need nodataval attr, so ensure it exists         ds.attrs = ds_attrs                        # if requested...        if in_type == 'Occurrence Points':                    # check if both shapefile and field provided             if in_occurrence_feat.value is None or in_pa_column is None:                arcpy.AddError('No occurrence feature and/or field provided.')                return            try:                # get path to feature instead of map layer                 desc = arcpy.Describe(in_occurrence_feat)                in_occurrence_feat = os.path.join(desc.path, desc.name)                            # read shapefile via arcpy, convert feat into dataframe of x, y, actual                df_records = arc.read_shp_for_threshold(in_occurrence_feat=in_occurrence_feat,                                                         in_pa_column=in_pa_column)                # intersect points with dataset and extract likelihood values                df_records = tools.intersect_records_with_xr(ds=ds,                                                              df_records=df_records,                                                              extract=True,                                                              res_factor=3,                                                              if_nodata='any')                    # rename column to predicted and check                df_records = df_records.rename(columns={'like': 'predicted'})                                # check if any records intersected dataset                 if len(df_records.index) == 0:                    arcpy.AddError('No shapefile points intersect GDV likelihood dataset.')                    return                                    # remove any records where vars contain nodata                df_records = tools.remove_nodata_records(df_records,                                                          nodata_value=ds.nodatavals)                                                                         # check again if any records exist                if len(df_records.index) == 0:                    arcpy.AddError('No shapefile points remain after empty values removed.')                    return                                except Exception as e:                arcpy.AddError('Could not read shapefile, see messages for details.')                arcpy.AddMessage(str(e))                return                            # check if some 1s and 0s exist             unq = df_records['actual'].unique()            if not np.any(unq == 1) or not np.any(unq == 0):                arcpy.AddError('Insufficient presence/absence points within NetCDF bounds.')                return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Thresholding GDV Likelihood...')        arcpy.SetProgressorPosition(6)                      try:            # perform thresholding using either shapefile points or std dev            if in_type == 'Occurrence Points' and df_records is not None:                ds = gdvspectra.threshold_likelihood(ds=ds,                                                     df=df_records,                                                      res_factor=3,                                                      if_nodata='any')            else:                ds = gdvspectra.threshold_likelihood(ds=ds,                                                     num_stdevs=in_std_dev,                                                      res_factor=3,                                                      if_nodata='any')                                                                 # rename var, convert to float32            ds = ds.rename({'like': 'thresh'}).astype('float32')        except Exception as e:            arcpy.AddError('Could not threshold data.')            arcpy.AddMessage(str(e))            return                    # check if any data was returned after threshold        if ds.to_array().isnull().all():            arcpy.AddError('Threshold returned no values, try modifying threshold.')            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Removing stray pixels, if requested...')        arcpy.SetProgressorPosition(7)                 # if requested...        if in_remove_stray:            try:                # remove salt n pepper                 ds = gdvspectra.remove_salt_pepper(ds, iterations=1)            except Exception as e:                arcpy.AddError('Could not remove stray pixels.')                arcpy.AddMessage(str(e))                return        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Binarising values, if requested...')        arcpy.SetProgressorPosition(8)                # if requested...        if in_convert_binary:            # set all threshold non-nan values to 1            ds = ds.where(ds.isnull(), 1)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(9)                # append attrbutes on to dataset and bands        ds.attrs = ds_attrs        ds['spatial_ref'].attrs = ds_spatial_ref_attrs        for var in ds:            ds[var].attrs = ds_band_attrs        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(10)           try:            # export netcdf file            tools.export_xr_as_nc(ds=ds, filename=out_nc)        except Exception as e:            arcpy.AddError('Could not export dataset.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Adding output to map, if requested...')        arcpy.SetProgressorPosition(11)                # if requested...        if in_add_result_to_map:            try:                # for current project, open current map                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap                                # remove threshold layer if already exists                 for layer in m.listLayers():                    if layer.name == 'likelihood_threshold.crf':                        m.removeLayer(layer)                                # create output folder using datetime as name                dt = datetime.datetime.now().strftime('%d%m%Y%H%M%S')                out_folder = os.path.join(os.path.dirname(out_nc), 'likelihood_threshold' + '_' + dt)                os.makedirs(out_folder)                            # disable visualise on map temporarily                arcpy.env.addOutputsToMap = False                                # create crf filename and copy it                out_file = os.path.join(out_folder, 'likelihood_threshold.crf')                crf = arcpy.CopyRaster_management(in_raster=out_nc,                                                   out_rasterdataset=out_file)                                                    # add to map                                  m.addDataFromPath(crf)              except Exception as e:                arcpy.AddWarning('Could not visualise output, aborting visualisation.')                arcpy.AddMessage(str(e))                pass                            try:                           # get symbology, update it                layer = m.listLayers('likelihood_threshold.crf')[0]                sym = layer.symbology                                # if layer has stretch coloriser, apply color                if hasattr(sym, 'colorizer'):                                        # apply percent clip type                    sym.colorizer.stretchType = 'PercentClip'                    sym.colorizer.minPercent = 0.01                    sym.colorizer.maxPercent = 0.99                                    # colorise deopending on binary or continious                    if in_convert_binary is True:                        cmap = aprx.listColorRamps('Yellow to Red')[0]                    else:                        cmap = aprx.listColorRamps('Bathymetric Scale')[0]                                        # apply colormap                    sym.colorizer.colorRamp = cmap                    # apply other basic options                    sym.colorizer.invertColorRamp = False                    sym.colorizer.gamma = 1.0                                            # update symbology                    layer.symbology = sym                                 except Exception as e:                arcpy.AddWarning('Could not colorise output, aborting colorisation.')                arcpy.AddMessage(str(e))                pass        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(12)                # close main dataset        ds.close()        del ds                 # notify user        arcpy.AddMessage('Generated GDV Threshold successfully.')                          returnclass GDVSpectra_Trend(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = 'GDVSpectra Trend'        self.description = 'Perform a time-series trend analysis on an existing ' \                           'GDV Likelihood data cube. Produces a map of areas where ' \                           'vegetation has continuously increased, decreased or ' \                           'has not changed.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """                # input like netcdf file        par_like_nc_file = arcpy.Parameter(                             displayName='Input GDV Likelihood NetCDF file',                             name='in_like_nc_file',                             datatype='DEFile',                             parameterType='Required',                             direction='Input')        par_like_nc_file.filter.list = ['nc']                # input mask netcdf file        par_mask_nc_file = arcpy.Parameter(                             displayName='Input GDV Threshold mask NetCDF file',                             name='in_mask_nc_file',                             datatype='DEFile',                             parameterType='Optional',                             direction='Input')        par_mask_nc_file.filter.list = ['nc']                # output netcdf location        par_out_nc_file = arcpy.Parameter(                                  displayName='Output GDV Trend NetCDF file',                                  name='out_nc_file',                                  datatype='DEFile',                                  parameterType='Required',                                  direction='Output')        par_out_nc_file.filter.list = ['nc']                # use all years        par_use_all_years = arcpy.Parameter(                          displayName='Use all years in input',                          name='in_use_all_years',                          datatype='GPBoolean',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_use_all_years.value = True                # set specific start year        par_start_year = arcpy.Parameter(                               displayName='Start year of trend analysis',                               name='in_start_year',                               datatype='GPLong',                               parameterType='Optional',                               direction='Input',                               multiValue=False)        par_start_year.filter.type = 'ValueList'        par_start_year.filter.list = []                # set specific end year        par_end_year = arcpy.Parameter(                               displayName='End year of trend analysis',                               name='in_end_year',                               datatype='GPLong',                               parameterType='Optional',                               direction='Input',                               multiValue=False)        par_end_year.filter.type = 'ValueList'        par_end_year.filter.list = []                        # set analysis type        par_analysis_type = arcpy.Parameter(                              displayName='Trend analysis method',                              name='in_analysis_type',                              datatype='GPString',                              parameterType='Required',                              direction='Input',                              multiValue=False)        par_analysis_type.filter.type = 'ValueList'        par_analysis_type.filter.list = ['Mann-Kendall', 'Theilsen Slope']        par_analysis_type.value = 'Mann-Kendall'                # mk p-value        par_mk_pvalue = arcpy.Parameter(                          displayName='P-value',                          name='in_mk_pvalue',                          datatype='GPDouble',                          parameterType='Optional',                          direction='Input',                          multiValue=False)        par_mk_pvalue.filter.type = 'Range'        par_mk_pvalue.filter.list = [0.001, 0.5]        par_mk_pvalue.value = 0.05        # mk direction        par_mk_direction = arcpy.Parameter(                            displayName='Trend direction',                            name='in_mk_direction',                            datatype='GPString',                            parameterType='Optional',                            direction='Input',                            multiValue=False)        par_mk_direction.filter.type = 'ValueList'        par_mk_direction.filter.list = ['Both', 'Increasing', 'Decreasing']        par_mk_direction.value = 'Both'                # ts alpha        par_ts_alpha = arcpy.Parameter(                         displayName='Alpha',                         name='in_ts_alpha',                         datatype='GPDouble',                         parameterType='Optional',                         direction='Input',                         multiValue=False)        par_ts_alpha.filter.type = 'Range'        par_ts_alpha.filter.list = [0.001, 0.999]        par_ts_alpha.value = 0.95                # add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True                # combine parameters        parameters = [            par_like_nc_file,            par_mask_nc_file,            par_out_nc_file,            par_use_all_years,            par_start_year,            par_end_year,            par_analysis_type,            par_mk_pvalue,            par_mk_direction,            par_ts_alpha,            par_add_result_to_map        ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # imports        try:            import numpy as np            import xarray as xr                   except:            arcpy.AddError('Python libraries xarray not installed.')            return                # globals         global GDVSPECTRA_TREND        # unpack global parameter values         curr_file = GDVSPECTRA_TREND.get('in_file')                # if input file added, run        if parameters[0].value is not None:            # if global has no matching file (or first run), reload all            if curr_file != parameters[0].valueAsText:                try:                    ds = xr.open_dataset(parameters[0].valueAsText)                    dts = np.unique(ds['time.year']).tolist()                    ds.close()                except:                    dts = []                # populate start and end year lists                parameters[4].filter.list = dts                parameters[5].filter.list = dts                                # reset start and end year selections                parameters[4].value = dts[0]                parameters[5].value = dts[-1]                        # update global values        GDVSPECTRA_TREND = {            'in_file': parameters[0].valueAsText,        }                # enable start and end years based on all years checkbox         if parameters[3].value is False:            parameters[4].enabled = True            parameters[5].enabled = True        else:            parameters[4].enabled = False            parameters[5].enabled = False        # enable relevant controls when manken or theils        if parameters[6].valueAsText == 'Mann-Kendall':            parameters[7].enabled = True            parameters[8].enabled = True            parameters[9].enabled = False        else:            parameters[7].enabled = False            parameters[8].enabled = False            parameters[9].enabled = True        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the GDV Spectra Trend module.        """                # safe imports        import os, sys                           # arcgis comes with these        import datetime                          # arcgis comes with this        import numpy as np                       # arcgis comes with this        import scipy                             # arcgis comes with this        import arcpy                             # arcgis comes with this        from tempfile import NamedTemporaryFile  # arcgis comes with this                # risky imports (not native to arcgis)        try:            import xarray as xr            import dask        except Exception as e:            arcpy.AddError('Python libraries xarray and dask not installed.')            arcpy.AddMessage(str(e))            return                        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                    # module folder            sys.path.append(FOLDER_MODULES)            import gdvspectra         except Exception as e:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            arcpy.AddMessage(str(e))            return                    # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        warnings.simplefilter(action='ignore', category=dask.array.core.PerformanceWarning)                    # grab parameter values         in_like_nc = parameters[0].valueAsText         # likelihood netcdf        in_mask_nc = parameters[1].valueAsText         # thresh mask netcdf        out_nc = parameters[2].valueAsText             # output netcdf        in_use_all_years = parameters[3].value         # use all years        in_start_year = parameters[4].value            # start year        in_end_year = parameters[5].value              # end year        in_trend_method = parameters[6].value          # trend method        in_mk_pvalue = parameters[7].value             # mk pvalue        in_mk_direction = parameters[8].value          # mk direction        in_ts_alpha = parameters[9].value              # ts alpha        in_add_result_to_map = parameters[10].value    # add result to map                        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning GDVSpectra Trend.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=6)                        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading and checking likelihood netcdf...')        arcpy.SetProgressorPosition(1)                try:            # do quick lazy load of likelihood netcdf for checking            ds_like = xr.open_dataset(in_like_nc)        except Exception as e:            arcpy.AddWarning('Could not quick load input likelihood NetCDF data.')            arcpy.AddMessage(str(e))            return                # check xr type, vars, coords, dims, attrs        if not isinstance(ds_like, xr.Dataset):            arcpy.AddError('Input likelihood NetCDF must be a xr dataset.')            return        elif len(ds_like) == 0:            arcpy.AddError('Input likelihood NetCDF has no data/variables/bands.')            return        elif 'x' not in ds_like.dims or 'y' not in ds_like.dims or 'time' not in ds_like.dims:            arcpy.AddError('Input likelihood NetCDF must have x, y and time dimensions.')            return        elif 'x' not in ds_like.coords or 'y' not in ds_like.coords or 'time' not in ds_like.coords:            arcpy.AddError('Input likelihood NetCDF must have x, y and time coords.')            return        elif 'spatial_ref' not in ds_like.coords:            arcpy.AddError('Input likelihood NetCDF must have a spatial_ref coord.')            return        elif len(ds_like['x']) == 0 or len(ds_like['y']) == 0:            arcpy.AddError('Input likelihood NetCDF must have at least one x, y index.')            return                elif len(ds_like['time']) < 3:            arcpy.AddError('Input likelihood NetCDF must have 3 or more times.')            return                            elif 'like' not in ds_like:            arcpy.AddError('Input likelihood NetCDF must have a "like" variable. Run GDVSpectra Likelihood.')            return        elif not hasattr(ds_like, 'time.year') or not hasattr(ds_like, 'time.month'):            arcpy.AddError('Input likelihood NetCDF must have time with year and month index.')            return        elif ds_like.attrs == {}:            arcpy.AddError('Input likelihood NetCDF attributes not found. NetCDF must have attributes.')            return        elif not hasattr(ds_like, 'crs'):            arcpy.AddError('Input likelihood NetCDF CRS attribute not found. CRS required.')            return        elif ds_like.crs != 'EPSG:3577':            arcpy.AddError('Input likelihood NetCDF CRS is not EPSG:3577. EPSG:3577 required.')                        return         elif not hasattr(ds_like, 'nodatavals'):            arcpy.AddError('Input likelihood NetCDF nodatavals attribute not found.')                        return         # check if variables (should only be like) are empty        if ds_like['like'].isnull().all() or (ds_like['like'] == 0).all():            arcpy.AddError('Input likelihood NetCDF "like" variable is empty. Please download again.')                        return         try:            # now, do proper open of likelihood netcdf (set nodata to nan)            ds_like = satfetcher.load_local_nc(nc_path=in_like_nc,                                                use_dask=True,                                                conform_nodata_to=np.nan)        except Exception as e:            arcpy.AddError('Could not properly load input likelihood NetCDF data.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Getting NetCDF attributes...')        arcpy.SetProgressorPosition(2)                    # get attributes from dataset        ds_attrs = ds_like.attrs        ds_band_attrs = ds_like['like'].attrs        ds_spatial_ref_attrs = ds_like['spatial_ref'].attrs                     # remove potential datetime duplicates, if time exists        ds_like = satfetcher.group_dupe_times(ds_like)                                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Reducing dataset based on time, if requested...')        arcpy.SetProgressorPosition(3)                # if requested...        if in_use_all_dates is False:                    # check start, end year valid            if in_start_year is None or in_end_year is None:                arcpy.AddError('Did not specify start and/or end years.')                return                     elif in_start_year >= in_end_year:                arcpy.AddError('Start year must be < end year.')                return                            # check if both years in dataset            years = ds_like['time.year']            if in_start_year not in years or in_end_year not in years:                arcpy.AddError('Start year is not in likelihood NetCDF.')                return                            # subset likelihood dataset based on times             years = [in_start_year, in_end_year]            ds_like = ds_like.where(ds_like['time.year'].isin(years), drop=True)                        # check if more than three years still exist             if len(ds_like['time']) < 3:                arcpy.AddError('Subset of likelihood NetCDF resulted in < 3 years of data.')                return                        # check if variables (should only be like) are empty            if ds_like['like'].isnull().all() or (ds_like['like'] == 0).all():                arcpy.AddError('Subset of likelihood NetCDF is empty. Increase year range.')                            return                                                        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Computing data into memory, please wait...')        arcpy.SetProgressorPosition(4)        # compute likelihood        ds_like = ds_like.compute()                # check if we still have values        if ds_like.to_array().isnull().all():            arcpy.AddError('Input likelihood NetCDF is empty. Please download again.')                        return                     # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading, checking and applying mask, if requested...')        arcpy.SetProgressorPosition(5)        # if requested...        if in_mask_nc is not None:                    try:                # do quick lazy load of mask netcdf for checking                ds_mask = xr.open_dataset(in_mask_nc)            except Exception as e:                arcpy.AddWarning('Could not quick load input mask NetCDF data.')                arcpy.AddMessage(str(e))                return                        # check xr type, vars, coords, dims, attrs            if not isinstance(ds_mask, xr.Dataset):                arcpy.AddError('Input mask NetCDF must be a xr dataset.')                return            elif len(ds_mask) == 0:                arcpy.AddError('Input mask NetCDF has no data/variables/bands.')                return            elif 'x' not in ds_mask.dims or 'y' not in ds_mask.dims:                arcpy.AddError('Input mask NetCDF must have x, y dimensions.')                return            elif 'x' not in ds_mask.coords or 'y' not in ds_mask.coords:                arcpy.AddError('Input mask NetCDF must have x, y and time coords.')                return            elif 'spatial_ref' not in ds_mask.coords:                arcpy.AddError('Input mask NetCDF must have a spatial_ref coord.')                return            elif len(ds_mask['x']) == 0 or len(ds_mask['y']) == 0:                arcpy.AddError('Input mask NetCDF must have at least one x, y index.')                return                    elif 'time' in ds_mask:                arcpy.AddError('Input mask NetCDF must not have a time dimension.')                return                                elif 'thresh' not in ds_mask:                arcpy.AddError('Input mask NetCDF must have a "thresh" variable. Run GDVSpectra Threshold.')                return            elif ds_mask.attrs == {}:                arcpy.AddError('Input mask NetCDF attributes not found. NetCDF must have attributes.')                return            elif not hasattr(ds_mask, 'crs'):                arcpy.AddError('Input mask NetCDF CRS attribute not found. CRS required.')                return            elif ds_mask.crs != 'EPSG:3577':                arcpy.AddError('Input mask NetCDF CRS is not EPSG:3577. EPSG:3577 required.')                            return             elif not hasattr(ds_mask, 'nodatavals'):                arcpy.AddError('Input mask NetCDF nodatavals attribute not found.')                            return             # check if variables (should only be thresh) are empty            if ds_mask['thresh'].isnull().all() or (ds_mask['thresh'] == 0).all():                arcpy.AddError('Input mask NetCDF "thresh" variable is empty. Please download again.')                            return             try:                # now, do proper open of mask netcdf (set nodata to nan)                ds_mask = satfetcher.load_local_nc(nc_path=in_mask_nc,                                                    use_dask=True,                                                    conform_nodata_to=np.nan)                                                                   # compute it!                ds_mask = ds_mask.compute()                            except Exception as e:                arcpy.AddError('Could not properly load input mask NetCDF data.')                arcpy.AddMessage(str(e))                return                            try:                # check if like and mask datasets overlap                 intersect = tools.all_xr_intersect([ds_like, ds_mask])                if not intersect:                    arcpy.AddError('Input datasets do not intersect.')                    arcpy.AddMessage(str(e))                    return                                # resample mask dataset to match likelihood                 ds_mask = tools.resample_xr(ds_from=ds_mask,                                             ds_to=ds_like,                                            resampling='nearest')                # squeeze                ds_mask = ds_mask.squeeze(drop=True)                            except Exception as e:                arcpy.AddError('Could not intersect input datasets.')                arcpy.AddMessage(str(e))                return                            # we made it, so mask likelihood            ds_like = ds_like.where(~ds_mask.isnull())            #ds_like = ds_like.squeeze(drop=True)                        # check if variables (should only be thresh) are empty            if ds_like['like'].isnull().all() or (ds_like['like'] == 0).all():                arcpy.AddError('Masked likelihood dataset is empty.')                            return                         # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Performing trend analysis, this can take awhile...')        arcpy.SetProgressorPosition(6)                # check if trend method is supported         if in_trend_method not in ['Mann-Kendall', 'Theilsen Slope']:            arcpy.AddError('Trend method not supported.')                        return                # check and perform mann-kendall or theil sen        if in_trend_method == 'Mann-Kendall':            # check mannkendall pvalue             if in_mk_pvalue is None:                arcpy.AddError('Mann-Kendall p-value not provided.')                            return            elif in_mk_pvalue < 0.001 or in_mk_pvalue > 0.5:                arcpy.AddError('Mann-Kendall p-value must be between 0.001 and 0.5.')                            return                            # check mannkendall direction             if in_mk_direction not in ['Increasing', 'Decreasing', 'Both']:                arcpy.AddError('Mann-Kendall direction not supported.')                            return                            # prepare mannkendal direction (must be inc, dec or both)            if in_mk_direction in ['Increasing', 'Decreasing']:                in_mk_direction = in_mk_direction.lower()[:3]            else:                in_mk_direction = 'both'                            try:                # perform mann-kendall trend analysis                ds_trend = gdvspectra.perform_mk_original(ds=ds_like,                                                           pvalue=in_mk_pvalue,                                                           direction=in_mk_direction)            except Exception as e:                arcpy.AddError('Could not perform Mann-Kendall trend analysis.')                arcpy.AddMessage(str(e))                return            else:            # check theil sen alpha             if in_ts_alpha is None:                arcpy.AddError('Theil-sen Alpha not provided.')                            return            elif in_ts_alpha < 0 or in_ts_alpha > 1:                arcpy.AddError('Theil-sen Alpha must be between 0 and 1.')                            return            try:                # perform theil-sen trend analysis                ds_trend = gdvspectra.perform_theilsen_slope(ds=ds_like,                                                              alpha=in_ts_alpha)            except Exception as e:                arcpy.AddError('Could not perform Theil-sen trend analysis.')                arcpy.AddMessage(str(e))                return            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(7)                # append attrbutes on to dataset and bands        ds_trend.attrs = ds_attrs        ds_trend['spatial_ref'].attrs = ds_spatial_ref_attrs        for var in ds_trend:            ds_trend[var].attrs = ds_band_attrs                                    # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(8)           try:            # export netcdf file            tools.export_xr_as_nc(ds=ds_trend, filename=out_nc)        except Exception as e:            arcpy.AddError('Could not export dataset.')            arcpy.AddMessage(str(e))            return                    # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Adding output to map, if requested...')        arcpy.SetProgressorPosition(9)        # if requested...        if in_add_result_to_map:            try:                # for current project, open current map                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap                                # remove trend layer if already exists                 for layer in m.listLayers():                    if layer.name == 'trend.crf':                        m.removeLayer(layer)                # create output folder using datetime as name                dt = datetime.datetime.now().strftime('%d%m%Y%H%M%S')                out_folder = os.path.join(os.path.dirname(out_nc), 'trend' + '_' + dt)                os.makedirs(out_folder)                                # disable visualise on map temporarily                arcpy.env.addOutputsToMap = False                                # create crf filename and copy it                out_file = os.path.join(out_folder, 'trend.crf')                crf = arcpy.CopyRaster_management(in_raster=out_nc,                                                   out_rasterdataset=out_file)                                                                  # add to map                                  m.addDataFromPath(crf)              except Exception as e:                arcpy.AddWarning('Could not visualise output, aborting visualisation.')                arcpy.AddMessage(str(e))                pass                        try:                # get symbology, update it                layer = m.listLayers('trend.crf')[0]                sym = layer.symbology                # if layer has stretch coloriser, apply color                if hasattr(sym, 'colorizer'):                    # apply percent clip type                    sym.colorizer.stretchType = 'PercentClip'                    sym.colorizer.minPercent = 0.001                    sym.colorizer.maxPercent = 0.999                    # set default trend cmap, override if mannkenn inc or dec used                    cmap = aprx.listColorRamps('Red-Blue (Continuous)')[0]                    if in_trend_method == 'Mann-Kendall':                        if in_mk_direction == 'inc':                            cmap = aprx.listColorRamps('Yellow-Green-Blue (Continuous)')[0]                        elif in_mk_direction == 'dec':                            cmap = aprx.listColorRamps('Yellow-Orange-Red (Continuous)')[0]                                            # apply colormap                    sym.colorizer.colorRamp = cmap                    # invert colormap if mannkenn decline                     if in_trend_method == 'Mann-Kendall' and in_mk_direction == 'dec':                        sym.colorizer.invertColorRamp = True                    else:                        sym.colorizer.invertColorRamp = False                                        # set gamma                    sym.colorizer.gamma = 1.0                    # update symbology                    layer.symbology = sym              except Exception as e:                arcpy.AddWarning('Could not colorise output, aborting colorisation.')                arcpy.AddMessage(str(e))                pass        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(10)                # close likelihood dataset        ds_like.close()        del ds_like                # close trend dataset        ds_trend.close()         del ds_trend                # close mask (if exists)        if in_mask_nc is not None:            ds_mask.close()            del ds_mask                    # notify user        arcpy.AddMessage('Generated GDV Trend successfully.')                          returnclass GDVSpectra_CVA(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = 'GDVSpectra CVA'        self.description = 'Perform a Change Vector Analysis (CVA) on a data cube.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """                # input netcdf file        par_raw_nc_path = arcpy.Parameter(                            displayName='Input satellite NetCDF file',                            name='in_raw_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Input')        par_raw_nc_path.filter.list = ['nc']                # input netcdf mask (thresh) file        par_mask_nc_path = arcpy.Parameter(                             displayName='Input GDV Threshold mask NetCDF file',                             name='in_mask_nc_path',                             datatype='DEFile',                             parameterType='Optional',                             direction='Input')        par_mask_nc_path.filter.list = ['nc']                # output folder location        par_out_nc_path = arcpy.Parameter(                            displayName='Output CVA NetCDF file',                            name='out_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_path.filter.list = ['nc']                # base start year        par_base_start_year = arcpy.Parameter(                                displayName='Baseline start year',                                name='in_base_start_year',                                datatype='GPLong',                                parameterType='Required',                                direction='Input',                                multiValue=False)        par_base_start_year.filter.type = 'ValueList'        par_base_start_year.filter.list = []                # base end year        par_base_end_year = arcpy.Parameter(                              displayName='Baseline end year',                              name='in_base_end_year',                              datatype='GPLong',                              parameterType='Required',                              direction='Input',                              multiValue=False)        par_base_end_year.filter.type = 'ValueList'        par_base_end_year.filter.list = []        # comparison start year        par_comp_start_year = arcpy.Parameter(                                displayName='Comparison start year',                                name='in_comp_start_year',                                datatype='GPLong',                                parameterType='Required',                                direction='Input',                                multiValue=False)        par_comp_start_year.filter.type = 'ValueList'        par_comp_start_year.filter.list = []                # comparison end year        par_comp_end_year = arcpy.Parameter(                              displayName='Comparison end year',                              name='in_comp_end_year',                              datatype='GPLong',                              parameterType='Required',                              direction='Input',                              multiValue=False)        par_comp_end_year.filter.type = 'ValueList'        par_comp_end_year.filter.list = []        # analysis months        par_analysis_months = arcpy.Parameter(                                displayName='Set analysis month(s)',                                name='in_analysis_months',                                datatype='GPLong',                                parameterType='Required',                                direction='Input',                                multiValue=True)        par_analysis_months.filter.type = 'ValueList'        par_analysis_months.filter.list = [m for m in range(1, 13)]        par_analysis_months.value = [9, 10, 11]        # cva magnitude threshold         par_tmf = arcpy.Parameter(                    displayName='Magnitude threshold',                    name='in_tmf',                    datatype='GPDouble',                    parameterType='Required',                    direction='Input',                    multiValue=False)        par_tmf.filter.type = 'Range'        par_tmf.filter.list = [0.0, 100.0]          par_tmf.value = 2.0          # set q upper for standardisation        par_ivt_qupper = arcpy.Parameter(                           displayName='Upper percentile',                           name='in_stand_qupper',                           datatype='GPDouble',                           parameterType='Required',                           direction='Input',                           category='Invariant Standardisation',                           multiValue=False)        par_ivt_qupper.filter.type = 'Range'        par_ivt_qupper.filter.list = [0.0, 1.0]        par_ivt_qupper.value = 0.99                         # set q lower for standardisation        par_ivt_qlower = arcpy.Parameter(                           displayName='Lower percentile',                           name='in_stand_qlower',                           datatype='GPDouble',                           parameterType='Required',                           direction='Input',                           category='Invariant Standardisation',                           multiValue=False)        par_ivt_qlower.filter.type = 'Range'        par_ivt_qlower.filter.list = [0.0, 1.0]        par_ivt_qlower.value = 0.05                     # set oa class values        par_fmask_flags = arcpy.Parameter(displayName='Include pixel flags',                            name='in_fmask_flags',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Satellite Quality Options',                            multiValue=True)        flags = [            'NoData',             'Valid',             'Cloud',             'Shadow',             'Snow',             'Water'            ]        par_fmask_flags.filter.type = 'ValueList'                par_fmask_flags.filter.list = flags        par_fmask_flags.values = ['Valid', 'Snow', 'Water']                # max cloud cover        par_max_cloud = arcpy.Parameter(                          displayName='Maximum cloud cover',                          name='in_max_cloud',                          datatype='GPDouble',                          parameterType='Required',                          direction='Input',                          category='Satellite Quality Options',                          multiValue=False)        par_max_cloud.filter.type = 'Range'        par_max_cloud.filter.list = [0.0, 100.0]        par_max_cloud.value = 10.0                # interpolate        par_interpolate = arcpy.Parameter(                            displayName='Interpolate NoData pixels',                            name='in_interpolate',                            datatype='GPBoolean',                            parameterType='Required',                            direction='Input',                            category='Satellite Quality Options',                            multiValue=False)        par_interpolate.value = True                # add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True                # combine parameters        parameters = [            par_raw_nc_path,            par_mask_nc_path,            par_out_nc_path,            par_base_start_year,            par_base_end_year,            par_comp_start_year,            par_comp_end_year,            par_analysis_months,            par_tmf,            par_ivt_qupper,            par_ivt_qlower,            par_fmask_flags,            par_max_cloud,            par_interpolate,            par_add_result_to_map            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # imports        try:            import numpy as np            import xarray as xr        except:            arcpy.AddError('Python library xarray not installed.')            return                    # globals         global GDVSPECTRA_CVA        # unpack global parameter values         curr_file = GDVSPECTRA_CVA.get('in_file')                        # if input file added, run        if parameters[0].value is not None:                    # if global has no matching file (or first run), reload all            if curr_file != parameters[0].valueAsText:                try:                    ds = xr.open_dataset(parameters[0].valueAsText)                    dts = np.unique(ds['time.year']).tolist()                    ds.close()                except:                    dts = []                # populate baseline start and end year lists                parameters[3].filter.list = dts                parameters[4].filter.list = dts                                # reset baseline start and end year selections                parameters[4].value = dts[0]                parameters[5].value = dts[0]                                # populate comparison start and end year lists                parameters[5].filter.list = dts                parameters[6].filter.list = dts                # reset comparison start and end year selections                parameters[5].value = dts[-1]                parameters[6].value = dts[-1]                        # update global values        GDVSPECTRA_CVA = {            'in_file': parameters[0].valueAsText,        }                    return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the GDV Spectra CVA module.        """                # safe imports        import os, sys        # arcgis comes with these        import datetime       # arcgis comes with these        import numpy as np    # arcgis comes with these        import pandas as pd   # arcgis comes with these                # risky imports (not native to arcgis)        try:            import xarray as xr            import dask        except Exception as e:            arcpy.AddError('Python libraries xarray and dask not installed.')            arcpy.AddMessage(str(e))            return                        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                    # module folder            sys.path.append(FOLDER_MODULES)            import gdvspectra, cog         except Exception as e:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            arcpy.AddMessage(str(e))            return                    # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        warnings.simplefilter(action='ignore', category=dask.array.core.PerformanceWarning)                # grab parameter values         in_raw_nc = parameters[0].valueAsText               # raw input satellite netcdf        in_mask_nc = parameters[1].valueAsText              # mask input satellite netcdf        out_nc = parameters[2].valueAsText                  # output gdv likelihood netcdf        in_base_start_year =  parameters[3].value           # base start year        in_base_end_year =  parameters[4].value             # base end year        in_comp_start_year =  parameters[5].value           # comp start year        in_comp_end_year =  parameters[6].value             # comp end year        in_analysis_months = parameters[7].valueAsText      # analysis months        in_tmf = parameters[8].value                        # magnitude threshold        in_ivt_qupper = parameters[9].value                 # upper quantile for standardisation        in_ivt_qlower = parameters[10].value                # lower quantile for standardisation        in_fmask_flags = parameters[11].valueAsText         # fmask flag values        in_max_cloud = parameters[12].value                 # max cloud percentage        in_interpolate = parameters[13].value               # interpolate missing pixels        in_add_result_to_map = parameters[14].value         # add result to map        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning GDVSpectra CVA.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=14)                                                                                            # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading and checking satellite netcdf...')        arcpy.SetProgressorPosition(1)                try:            # do quick lazy load of satellite netcdf for checking            ds = xr.open_dataset(in_raw_nc)        except Exception as e:            arcpy.AddWarning('Could not quick load input satellite NetCDF data.')            arcpy.AddMessage(str(e))            return                # check xr type, vars, coords, dims, attrs        if not isinstance(ds, xr.Dataset):            arcpy.AddError('Input satellite NetCDF must be a xr dataset.')            return        elif len(ds) == 0:            arcpy.AddError('Input NetCDF has no data/variables/bands.')            return        elif 'x' not in ds.dims or 'y' not in ds.dims or 'time' not in ds.dims:            arcpy.AddError('Input satellite NetCDF must have x, y and time dimensions.')            return        elif 'x' not in ds.coords or 'y' not in ds.coords or 'time' not in ds.coords:            arcpy.AddError('Input satellite NetCDF must have x, y and time coords.')            return        elif 'spatial_ref' not in ds.coords:            arcpy.AddError('Input satellite NetCDF must have a spatial_ref coord.')            return        elif len(ds['x']) == 0 or len(ds['y']) == 0 or len(ds['time']) == 0:            arcpy.AddError('Input satellite NetCDF must have all at least one x, y and time index.')            return        elif 'oa_fmask' not in ds and 'fmask' not in ds:            arcpy.AddError('Expected cloud mask band not found in satellite NetCDF.')            return        elif not hasattr(ds, 'time.year') or not hasattr(ds, 'time.month'):            arcpy.AddError('Input satellite NetCDF must have time with year and month component.')            return        elif len(ds.groupby('time.year')) < 2:            arcpy.AddError('Input satellite NetCDF must have >= 2 years of data.')            return        elif ds.attrs == {}:            arcpy.AddError('Satellite NetCDF must have attributes.')            return        elif not hasattr(ds, 'crs'):            arcpy.AddError('Satellite NetCDF CRS attribute not found. CRS required.')            return        elif ds.crs != 'EPSG:3577':            arcpy.AddError('Satellite NetCDF CRS is not in GDA94 Albers (EPSG:3577).')                        return         elif not hasattr(ds, 'nodatavals'):            arcpy.AddError('Satellite NetCDF nodatavals attribute not found.')                        return                     # efficient: if all nan, 0 at first var, assume rest same, so abort        if ds[list(ds)[0]].isnull().all() or (ds[list(ds)[0]] == 0).all():            arcpy.AddError('Satellite NetCDF has empty variables. Please download again.')                        return                     try:            # now, do proper open of satellite netcdf properly (and set nodata to nan)            ds = satfetcher.load_local_nc(nc_path=in_raw_nc,                                           use_dask=True,                                           conform_nodata_to=np.nan)        except Exception as e:            arcpy.AddError('Could not properly load input satellite NetCDF data.')            arcpy.AddMessage(str(e))            return                                            # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Getting NetCDF attributes...')        arcpy.SetProgressorPosition(2)        # get attributes from dataset        ds_attrs = ds.attrs        ds_band_attrs = ds[list(ds)[0]].attrs        ds_spatial_ref_attrs = ds['spatial_ref'].attrs        # remove potential datetime duplicates        ds = satfetcher.group_dupe_times(ds)        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Removing invalid pixels and empty dates...')        arcpy.SetProgressorPosition(3)                  # convert fmask as text to numeric code equivalents              in_fmask_flags = [e for e in in_fmask_flags.split(';')]                in_fmask_flags = arc.convert_fmask_codes(in_fmask_flags)                # check if flags selected, if not, select all         if len(in_fmask_flags) == 0:            arcpy.AddWarning('No flags set, selecting default')            in_fmask_flags = [1, 4, 5]                # check numeric flags are valid         for flag in in_fmask_flags:            if flag not in [0, 1, 2, 3, 4, 5, 6]:                arcpy.AddError('Pixel flag not supported.')                return                # check if duplicate flags         u, c = np.unique(in_fmask_flags, return_counts=True)        if len(u[c > 1]) > 0:            arcpy.AddError('Duplicate pixel flags detected.')            return                    # check if mask band exists        mask_band = arc.get_name_of_mask_band(list(ds))                try:            # remove invalid pixels and empty scenes            ds = cog.remove_fmask_dates(ds=ds,                                         valid_class=in_fmask_flags,                                         max_invalid=in_max_cloud,                                         mask_band=mask_band,                                         nodata_value=np.nan,                                         drop_fmask=True)        except Exception as e:            arcpy.AddError('Could not cloud mask pixels.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Conforming satellite band names...')        arcpy.SetProgressorPosition(4)                        try:            # get platform name from attributes, error if no attributes            in_platform = arc.get_platform_from_dea_attrs(ds_attrs)            # conform dea aws band names based on platform            ds = satfetcher.conform_dea_ard_band_names(ds=ds,                                                        platform=in_platform.lower())           except Exception as e:             arcpy.AddError('Could not get platform from attributes.')            arcpy.AddMessage(str(e))            return        # check if all expected bands are in dataset         for band in ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']:            if band not in ds:                arcpy.AddError('Satellite NetCDF is missing band: {}. Need all bands.'.format(band))                return                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Reducing dataset months to requested...')        arcpy.SetProgressorPosition(5)        # prepare analysis month(s)        if in_analysis_months == '':            arcpy.AddError('Must include at least one analysis month.')            return                    # unpack month(s)        analysis_months = [int(e) for e in in_analysis_months.split(';')]        try:            # reduce xr dataset into only wet, dry months (force rechunk via time)            ds = gdvspectra.subset_months(ds=ds.chunk({'time': -1}),                                           month=analysis_months,                                          inplace=True)        except Exception as e:             arcpy.AddError('Could not subset Satellite NetCDF by analysis months.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Calculating tasselled cap vegetation index...')        arcpy.SetProgressorPosition(6)                 try:            # calculate tasselled cap green and bare            ds = tools.calculate_indices(ds=ds,                                          index=['tcg', 'tcb'],                                          rescale=False,                                          drop=True)            # add band attrs back on            ds['tcg'].attrs = ds_band_attrs               ds['tcb'].attrs = ds_band_attrs        except Exception as e:             arcpy.AddError('Could not calculate tasselled cap index.')            arcpy.AddMessage(str(e))            return        # check once again: if all nan, 0, abort        if ds['tcg'].isnull().all() or ds['tcb'].isnull().all():            arcpy.AddError('Satellite NetCDF has empty variables. Please download again.')                        return         # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Reducing month(s) into annual medians...')        arcpy.SetProgressorPosition(7)                # reduce months into annual medians (year starts, YS)        try:            ds = gdvspectra.resample_to_freq_medians(ds=ds,                                                     freq='YS',                                                     inplace=True)                                                                 # add band attrs back on            ds['tcg'].attrs = ds_band_attrs               ds['tcb'].attrs = ds_band_attrs                    except Exception as e:             arcpy.AddError('Could not resample months in Satellite NetCDF.')            arcpy.AddMessage(str(e))            return                    # again: if all nan, 0, abort        if ds['tcg'].isnull().all() or ds['tcb'].isnull().all():            arcpy.AddError('Satellite NetCDF has empty variables. Please download again.')                        return                                            # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Interpolating dataset, if requested...')        arcpy.SetProgressorPosition(7)         # if requested...        if in_interpolate:            try:                # interpolate along time dimension (linear)                ds = tools.perform_interp(ds=ds, method='full')            except Exception as e:                 arcpy.AddError('Could not interpolate satellite NetCDF.')                arcpy.AddMessage(str(e))                return                        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Computing data into memory, please wait...')        arcpy.SetProgressorPosition(8)        # compute!         ds = ds.compute()        # check if all nan again        if ds.to_array().isnull().all():            arcpy.AddError('Satellite NetCDF is empty. Please download again.')                        return         # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Standardising data to invariant targets...')        arcpy.SetProgressorPosition(12)        # check upper quantile        if in_ivt_qlower < 0 or in_ivt_qlower >= 0.5:            arcpy.AddMessage('Lower quantile must be between 0, 0.5. Setting to default.')            in_ivt_qlower = 0.05        # do same for upper quantile        if in_ivt_qupper <= 0.5 or in_ivt_qupper > 1.0:            arcpy.AddMessage('Upper quantile must be between 0.5, 1.0. Setting to default.')            in_ivt_qlower = 0.99         # check if upper <= lower         if in_ivt_qupper <= in_ivt_qlower:            arcpy.AddError('Upper quantile must be > than lower quantile value.')            return        try:            # standardise to targets            ds = gdvspectra.standardise_to_targets(ds,                                                    q_upper=in_ivt_qupper,                                                    q_lower=in_ivt_qlower)        except Exception as e:             arcpy.AddError('Could not standardise satellite data to invariant targets.')            arcpy.AddMessage(str(e))            return                    # final check to see if data exists        if ds['tcg'].isnull().all() or ds['tcb'].isnull().all():            arcpy.AddError('Could not standardise satellite NetCDF.')            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Performing CVA...')        arcpy.SetProgressorPosition(9)                # check baseline and comparison start and end years        if in_base_end_year < in_base_start_year:            arcpy.AddError('Baseline end year must not be < start year.')            return        elif in_comp_end_year < in_comp_start_year:            arcpy.AddError('Comparison end year must not be < start year.')            return        elif in_comp_start_year < in_base_start_year:            arcpy.AddError('Comparison start year must not be < baseline start year.')            return                # check if baseline and comparison years in dataset        years = ds['time.year']        if in_base_start_year not in years or in_base_end_year not in years:            arcpy.AddError('Baseline start and end years not found in dataset.')            return        elif in_comp_start_year not in years or in_comp_end_year not in years:            arcpy.AddError('Comparison start and end years not found in dataset.')            return                    # check magnitude value         if in_tmf < 0 or in_tmf > 100:            arcpy.AddError('CVA threshold magnitude must be between 0 and 100.')            return                try:            # generate cva            ds_cva = gdvspectra.perform_cva(ds=ds,                                            base_times=(in_base_start_year, in_base_end_year),                                            comp_times=(in_comp_start_year, in_comp_end_year),                                            reduce_comp=False,                                            vege_var = 'tcg',                                            soil_var = 'tcb',                                            tmf=in_tmf)        except Exception as e:             arcpy.AddError('Could not perform CVA.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Isolating CVA magnitude quartiles...')        arcpy.SetProgressorPosition(10)                                                        try:            # isolate cva magnitude via angle quartiles            ds_cva = gdvspectra.isolate_cva_quarters(ds=ds_cva,                                                      drop_orig_vars=True)        except Exception as e:             arcpy.AddError('Could not isolate CVA quartiles.')            arcpy.AddMessage(str(e))            return                                                                                     # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading, checking and applying mask, if requested...')        arcpy.SetProgressorPosition(5)        # if requested...        if in_mask_nc is not None:                    try:                # do quick lazy load of mask netcdf for checking                ds_mask = xr.open_dataset(in_mask_nc)            except Exception as e:                arcpy.AddWarning('Could not quick load input mask NetCDF data.')                arcpy.AddMessage(str(e))                return                    # check xr type, vars, coords, dims, attrs            if not isinstance(ds_mask, xr.Dataset):                arcpy.AddError('Input mask NetCDF must be a xr dataset.')                return            elif len(ds_mask) == 0:                arcpy.AddError('Input mask NetCDF has no data/variables/bands.')                return            elif 'x' not in ds_mask.dims or 'y' not in ds_mask.dims:                arcpy.AddError('Input mask NetCDF must have x, y dimensions.')                return            elif 'x' not in ds_mask.coords or 'y' not in ds_mask.coords:                arcpy.AddError('Input mask NetCDF must have x, y and time coords.')                return            elif 'spatial_ref' not in ds_mask.coords:                arcpy.AddError('Input mask NetCDF must have a spatial_ref coord.')                return            elif len(ds_mask['x']) == 0 or len(ds_mask['y']) == 0:                arcpy.AddError('Input mask NetCDF must have at least one x, y index.')                return                    elif 'time' in ds_mask:                arcpy.AddError('Input mask NetCDF must not have a time dimension.')                return                                elif 'thresh' not in ds_mask:                arcpy.AddError('Input mask NetCDF must have a "thresh" variable. Run GDVSpectra Threshold.')                return            elif ds_mask.attrs == {}:                arcpy.AddError('Input mask NetCDF attributes not found. NetCDF must have attributes.')                return            elif not hasattr(ds_mask, 'crs'):                arcpy.AddError('Input mask NetCDF CRS attribute not found. CRS required.')                return            elif ds_mask.crs != 'EPSG:3577':                arcpy.AddError('Input mask NetCDF CRS is not EPSG:3577. EPSG:3577 required.')                            return             elif not hasattr(ds_mask, 'nodatavals'):                arcpy.AddError('Input mask NetCDF nodatavals attribute not found.')                            return                     # check if variables (should only be thresh) are empty            if ds_mask['thresh'].isnull().all() or (ds_mask['thresh'] == 0).all():                arcpy.AddError('Input mask NetCDF "thresh" variable is empty. Please download again.')                            return                     try:                # now, do proper open of mask netcdf (set nodata to nan)                ds_mask = satfetcher.load_local_nc(nc_path=in_mask_nc,                                                    use_dask=True,                                                    conform_nodata_to=np.nan)                # compute it!                ds_mask = ds_mask.compute()            except Exception as e:                arcpy.AddError('Could not properly load input mask NetCDF data.')                arcpy.AddMessage(str(e))                return                            try:                # check if like and mask datasets overlap                 intersect = tools.all_xr_intersect([ds_cva, ds_mask])                if not intersect:                    arcpy.AddError('Input datasets do not intersect.')                    arcpy.AddMessage(str(e))                    return                # resample mask dataset to match likelihood                 ds_mask = tools.resample_xr(ds_from=ds_mask,                                             ds_to=ds_cva,                                            resampling='nearest')                # squeeze                ds_mask = ds_mask.squeeze(drop=True)            except Exception as e:                arcpy.AddError('Could not intersect input datasets.')                arcpy.AddMessage(str(e))                return                                # we made it, so mask cva            ds_cva = ds_cva.where(~ds_mask.isnull())            #ds_cva = ds_cva.squeeze(drop=True)            # check if variables are empty            if ds_cva.to_array().isnull().all():                arcpy.AddError('Masked CVA dataset is empty.')                            return                                                           # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(7)        # append attrbutes on to dataset and bands        ds_cva.attrs = ds_attrs        ds_cva['spatial_ref'].attrs = ds_spatial_ref_attrs        for var in ds_cva:            ds_cva[var].attrs = ds_band_attrs                                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(8)           try:            # export netcdf file            tools.export_xr_as_nc(ds=ds_cva, filename=out_nc)        except Exception as e:            arcpy.AddError('Could not export dataset.')            arcpy.AddMessage(str(e))            return                                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Adding output to map, if requested...')        arcpy.SetProgressorPosition(9)        # if requested...        if in_add_result_to_map:                        try:                # for current project, open current map                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap                                # remove cva layer if already exists                 for layer in m.listLayers():                    if layer.name == 'cva.crf':                        m.removeLayer(layer)                # create output folder using datetime as name                dt = datetime.datetime.now().strftime('%d%m%Y%H%M%S')                out_folder = os.path.join(os.path.dirname(out_nc), 'cva' + '_' + dt)                os.makedirs(out_folder)                                # disable visualise on map temporarily                arcpy.env.addOutputsToMap = False                                # create crf filename and copy it                out_file = os.path.join(out_folder, 'cva.crf')                crf = arcpy.CopyRaster_management(in_raster=out_nc,                                                   out_rasterdataset=out_file)                                                                  # add to map                                  m.addDataFromPath(crf)             except Exception as e:                arcpy.AddWarning('Could not visualise output, aborting visualisation.')                arcpy.AddMessage(str(e))                pass                        try:                # get symbology, update it                layer = m.listLayers('cva.crf')[0]                sym = layer.symbology                                # if layer has stretch coloriser, apply color                if hasattr(sym, 'colorizer'):                    if sym.colorizer.type == 'RasterStretchColorizer':                        # apply percent clip type                        sym.colorizer.stretchType = 'PercentClip'                        sym.colorizer.minPercent = 0.001                        sym.colorizer.maxPercent = 0.999                        # apply color map                        cmap = aprx.listColorRamps('Spectrum By Wavelength-Full Bright')[0]                        sym.colorizer.colorRamp = cmap                        # apply other basic options                        sym.colorizer.invertColorRamp = False                        sym.colorizer.gamma = 1.0                        # update symbology                        layer.symbology = sym            except Exception as e:                arcpy.AddWarning('Could not colorise output, aborting colorisation.')                arcpy.AddMessage(str(e))                pass        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(14)                # close satellite dataset        ds.close()        del ds                # close cva dataset        ds_cva.close()        del ds_cva                # close mask dataset, if exists         if in_mask_nc is not None:            ds_mask.close()            del ds_mask        # notify user        arcpy.AddMessage('Generated CVA successfully.')                          returnclass Phenolopy_Metrics(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = 'Phenolopy Metrics'        self.description = 'Calculate various metrics that describe various. ' \                           'aspects of vegetation phenology from a data cube. ' \                           'Key metrics include Peak of Season (POS), Start and ' \                           'End of Season (SOS, EOS), and various productivity metrics.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """                # input netcdf file        par_raw_nc_path = arcpy.Parameter(                            displayName='Input Satellite NetCDF file',                            name='in_raw_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Input')        par_raw_nc_path.filter.list = ['nc']                # output netcdf file        par_out_nc_path = arcpy.Parameter(                            displayName='Output Phenometrics Netcdf file',                            name='out_metrics_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_path.filter.list = ['nc']                # use all dates        par_use_all_dates = arcpy.Parameter(                              displayName='Use median of all input images',                              name='in_use_all_dates',                              datatype='GPBoolean',                              parameterType='Required',                              direction='Input',                              multiValue=False)        par_use_all_dates.value = True                # set specific year        par_specific_years = arcpy.Parameter(                              displayName='Set specific year(s) aggregate',                              name='in_specific_years',                              datatype='GPLong',                              parameterType='Optional',                              direction='Input',                              multiValue=True)        par_specific_years.filter.type = 'ValueList'        par_specific_years.filter.list = []        par_specific_years.value = None                # input vegetation index         par_veg_idx = arcpy.Parameter(                        displayName='Vegetation index',                        name='in_veg_idx',                        datatype='GPString',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_veg_idx.filter.type = 'ValueList'        par_veg_idx.filter.list = [            'NDVI',            'EVI',             'SAVI',            'MSAVI',            'SLAVI',            'MAVI',            'kNDVI',            'TCG'            ]        par_veg_idx.value = 'MAVI'                # input time-series resampling        par_resampling_interval = arcpy.Parameter(                                    displayName='Resampling interval',                                    name='in_resampling_interval',                                    datatype='GPString',                                    parameterType='Required',                                    direction='Input',                                    multiValue=False)        par_resampling_interval.filter.type = 'ValueList'        par_resampling_interval.filter.list = [            'Weekly',            'Bi-monthly',             'Monthly'            ]        par_resampling_interval.value = 'Bi-monthly'                      # input metrics        par_metrics = arcpy.Parameter(                        displayName='Phenological metrics',                        name='in_metrics',                        datatype='GPString',                        parameterType='Required',                        direction='Input',                        multiValue=True)        metrics = [            'POS: Peak of season',            'MOS: Middle of season',            'VOS: Valley of season',            'BSE: Base of season',            'AOS: Amplitude of season',            'SOS: Start of season',            'EOS: End of season',            'LOS: Length of season',            'ROI: Rate of increase',            'ROD: Rate of decrease',            'LIOS: Long integral of season',            'SIOS: Short integral of season',            'LIOT: Long integral of total',            'SIOT: Short integral of total'            ]        par_metrics.filter.type = 'ValueList'                par_metrics.filter.list = metrics        remove = [            'MOS: Middle of season',             'BSE: Base of season',             'AOS: Amplitude of season',            'LOS: Length of season'            ]        par_metrics.values = [m for m in metrics if m not in remove]                       # input calculate nos        par_calc_nos = arcpy.Parameter(                         displayName='Calculate NOS (Num of seasons)',                         name='in_calc_nos',                         datatype='GPBoolean',                         parameterType='Required',                         direction='Input',                         multiValue=False)        par_calc_nos.value = False        # input method type        par_method_type = arcpy.Parameter(                            displayName='Season detection method',                            name='in_method_type',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Phenolopy Options',                            multiValue=False)        par_method_type.filter.list = [            'First of slope',            'Median of slope',            'Absolute value',            'Seasonal amplitude',            'Relative amplitude'            ]        par_method_type.values = 'Median of slope'                # input peak detection type        par_peak_metric = arcpy.Parameter(                            displayName='Peak detection metric',                            name='in_peak_metric',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Phenolopy Options',                            multiValue=False)        par_peak_metric.filter.list = ['POS: Peak of season', 'MOS: Middle of season']        par_peak_metric.values = 'POS: Peak of season'            # input base detection type        par_base_metric = arcpy.Parameter(                            displayName='Base detection metric',                            name='in_base_metric',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Phenolopy Options',                            multiValue=False)        par_base_metric.filter.list = ['BSE: Base of season', 'VOS: Valley of season']        par_base_metric.values = 'VOS: Valley of season'                # input threshold side        par_threshold_side = arcpy.Parameter(                               displayName='Threshold side',                               name='in_threshold_side',                               datatype='GPString',                               parameterType='Required',                               direction='Input',                               category='Phenolopy Options',                               multiValue=False)        par_threshold_side.filter.list = ['One sided', 'Two sided']        par_threshold_side.values = 'Two sided'                # input seasonal amplitude factor. used only during  season amplitude method        par_seasonal_amp_factor = arcpy.Parameter(                                    displayName='Seasonal amplitude factor',                                    name='in_seasonal_amp_factor',                                    datatype='GPDouble',                                    parameterType='Optional',                                    direction='Input',                                    category='Phenolopy Options',                                    multiValue=False)        par_seasonal_amp_factor.filter.type = 'Range'        par_seasonal_amp_factor.filter.list = [0.0, 1.0]        par_seasonal_amp_factor.value = 0.5                # input absolute value. used only during absolute value method        par_absolute_value = arcpy.Parameter(                               displayName='Absolute value',                               name='in_absolute_value',                               datatype='GPDouble',                               parameterType='Optional',                               direction='Input',                               category='Phenolopy Options',                               multiValue=False)        par_absolute_value.value = 0.0        # input smooth method        par_smooth_method = arcpy.Parameter(                              displayName='Smoothing method',                              name='in_smooth_method',                              datatype='GPString',                              parameterType='Required',                              direction='Input',                              category='Smoothing Options',                              multiValue=False)        par_smooth_method.filter.list = ['Savitsky-Golay', 'Symmetrical Gaussian']        par_smooth_method.values = 'Savitsky-Golay'                 # input savitsky window length. only needed for savitsky        par_savitsky_window_length = arcpy.Parameter(                                       displayName='Window length',                                       name='in_savitsky_window_length',                                       datatype='GPLong',                                       parameterType='Required',                                       direction='Input',                                       category='Smoothing Options',                                       multiValue=False)        par_savitsky_window_length.filter.type = 'Range'        par_savitsky_window_length.filter.list = [3, 99]        par_savitsky_window_length.value = 3                 # input savitsky polyorder. only needed for savitsky        par_savitsky_polyorder = arcpy.Parameter(                                   displayName='Polyorder',                                   name='in_savitsky_polyorder',                                   datatype='GPLong',                                   parameterType='Required',                                   direction='Input',                                   category='Smoothing Options',                                   multiValue=False)        par_savitsky_polyorder.filter.type = 'Range'        par_savitsky_polyorder.filter.list = [1, 100]        par_savitsky_polyorder.value = 1                 # input symmetrical gaussian sigma. only needed for symm gauss        par_gaussian_sigma = arcpy.Parameter(                               displayName='Sigma',                               name='in_gaussian_sigma',                               datatype='GPLong',                               parameterType='Required',                               direction='Input',                               category='Smoothing Options',                               multiValue=False)        par_gaussian_sigma.filter.type = 'Range'        par_gaussian_sigma.filter.list = [1, 100]        par_gaussian_sigma.value = 1                        # input outlier removal method        par_outlier_method = arcpy.Parameter(                               displayName='Outlier removal method',                               name='in_outlier_method',                               datatype='GPString',                               parameterType='Required',                               direction='Input',                               category='Outlier Removal',                               multiValue=False)        par_outlier_method.filter.list = ['Local Median Threshold', 'Z-Score']        par_outlier_method.values = 'Local Median Threshold'                        # input user factor        par_user_factor = arcpy.Parameter(                            displayName='User factor',                            name='in_user_factor',                            datatype='GPLong',                            parameterType='Required',                            direction='Input',                            category='Outlier Removal',                            multiValue=False)        par_user_factor.filter.type = 'Range'        par_user_factor.filter.list = [1, 100]        par_user_factor.value = 2                                  # input zscore pvalue        par_zscore_pvalue = arcpy.Parameter(                              displayName='Zscore p-value',                              name='in_zscore_pvalue',                              datatype='GPDouble',                              parameterType='Required',                              direction='Input',                              category='Outlier Removal',                              multiValue=False)        par_zscore_pvalue.filter.type = 'Range'        par_zscore_pvalue.filter.list = [0.0001, 0.2]        par_zscore_pvalue.value = 0.05                      # input oa fmask         par_fmask_flags = arcpy.Parameter(                            displayName='Include flags',                            name='in_fmask_flags',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Satellite Quality Options',                            multiValue=True)        flags = ['NoData', 'Valid', 'Cloud', 'Shadow', 'Snow', 'Water']        par_fmask_flags.filter.type = 'ValueList'              par_fmask_flags.filter.list = flags        par_fmask_flags.values = ['Valid', 'Snow', 'Water']                # input max cloud cover        par_max_cloud = arcpy.Parameter(                          displayName='Maximum cloud cover',                          name='in_max_cloud',                          datatype='GPDouble',                          parameterType='Optional',                          direction='Input',                          category='Satellite Quality Options',                          multiValue=False)        par_max_cloud.filter.type = 'Range'        par_max_cloud.filter.list = [0.0, 100.0]        par_max_cloud.value = 10.0                # input add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True        # combine parameters        parameters = [            par_raw_nc_path,            par_out_nc_path,            par_use_all_dates,            par_specific_years,            par_veg_idx,            par_resampling_interval,            par_metrics,            par_calc_nos,            par_method_type,            par_peak_metric,            par_base_metric,            par_threshold_side,            par_seasonal_amp_factor,            par_absolute_value,            par_smooth_method,            par_savitsky_window_length,            par_savitsky_polyorder,            par_gaussian_sigma,            par_outlier_method,            par_user_factor,            par_zscore_pvalue,            par_fmask_flags,             par_max_cloud,            par_add_result_to_map            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # imports        try:            import numpy as np            import xarray as xr        except:            arcpy.AddError('Python library xarray not installed.')            return                # enable seasonal amp factor if seasonal amp method selected        if parameters[8].value == 'Seasonal amplitude':            parameters[12].enabled = True  # enable seasonal amp factor        else:            parameters[12].enabled = False  # disable seasonal amp factor                    # enable absolute value if absolute value method selected        if parameters[8].value == 'Absolute value':            parameters[13].enabled = True  # enable absolute value        else:            parameters[13].enabled = False  # disable absolute value                    # enable window length and polyorder when savitsky method selected        if parameters[14].value == 'Savitsky-Golay':            parameters[15].enabled = True  # enable window lngth            parameters[16].enabled = True  # enable polyorder        else:            parameters[15].enabled = False  # disable window lngth            parameters[16].enabled = False  # disable polyorder                   # enable sigma when symmetrical gaussian method selected        if parameters[14].value == 'Symmetrical Gaussian':            parameters[17].enabled = True  # enable sigma        else:            parameters[17].enabled = False  # disable sigma                    # enable zscore pvalue when zscore selected        if parameters[18].value == 'Z-Score':            parameters[20].enabled = True  # enable zscore pvalue        else:            parameters[20].enabled = False  # disable zscore pvalue        # modify use all dates boolean control        if not parameters[2].value:                    # disable specific year control            parameters[3].enabled = True                        # try and get nc if exists            years = []            try:                # get path, open dataset, extract years                nc_path = parameters[0].valueastext                ds = xr.open_dataset(nc_path)                                # check if time dim/coord in ds, if not, return                 if 'time' in ds and hasattr(ds, 'time.year') and hasattr(ds, 'time.month'):                    years = np.unique(ds['time.year']).tolist()                else:                    years = []            except:                arcpy.AddError('Could not read times from input NetCDF.')                return                        # update list of years            parameters[3].filter.list = years        else:            # disable specific year control            parameters[3].enabled = False                        # update list of years to empty            parameters[3].filter.list = []                return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the Phenolopy Metrics module.        """                # safe imports        import os, sys       # arcgis comes with these        import datetime      # arcgis comes with this        import numpy as np   # arcgis comes with this        import pandas as pd  # arcgis comes with this        import tempfile      # arcgis comes with this        # risk imports (non-native to arcgis)        try:            import xarray as xr  # not in arcgis        except:            arcpy.AddError('Python library Xarray is not installed.')            return                # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                          # module folder            sys.path.append(FOLDER_MODULES)            import phenolopy, cog           except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return                    # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)                                        # grab parameter values         in_nc = parameters[0].valueAsText                 # raw input satellite netcdf        out_nc = parameters[1].valueAsText                # output phenometrics netcdf        in_use_all_dates = parameters[2].value            # use all dates in nc         in_specific_years = parameters[3].valueAsText     # set specific year         in_veg_idx = parameters[4].value                  # vege index name        in_resample_interval = parameters[5].value        # resample interval        in_metrics = parameters[6].valueAsText            # phenometrics        in_calc_nos = parameters[7].value                 # calculate nos        in_method_type = parameters[8].value              # phenolopy method type        in_peak_metric = parameters[9].value              # peak metric        in_base_metric = parameters[10].value             # base metric        in_threshold_side = parameters[11].value          # threshold side        in_seasonal_amp_factor = parameters[12].value     # seasonal amplitude factor        in_absolute_value = parameters[13].value          # absolute value        in_smooth_method = parameters[14].value           # smoothing method         in_sav_window_length = parameters[15].value       # savitsky window length         in_sav_polyorder = parameters[16].value           # savitsky polyorder         in_gaussian_sigma = parameters[17].value          # gaussian sigma        in_outlier_method = parameters[18].value          # outlier method        in_user_factor = parameters[19].value             # outlier cutoff user factor        in_zscore_pvalue = parameters[20].value           # zscore pvalue        in_fmask_flags = parameters[21].valueAsText       # fmask flag values        in_max_cloud = parameters[22].value               # max cloud percentage        in_add_result_to_map = parameters[23].value       # add result to map        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning Phenolopy Metrics.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                             min_range=0, max_range=17)                                    # convetr specific years if exists        if in_specific_years is not None:            in_specific_years = [int(e) for e in in_specific_years.split(';')]                                            # convert resample interval to correct input (e.g. Bi-monthly to 1MS)        in_resample_interval = arc.convert_resample_interval_code(in_resample_interval)                # convert arcgis multi-value format to list of values        in_metrics = in_metrics.lower().replace("'", '').split(';')        in_metrics = [e.split(':')[0] for e in in_metrics]        # convert phenolopy method from arcgs to module friendly name        in_method_type = in_method_type.lower().replace(' ', '_')        # convert arcgis peak and base metrics format to module friendly names        in_peak_metric = in_peak_metric.lower().split(':')[0]        in_base_metric = in_base_metric.lower().split(':')[0]                # convert arcgis threshold side format to module friendly name        in_threshold_side = in_threshold_side.lower().replace(' ', '_')                # convert arcgis smoother name to module friendly name        in_smooth_method = arc.convert_smoother_code(in_smooth_method)                # check if savitsky parameters are correct, if selected        in_sav_window_length, in_sav_polyorder = arc.check_savitsky_inputs(in_sav_window_length,                                                                            in_sav_polyorder)                # convert arcgis outlier method to module friendly name        if in_outlier_method == 'Local Median Threshold':            in_outlier_method = 'median'        else:            in_outlier_method = 'zscore'                    # convert fmask flags as text to numeric code equivalents        in_fmask_flags = [e for e in in_fmask_flags.split(';')]        in_fmask_flags = arc.convert_fmask_codes(in_fmask_flags)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Loading and checking netcdf...')        arcpy.SetProgressorPosition(1)                # load raw netcdf        ds = satfetcher.load_local_nc(nc_path=in_nc,                                       use_dask=True,                                       conform_nodata_to=np.nan)                                              # check netcdf if it has bands, get attributes for ds and a band        if len(ds.data_vars) == 0:            arcpy.AddError('Input NetDF must be a xr dataset.')            return        elif len(ds.data_vars) == 0:            arcpy.AddError('Input NetDF has no data/variables/bands.')            return                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Getting NetCDF attributes and mask information...')        arcpy.SetProgressorPosition(2)                        # get attributes from dataset        ds_attrs = ds.attrs        ds_band_attrs = ds[list(ds.data_vars)[0]].attrs        ds_spatial_ref_attrs = ds['spatial_ref'].attrs                                # check if expected band name exists        mask_band = arc.get_name_of_mask_band(list(ds.data_vars))        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Removing invalid pixels and empty dates...')        arcpy.SetProgressorPosition(3)                # remove invalid pixels and empty scenes        ds = cog.remove_fmask_dates(ds=ds,                                     valid_class=in_fmask_flags,                                     max_invalid=in_max_cloud,                                     mask_band=mask_band,                                     nodata_value=np.nan,                                     drop_fmask=True)                                             # get platform name from attributes, error if no attributes        in_platform = arc.get_platform_from_dea_attrs(ds_attrs)                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Calculating vegetation index...')        arcpy.SetProgressorPosition(4)                # conform dea aws band names based on platform        ds = satfetcher.conform_dea_ard_band_names(ds=ds,                                                    platform=in_platform.lower())         # calculate vegetation index         ds = tools.calculate_indices(ds=ds,                                      index=in_veg_idx.lower(),                                      custom_name='veg_idx',                                      rescale=False,                                      drop=True)                # append original attributes on to new band        ds['veg_idx'].attrs = ds_band_attrs                 # group duplicate times if exist and rechunk        ds = satfetcher.group_dupe_times(ds)        ds = ds.chunk({'time': -1})                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Correcting edge dates...')        arcpy.SetProgressorPosition(5)                # ensure first/last date are start/end of year        ds = phenolopy.conform_edge_dates(ds=ds)                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Performing a resample to equalise dates...')        arcpy.SetProgressorPosition(6)                # resample to weekly medians, prior to group-resample        ds = phenolopy.resample(ds=ds,                                 interval='1W',                                inplace=True)                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Interpolating initial gaps...')        arcpy.SetProgressorPosition(7)                # interpolate missing values        ds = phenolopy.interpolate(ds=ds,                                    method='full',                                    inplace=True)        # # # # #        # subset data to specific years if requested        if not in_use_all_dates:                    # notify and increment progess bar            arcpy.SetProgressorLabel('Subsetting data to requeste year(s)...')            arcpy.SetProgressorPosition(8)                    # if actual years provided, execute            if len(in_specific_years) > 0:                ds = ds.where(ds['time.year'].isin(in_specific_years), drop=True)            else:                arcpy.AddError('No analysis year(s) provided.')                return        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Grouping times...')        arcpy.SetProgressorPosition(8)                # group and reduce dataset into median weeks (52 for one year)        ds = phenolopy.group(ds=ds,                              interval='week',                             inplace=True)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Removing outliers...')        arcpy.SetProgressorPosition(9)                #  remove outliers from data using requeted method        ds = phenolopy.remove_outliers(ds=ds,                                        method=in_outlier_method,                                        user_factor=in_user_factor,                                        z_pval=in_zscore_pvalue)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Performing resample...')        arcpy.SetProgressorPosition(10)                # resample data using user interval        ds = phenolopy.resample(ds=ds,                                 interval=in_resample_interval,                                inplace=True)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Removing overshot times...')        arcpy.SetProgressorPosition(11)                # resampling can overshoot to previous/next year... remove those         ds = phenolopy.remove_overshoot_times(ds=ds, max_times=3)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Smoothing time-series...')        arcpy.SetProgressorPosition(12)               # use smoothing filter to smooth across time dimension        ds = phenolopy.smooth(ds=ds,                               method=in_smooth_method,                               window_length=in_sav_window_length,                               polyorder=in_sav_polyorder,                               sigma=in_gaussian_sigma)                                              # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Computing data into memory...')        arcpy.SetProgressorPosition(13)                # compute prior to phenometrics        ds = ds.compute()                        # # # # #        # calculate number of seasons, if requested        if in_calc_nos:                    # notify and increment progess bar            arcpy.SetProgressorLabel('Calculating number of seasons...')            arcpy.SetProgressorPosition(14)            # calculate number of seasons (num of major peaks) per-pixel            ds_nos = phenolopy.calc_num_seasons(ds=ds)                    # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Calculating phenometrics...')        arcpy.SetProgressorPosition(14)                # calc phenometrics via phenolopy!        ds = phenolopy.calc_phenometrics(ds=ds,                                          metric=in_metrics,                                         peak_metric=in_peak_metric,                                          base_metric=in_base_metric,                                          method=in_method_type,                                          factor=in_seasonal_amp_factor,                                          thresh_sides=in_threshold_side,                                          abs_value=in_absolute_value)                                                 # add number of seasons to dataset if calculated        if in_calc_nos:            ds['nos_values'] = ds_nos['nos_values']                                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(15)                # append attrbutes on to dataset and bands        ds.attrs = ds_attrs        ds['spatial_ref'].attrs = ds_spatial_ref_attrs        for var in list(ds.data_vars):            ds[var].attrs = ds_band_attrs                 # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(16)          # set any nan to 0        ds = ds.where(~ds.isnull(), 0)        # remove any rogue dimensions        ds = ds.squeeze(drop=True)        # export netcdf file        tools.export_xr_as_nc(ds=ds, filename=out_nc)                        # # # # #        # add multi-dim raster to current map        if in_add_result_to_map:                        # notify and increment progess bar            arcpy.SetProgressorLabel('Adding metrics to current ArcGIS map...')            arcpy.SetProgressorPosition(16)                                    # create output folder with dt            dt = datetime.datetime.now().strftime("%d%m%Y%H%M%S")            out_folder = os.path.join(os.path.dirname(out_nc), 'metrics' + '_' + dt)            os.makedirs(out_folder)            try:                # try to get current map, fail if doesnt exist                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap                                # enable auto-add to map                #arcpy.env.addOutputsToMap = False                                # setup a group layer via template                grp_lyr = arcpy.mp.LayerFile(GRP_LYR_FILE)                grp = m.addLayer(grp_lyr)[0]                grp.name = 'metrics'                # loop each metric and export a seperate crf                for var in list(ds.data_vars):                                # create temporary netcdfil for one var (prevents 2.9 bug)                    with tempfile.NamedTemporaryFile() as tmp:                        tmp_nc = '{}_{}.nc'.format(tmp.name, var)                        ds[var].to_dataset().to_netcdf(tmp_nc)                                            # build in-memory crf for temp netcdf                    out_crf = os.path.join(out_folder, 'temp_{}.crf'.format(var))                    lyr = arcpy.md.MakeMultidimensionalRasterLayer(in_multidimensional_raster=tmp_nc,                                                                    out_multidimensional_raster_layer=out_crf)                                        # export final tif                    out_tif = os.path.join(out_folder, '{}.tif'.format(var))                    arcpy.management.CopyRaster(in_raster=lyr, out_rasterdataset=out_tif)                                                                          # add to current map                    m.addDataFromPath(out_tif)                                        # determine optimal cmap based on data type (or los)                    if 'values' in var:                        if 'los' in var:                            cmap, cutoff = 'Spectrum By Wavelength-Full Bright', 0.0                        elif 'rod' in var or 'roi' in var:                            cmap, cutoff = 'Orange-Red (Continuous)', 1.0                        else:                            cmap, cutoff = 'Precipitation', 0.5                    else:                        cmap, cutoff = 'Temperature', 0.0                    # apply symbology to layer                    sym = arc.apply_cmap(aprx=aprx,                                          lyr_name='{}.tif'.format(var),                                         cmap_name=cmap,                                         cutoff_pct=cutoff)                                                             # rename lyr, add to group, remove second instance                        m.addLayerToGroup(grp, sym)                    m.removeLayer(sym)                                except:                arcpy.AddWarning('Could not visualise output.')        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(17)                # close and del dataset        ds.close()        del ds        # notify user        arcpy.AddMessage('Generated Phenometrics successfully.')        returnclass Nicher_SDM(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = 'Nicher SDM'        self.description = 'Generate a species distribution model.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set various ArcGIS Pro UI controls. Data validation        is enforced via ArcGIS Pro API.        """                # input topographic variable tif(s) and type(s)        par_topo_tifs = arcpy.Parameter(                          displayName='Input topographic variables and variable type',                          name='in_topo_tifs',                          datatype='GPValueTable',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_topo_tifs.columns = [['DEFile', 'Raster'], ['GPString', 'Type']]        par_topo_tifs.filters[0].list = ['tif']        par_topo_tifs.filters[1].type = 'ValueList'        par_topo_tifs.filters[1].list = ['Continuous', 'Categorical']                         # output netcdf file        par_out_nc_path = arcpy.Parameter(                            displayName='Output SDM Netcdf file',                            name='out_sdm_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_path.filter.list = ['nc']        # input occurrence points shapefile        par_occurrence_feat = arcpy.Parameter(                               displayName='Input occurrence point feature',                                name='in_occurrence_feat',                                datatype='GPFeatureLayer',                                parameterType='Required',                                direction='Input',                                multiValue=False)        par_occurrence_feat.filter.list = ['Point']                # input number of pseudoabsence        par_num_pseudos = arcpy.Parameter(                            displayName='Number of psuedoabsence points',                            name='in_num_pseudos',                            datatype='GPLong',                            parameterType='Required',                            direction='Input',                            category='Occurrence Data Options',                            multiValue=False)        par_num_pseudos.filter.type = 'Range'        par_num_pseudos.filter.list = [1, 10000]        par_num_pseudos.value = 500                # input exclusion buffer        par_exclusion_buffer = arcpy.Parameter(                                 displayName='Exclusion buffer area',                                 name='in_exclusion_buffer',                                 datatype='GPLong',                                 parameterType='Required',                                 direction='Input',                                 category='Occurrence Data Options',                                 multiValue=False)        par_exclusion_buffer.filter.type = 'Range'        par_exclusion_buffer.filter.list = [1, 10000]        par_exclusion_buffer.value = 250                # input whether to equalise absence to presence        par_equalise_absence = arcpy.Parameter(                                 displayName="Equalise number of absence points",                                 name="in_equalise_absence",                                 datatype="GPBoolean",                                 parameterType="Required",                                 direction="Input",                                 category='Occurrence Data Options',                                 multiValue=False)        par_equalise_absence.value = False                # input model estimator        par_estimator = arcpy.Parameter(                          displayName='Model estimator',                          name='in_etimator',                          datatype='GPString',                          parameterType='Required',                          direction='Input',                          category='Nicher Options',                          multiValue=False)        estimators = ['Random Forest', 'Extra Trees']        par_estimator.filter.type = 'ValueList'              par_estimator.filter.list = estimators        par_estimator.value = 'Random Forest'                 # input number of model estimators        par_num_estimators = arcpy.Parameter(                               displayName='Number of model estimators',                               name='in_num_estimators',                               datatype='GPLong',                               parameterType='Required',                               direction='Input',                               category='Nicher Options',                               multiValue=False)        par_num_estimators.filter.type = 'Range'        par_num_estimators.filter.list = [5, 1000]        par_num_estimators.value = 100                # input number of model replicates        par_num_replicates = arcpy.Parameter(                               displayName='Number of model replications',                               name='in_num_replicates',                               datatype='GPLong',                               parameterType='Required',                               direction='Input',                               category='Nicher Options',                               multiValue=False)        par_num_replicates.filter.type = 'Range'        par_num_replicates.filter.list = [1, 100]        par_num_replicates.value = 5                # input test ratio        par_test_ratio = arcpy.Parameter(                           displayName='Test set ratio',                           name='in_test_ratio',                           datatype='GPDouble',                           parameterType='Required',                           direction='Input',                           category='Nicher Options',                           multiValue=False)        par_test_ratio.filter.type = 'Range'        par_test_ratio.filter.list = [0, 1]        par_test_ratio.value = 0.1                    # input add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True                # combine parameters        parameters = [            par_topo_tifs,            par_out_nc_path,            par_occurrence_feat,            par_num_pseudos,            par_exclusion_buffer,            par_equalise_absence,            par_estimator,            par_num_estimators,            par_num_replicates,            par_test_ratio,            par_add_result_to_map            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                if parameters[0].altered:                    # create a value table object, fill with current values from parameter on ui            vals_table = arcpy.ValueTable(len(parameters[0].columns))            vals_table.loadFromString(parameters[0].valueAsText)                        # create a list of the values in the parameter, this will be a list of lists            vals_list = parameters[0].values                    # iterate each row, update            for i, item in enumerate(vals_list):                lyr, lyr_type = item                                try:                    # if in-app layer, get source                    path = lyr.dataSource                except:                    # else its a external file, so just get value                     path = lyr.value                # only change type if empty                if lyr_type == '':                    newvalue = "'{}' '{}'".format(path, 'Continuous')                else:                    newvalue = "'{}' '{}'".format(path, lyr_type)                                # update row                vals_table.setRow(i, newvalue)                            # reconstruct the valuetable on the ui            parameters[0].value = vals_table.exportToString()                   return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the Nicher SDM module.        """                        # safe imports        import os, sys                         # arcgis comes with these        import datetime                        # arcgis comes with this        import numpy as np                     # arcgis comes with this        import pandas as pd                    # arcgis comes with this        import tempfile                        # arcgis comes with this        from io import StringIO                # arcgis comes with this        from contextlib import redirect_stdout # arcgis comes with this        # risk imports (non-native to arcgis)        try:            import xarray as xr                # not in arcgis        except:            arcpy.AddError('Python library Xarray is not installed.')            return                # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                          # module folder            sys.path.append(FOLDER_MODULES)            import nicher, cog                      except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return                    # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        # grab parameter values         in_topo_tifs_and_types = parameters[0].value      # topo vars and types        out_nc = parameters[1].valueAsText                # output nicher sdm netcdf        in_occurrence_feat = parameters[2]                # occurrence point shapefile           in_num_pseudos = parameters[3].value              # num pseudoabsences         in_exclusion_buffer = parameters[4].value         # exclusion buffer        in_equalise_absence = parameters[5].value         # equalise absence points        in_estimator = parameters[6].value                # estimator         in_num_estimator = parameters[7].value            # number of estimators        in_num_replicates = parameters[8].value           # number of replicates        in_test_ratio = parameters[9].value               # test train ratio        in_add_result_to_map = parameters[10].value       # add result to map        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning Nicher Species Distribution Modelling.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                             min_range=0, max_range=16)        # unpack topographic variables into relevant lists        rast_cont_list, rast_cate_list = [], []        for item in in_topo_tifs_and_types:            if item[1] == 'Continuous':                rast_cont_list.append(item[0].value)            elif item[1] == 'Categorical':                rast_cate_list.append(item[0].value)        # prepare occurrence shapefile        shp_desc = arcpy.Describe(in_occurrence_feat)        in_occurrence_feat = os.path.join(shp_desc.path, shp_desc.name)                # prepare estimator model type        in_estimator = 'rf' if in_estimator == 'Random Forest' else 'et'        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Loading and checking topographic GeoTIFs...')        arcpy.SetProgressorPosition(1)        # load rasters (continuous and categorical)        combined_rast_lists = rast_cont_list + rast_cate_list        ds = satfetcher.load_local_rasters(rast_path_list=combined_rast_lists,                                            use_dask=True,                                            conform_nodata_to=-999)                                                   # check netcdf if it has bands, get attributes for ds and a band        if len(ds.data_vars) == 0:            arcpy.AddError('No bands/variables detected in input NetCDF.')            return        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Getting Dataset attributes...')        arcpy.SetProgressorPosition(2)                        # get attributes from dataset        ds_attrs = ds.attrs        ds_band_attrs = ds[list(ds.data_vars)[0]].attrs                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Computing topographic variable GeoTIFs into memory...')        arcpy.SetProgressorPosition(3)                # compute into memory        ds = ds.compute()                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Reading occurrence points...')        arcpy.SetProgressorPosition(4)                  # extract point x and y from shapefile as pandas dataframe        df_records = tools.read_shapefile(shp_path=in_occurrence_feat)          # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Preparing occurrence points...')        arcpy.SetProgressorPosition(5)                # subset columns        df_presence = tools.subset_records(df_records=df_records,                                            p_a_column=None)        # drop presence column        df_presence = df_presence.drop('actual', axis='columns')                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Generating pseudoabsence points...')        arcpy.SetProgressorPosition(6)        # generate absences using dataset pixels and occurrence coords        df_absence = nicher.generate_absences(ds=ds,                                               num_abse=in_num_pseudos,                                               occur_shp_path=in_occurrence_feat,                                              buff_m=in_exclusion_buffer,                                               res_factor=3)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Extracting topographic variable values to points...')        arcpy.SetProgressorPosition(7)                # extract values for presence points        df_presence_data = tools.extract_xr_values(ds=ds,                                                    coords=df_presence,                                                    keep_xy=False,                                                    res_factor=3)                # do same for absence points        df_absence_data = tools.extract_xr_values(ds=ds,                                                   coords=df_absence,                                                   keep_xy=False,                                                   res_factor=3)                                                                                                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Removing NoData values from points points...')        arcpy.SetProgressorPosition(8)           # remove all presence records containing nodata values        df_presence_data = tools.remove_nodata_records(df_records=df_presence_data,                                                       nodata_value=ds.nodatavals)                                                               # remove all absence records containing nodata values        df_absence_data = tools.remove_nodata_records(df_records=df_absence_data,                                                       nodata_value=ds.nodatavals)        # # # # #        # if user wants to equalise points, do it        if in_equalise_absence:                        # progress bar            arcpy.SetProgressorLabel('Equalising number of absence points...')            arcpy.SetProgressorPosition(8)                          # equalise absence to match number of presence            df_absence_data = gdvsdm.equalise_abse_records(df_presence=df_presence_data,                                                            df_absence=df_absence_data)                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Combinging presence and absence points...')        arcpy.SetProgressorPosition(9)                         # take pres and abse records and combine, add new pres/abse column        df_pres_abse_data = nicher.combine_pres_abse_records(df_presence=df_presence_data,                                                              df_absence=df_absence_data)                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Computing Pearson correlation matrix...')        arcpy.SetProgressorPosition(10)         # display general rule of thumb text         msg = 'Correlation matrix collinearity: < 0.6 weak , 0.6-0.8 moderate, >= 0.8 strong.'        arcpy.AddMessage(msg)        # generate matrix and capture result for arcgis        f = StringIO()        with redirect_stdout(f):            nicher.generate_correlation_matrix(df_records=df_pres_abse_data,                                               show_fig=False,                                               show_text=True)            arcpy.AddMessage(f.getvalue())        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Computing Variance Inflation Factor...')        arcpy.SetProgressorPosition(11)         # display general rule of thumb text         msg = 'Variance Inflation Factor score: 1 = No multicolinearity, 1-5 = ' \              'moderate, > 5 = high, > 10 = Remove'        arcpy.AddMessage(msg)            # generate vif and capture result for arcgis        f = StringIO()        with redirect_stdout(f):            nicher.generate_vif_scores(df_records=df_pres_abse_data)                       arcpy.AddMessage(f.getvalue())                                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Performing Species Distribution Modelling...')        arcpy.SetProgressorPosition(12)             # create a random forest estimator using default sklearn parameters        estimator = nicher.create_estimator(estimator_type=in_estimator,                                             n_estimators=in_num_estimator)                                                    # generate SDM and capture outputs to arcgis        f = StringIO()        with redirect_stdout(f):            ds = nicher.generate_sdm(ds=ds,                                      df_records=df_pres_abse_data,                                      estimator=estimator,                                      rast_cont_list=rast_cont_list,                                      rast_cate_list=rast_cate_list,                                      replicates=in_num_replicates,                                      test_ratio=in_test_ratio,                                      equalise_test_set=False,                                      calc_accuracy_stats=True,                                     plot_stats=False)                                 arcpy.AddMessage(f.getvalue())                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(13)                # append attrbutes on to dataset and bands        ds.attrs = ds_attrs        for var in list(ds.data_vars):            ds[var].attrs = ds_band_attrs                                                        # manually create albers attributes        ds = tools.build_xr_attributes(ds)                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(14)                                               # export netcdf file        tools.export_xr_as_nc(ds=ds, filename=out_nc)                        # # # # #        # add multi-dim raster to current map        if in_add_result_to_map:                        # notify and increment progess bar            arcpy.SetProgressorLabel('Adding SDM results to current ArcGIS map...')            arcpy.SetProgressorPosition(15)                                    # create output folder with dt            dt = datetime.datetime.now().strftime("%d%m%Y%H%M%S")            out_folder = os.path.join(os.path.dirname(out_nc), 'sdm' + '_' + dt)            os.makedirs(out_folder)                        try:                # try to get current map, fail if does not exist                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap                                # enable auto-add to map                #arcpy.env.addOutputToMap = False                                # setup a group layer via template                grp_lyr = arcpy.mp.LayerFile(GRP_LYR_FILE)                grp = m.addLayer(grp_lyr)[0]                grp.name = 'sdm'                                # loop each var and export a seperate crf                for var in list(ds.data_vars):                                    # create temporary netcdfil for one var (prevents 2.9 bug)                    with tempfile.NamedTemporaryFile() as tmp:                        tmp_nc = '{}_{}.nc'.format(tmp.name, var)                        ds[var].to_dataset().to_netcdf(tmp_nc)                                # build in-memory crf for temp netcdf                    out_crf = os.path.join(out_folder, 'temp_{}.crf'.format(var))                    lyr = arcpy.md.MakeMultidimensionalRasterLayer(in_multidimensional_raster=tmp_nc,                                                                    out_multidimensional_raster_layer=out_crf)                                        # export final tif                    out_tif = os.path.join(out_folder, '{}.tif'.format(var))                    arcpy.management.CopyRaster(in_raster=lyr, out_rasterdataset=out_tif)                                                                          # add to current map                    m.addDataFromPath(out_tif)                                        # apply symbology to layer                    sym = arc.apply_cmap(aprx=aprx,                                          lyr_name='{}.tif'.format(var),                                         cmap_name='Spectrum By Wavelength-Full Bright',                                         cutoff_pct=0.0)                                        # rename lyr, add to group, remove second instance                    m.addLayerToGroup(grp, sym)                    m.removeLayer(sym)                                except:                arcpy.AddWarning('Could not visualise output.')        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(16)                # close and del dataset        ds.close()        del ds        # notify user        arcpy.AddMessage('Generated SDM successfully.')                returnclass Nicher_Masker(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = "Nicher Masker"        self.description = "Use another raster layer to mask out areas " \                           "from SDM outputs."        self.canRunInBackground = False    def getParameterInfo(self):        """        Set various ArcGIS Pro UI controls. Data validation        is enforced via ArcGIS Pro API.        """                # input sdm netcdf file        par_sdm_nc_path = arcpy.Parameter(                            displayName='Input SDM NetCDF file',                            name='in_sdm_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Input')        par_sdm_nc_path.filter.list = ['nc']                # input mask tif or netcdf file        par_mask_file_path = arcpy.Parameter(                            displayName='Input mask NetCDF or GeoTIFF file',                            name='in_mask_file_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Input')        par_mask_file_path.filter.list = ['tif', 'nc']                # output netcdf file        par_out_nc_path = arcpy.Parameter(                            displayName='Output masked SDM Netcdf file',                            name='out_masked_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_path.filter.list = ['nc']                # input minimum value        par_min_value = arcpy.Parameter(                          displayName='Minimum value',                          name='in_min_value',                          datatype='GPDouble',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_min_value.value = 2        # input max value        par_max_value = arcpy.Parameter(                          displayName='Maximum value',                          name='in_max_value',                          datatype='GPDouble',                          parameterType='Optional',                          direction='Input',                          multiValue=False)                # input replacement value        par_replacement_value = arcpy.Parameter(                                  displayName='Replacement value',                                  name='in_replacement_value',                                  datatype='GPDouble',                                  parameterType='Required',                                  direction='Input',                                  multiValue=False)        par_replacement_value.value = 0                parameters = [            par_sdm_nc_path,            par_mask_file_path,            par_out_nc_path,            par_min_value,            par_max_value,            par_replacement_value        ]        return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the Nicher Mask module.        """                # safe imports        import os, sys                         # arcgis comes with these        import datetime                        # arcgis comes with this        import numpy as np                     # arcgis comes with this        # risk imports (non-native to arcgis)        try:            import xarray as xr                # not in arcgis        except:            arcpy.AddError('Python library Xarray is not installed.')            return                # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                          # module folder            sys.path.append(FOLDER_MODULES)            import nicher, cog                      except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return                    # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        # grab parameter values         in_sdm_nc_path = parameters[0].valueAsText     # input sdm netcdf        in_mask_file_path = parameters[1].valueAsText  # input mask geotiff / netcdf        out_nc_path = parameters[2].valueAsText        # output masked sdm netcdf        in_min_value = parameters[3].value             # input min value        in_max_value = parameters[4].value             # input max value        in_replacement_value = parameters[5].value     # input replacement value                        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning Nicher Species Distribution Model Masker.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                             min_range=0, max_range=8)                                    # get mask file type               mask_filetype = 'tif' if in_mask_file_path.endswith('.tif') else 'nc'                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Reading SDM NetCDF...')        arcpy.SetProgressorPosition(1)                # load netcdf. set nodata to nan to mimic dea odc        ds_sdm = satfetcher.load_local_nc(nc_path=in_sdm_nc_path,                                           use_dask=True,                                           conform_nodata_to=np.nan)                                              # check netcdf if it has bands, get attributes for ds and a band               if len(ds_sdm.data_vars) == 0:            arcpy.AddError('No bands/variables detected in input NetCDF.')            return        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Getting Dataset attributes...')        arcpy.SetProgressorPosition(2)                # get attributes from dataset        ds_attrs = ds_sdm.attrs        ds_band_attrs = ds_sdm[list(ds_sdm.data_vars)[0]].attrs        ds_spatial_ref_attrs = ds_sdm['spatial_ref'].attrs                           # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Reading SDM Mask NetCDF/GeoTIFF...')        arcpy.SetProgressorPosition(2)                        # open input depeneding on file type        if mask_filetype == 'tif':            ds_mask = satfetcher.load_local_rasters(rast_path_list=in_mask_file_path,                                                     use_dask=True,                                                     conform_nodata_to=np.nan)        else:            ds_mask = satfetcher.load_local_nc(nc_path=in_mask_file_path,                                                use_dask=True,                                                conform_nodata_to=np.nan)                # check netcdf if it has bands, get attributes for ds and a band        if isinstance(ds_mask, (xr.DataArray)):            ds_mask = ds_mask.to_dataset(dim='variable')        elif len(ds_mask.data_vars) == 0:            arcpy.AddError('No bands/variables detected in input mask file.')            return                    # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Clipping and resampling SDM Mask to match SDM NetCDF...')        arcpy.SetProgressorPosition(3)           # clip and resample the mask to extent and size of sdm netcdf        ds_mask = tools.clip_xr_to_xr(ds_a=ds_mask, ds_b=ds_sdm, inplace=True)        ds_mask = tools.resample_xr(ds_from=ds_mask, ds_to=ds_sdm, resampling='nearest')        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Computing SDM and SDM Mask datasets into memory...')        arcpy.SetProgressorPosition(4)          # compute into memory        ds_sdm = ds_sdm.compute()        ds_mask = ds_mask.compute()                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Preparing SDM Mask...')        arcpy.SetProgressorPosition(5)                       # prepare mask        if in_max_value is not None:            ds_mask = xr.where((ds_mask > in_min_value) &                                (ds_mask < in_max_value), True, False)        else:            ds_mask = xr.where(ds_mask > in_min_value, True, False)                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Applying mask to SDM data...')        arcpy.SetProgressorPosition(6)                # if mask is still a dataset, convert to array        if isinstance(ds_mask, xr.Dataset):            ds_mask = ds_mask.to_array().squeeze(drop=True)                                # mask out lowest value and set to replacement value        ds_sdm = ds_sdm.where(ds_mask, in_replacement_value)                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(7)                # append attrbutes on to dataset and bands        ds_sdm.attrs = ds_attrs        ds_sdm['spatial_ref'].attrs = ds_spatial_ref_attrs        for var in list(ds_sdm.data_vars):            ds_sdm[var].attrs = ds_band_attrs                        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Exporting NetCDF...')        arcpy.SetProgressorPosition(8)                        # export netcdf file        tools.export_xr_as_nc(ds=ds_sdm, filename=out_nc_path)                        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(9)                # close and del dataset        ds_sdm.close()        ds_mask.close()        del ds_sdm, ds_mask        # notify user        arcpy.AddMessage('Generated SDM Mask successfully.')                returnclass VegFrax_Fractional_Cover(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = "VegFrax Fractional Cover"        self.description = "Extrapolate small areas of high resolution " \                           "classifed imagery across larger areas of lower " \                           "resolution imagery, such as Landsat or Sentinel."        self.canRunInBackground = False    def getParameterInfo(self):        """        Set various ArcGIS Pro UI controls. Data validation        is enforced via ArcGIS Pro API.        """                # input low res satellite netcdf/tif file        par_low_res_path = arcpy.Parameter(                             displayName='Input low-resolution Satellite data NetCDF',                             name='in_low_res_path',                             datatype='DEFile',                             parameterType='Required',                             direction='Input')        par_low_res_path.filter.list = ['nc']        # input high res satellite tif file        par_high_res_path = arcpy.Parameter(                              displayName='Input classified high-resolution GeoTIF',                              name='in_high_res_path',                              datatype='DEFile',                              parameterType='Required',                              direction='Input')        par_high_res_path.filter.list = ['tif']                        # output netcdf file        par_out_nc_path = arcpy.Parameter(                            displayName='Output Fractional Cover Netcdf',                            name='out_vegfrax_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_path.filter.list = ['nc']                      # input start year for low res netcdf/file        par_from_date = arcpy.Parameter(                                 displayName='Subset date of low resolution data from',                                 name='in_from_date',                                 datatype='GPDate',                                 parameterType='Required',                                 direction='Input',                                 multiValue=False)        par_from_date.values = '2015/01/01'                # input end year for low res netcdf/file        par_date_to = arcpy.Parameter(                        displayName='Subset date of low resolution data to',                        name='in_to_date',                        datatype='GPDate',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_date_to.values = '2020/12/31'                # input aggregator        par_aggregator = arcpy.Parameter(                                    displayName='Aggregator',                                    name='in_aggregator',                                    datatype='GPString',                                    parameterType='Required',                                    direction='Input',                                    multiValue=False)        par_aggregator.filter.type = 'ValueList'        par_aggregator.filter.list = [            'Mean',            'Median'            ]        par_aggregator.value = 'Median'                 # input classes list from high res netcdf/tif         par_classes = arcpy.Parameter(                        displayName='Set fractonal classes',                        name='in_classes',                        datatype='GPString',                        parameterType='Required',                        direction='Input',                        multiValue=True,                        enabled=False)        classes = ['No Classes']        par_classes.filter.type = 'ValueList'              par_classes.filter.list = classes        par_classes.values = ['Class: NoClass']                # input merge selected classes        par_merge_classes = arcpy.Parameter(                              displayName='Merge selected classes',                              name='in_merge_classes',                              datatype='GPBoolean',                              parameterType='Required',                              direction='Input',                              multiValue=False,                              enabled=False)        par_merge_classes.value = False                # input number of samples        par_num_samples = arcpy.Parameter(                            displayName='Number of random samples per class',                            name='in_num_samples',                            datatype='GPLong',                            parameterType='Required',                            direction='Input',                            category='VegFrax Options',                            multiValue=False)        par_num_samples.filter.type = 'Range'        par_num_samples.filter.list = [10, 10000]        par_num_samples.value = 200                # input number of model estimators        par_num_estimators = arcpy.Parameter(                               displayName='Number of model estimators',                               name='in_num_estimators',                               datatype='GPLong',                               parameterType='Required',                               direction='Input',                               category='VegFrax Options',                               multiValue=False)        par_num_estimators.filter.type = 'Range'        par_num_estimators.filter.list = [5, 1000]        par_num_estimators.value = 100                # input number of model validations        par_num_validations = arcpy.Parameter(                                displayName='Number of model validations',                                name='in_num_validations',                                datatype='GPLong',                                parameterType='Required',                                direction='Input',                                category='VegFrax Options',                                multiValue=False)        par_num_validations.filter.type = 'Range'        par_num_validations.filter.list = [1, 100]        par_num_validations.value = 10                # input oa fmask         par_fmask_flags = arcpy.Parameter(                            displayName='Include flags',                            name='in_fmask_flags',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Satellite Quality Options',                            multiValue=True)        flags = ['NoData', 'Valid', 'Cloud', 'Shadow', 'Snow', 'Water']        par_fmask_flags.filter.type = 'ValueList'              par_fmask_flags.filter.list = flags        par_fmask_flags.values = ['Valid', 'Snow', 'Water']                # input max cloud cover        par_max_cloud = arcpy.Parameter(                          displayName='Maximum cloud cover',                          name='in_max_cloud',                          datatype='GPDouble',                          parameterType='Optional',                          direction='Input',                          category='Satellite Quality Options',                          multiValue=False)        par_max_cloud.filter.type = 'Range'        par_max_cloud.filter.list = [0.0, 100.0]        par_max_cloud.value = 10.0                # input add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True                # combine parameters        parameters = [            par_low_res_path,            par_high_res_path,            par_out_nc_path,            par_from_date,            par_date_to,            par_aggregator,            par_classes,            par_merge_classes,            par_num_samples,            par_num_estimators,            par_num_validations,            par_fmask_flags,            par_max_cloud,            par_add_result_to_map            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # risk imports (non-native to arcgis)        try:            import numpy as np   # this is in arcgis, but ok            import xarray as xr  # not in arcgis            import rasterio      # not in arcgis        except:            arcpy.AddError('Python library Xarray is not installed.')            return                    # update ui datetimes input ds altered - only do on first input change        if parameters[0].altered and not parameters[0].hasBeenValidated:            try:                # get nc path, load nc                 nc_path = parameters[0].valueAsText                ds = xr.open_dataset(nc_path)                                # convert array to dataset if array detected                 if isinstance(ds, xr.DataArray):                    ds = ds.to_dataset(name='variable')                      # check if got time dim, if so, pluck dts                if 'time' in list(ds.dims):                    start_dt = ds['time'].isel(time=0).dt.strftime('%Y-%m-%d').values                    end_dt = ds['time'].isel(time=-1).dt.strftime('%Y-%m-%d').values                # update date controls                parameters[3].value = str(start_dt)                parameters[4].value = str(end_dt)            except:                arcpy.AddError('Could not open low resolution input NetCDF.')                return                        # modify ui classes selector list if high res altered        if parameters[1].altered and not parameters[1].hasBeenValidated:            try:                # open provided high res tif                 tif_path = parameters[1].valueAsText                ds = xr.open_rasterio(tif_path)                                                   # check bands, if single, create classes list (strings)                if len(ds) != 1:                    arcpy.AddError('High resolution raster can not be multiband.')                    return                                # check if nodata value embedded in xr                if not hasattr(ds, 'nodatavals'):                    arcpy.AddError('Dataset does not have nodata value attribute.')                    return                elif ds.nodatavals == 'unknown':                    arcpy.AddError('Dataset nodata value is unknown.')                    return                                    # get all unique classes in dataset and remove nodata                classes = np.unique(ds)                classes = classes[classes != ds.nodatavals]                                # check if we got something                if len(classes) <= 0:                    arcpy.AddError('No classes detected in dataset.')                    return                                    # finally, load all non-nodata classes into ui                text_classes = []                for c in classes:                    text_classes.append('Class: {}'.format(c))                                        # do best to sort alphabetically                text_classes.sort()                # update classes ui control with new classes and enable                parameters[6].enabled = True                parameters[6].filter.list = text_classes                parameters[6].values = text_classes                            except:                arcpy.AddError('Could not open high resolution input GeoTIFF.')                return                  # update ui merge selected classes when classes selector is enabled        if parameters[6].enabled:            parameters[7].enabled = True  # enable merge classes option        else:            parameters[7].enabled = False  # disable merge classes option                      return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the VegFrax Fractional Cover module.        """                # safe imports        import os, sys                           # arcgis comes with these        import datetime                          # arcgis comes with this        import numpy as np                       # arcgis comes with this        import pandas as pd                      # arcgis comes with this        import tempfile                          # arcgis comes with this        from io import StringIO                  # arcgis comes with this        from contextlib import redirect_stdout   # arcgis comes with this        # risk imports (non-native to arcgis)        try:            import xarray as xr                  # not in arcgis        except:            arcpy.AddError('Python library Xarray is not installed.')            return                # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                          # module folder            sys.path.append(FOLDER_MODULES)            import vegfrax, cog                      except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return                    # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        # grab parameter values         in_low_res_nc = parameters[0].valueAsText         # raw input low res satellite netcdf        in_high_res_tif = parameters[1].valueAsText       # raw input high res satellite tif        out_nc = parameters[2].valueAsText                # output vegfrax netcdf        in_from_date = parameters[3].value                # start date of aggregate        in_to_date = parameters[4].value                  # end date of aggregate        in_aggregator = parameters[5].value               # aggregator        in_classes = parameters[6].valueAsText            # selected classes        in_merge_classes = parameters[7].value            # merge selected classes               in_num_samples = parameters[8].value              # number of samples        in_num_estimators = parameters[9].value           # number of model estimators        in_num_validations = parameters[10].value         # number of model validations        in_fmask_flags = parameters[11].valueAsText       # fmask flag values        in_max_cloud = parameters[12].value               # max cloud percentage        in_add_result_to_map = parameters[13].value       # add result to map        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning VegFrax Fractional Cover.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                             min_range=0, max_range=20)        # convert datetime strings to numpy datetime64        in_from_date = arc.datetime_to_numpy(in_from_date)        in_to_date = arc.datetime_to_numpy(in_to_date)               # convert arcgis multi-value format to list of values and notify               in_classes = arc.prepare_vegfrax_classes(in_classes)                # convert fmask flags as text to numeric code equivalents        in_fmask_flags = [e for e in in_fmask_flags.split(';')]        in_fmask_flags = arc.convert_fmask_codes(in_fmask_flags)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Loading and checking low resolution netcdf...')        arcpy.SetProgressorPosition(1)                # load low res netcdf. set nodata to nan to mimic dea odc        ds_low = satfetcher.load_local_nc(nc_path=in_low_res_nc,                                           use_dask=True,                                           conform_nodata_to=np.nan)                                              # check netcdf if it has bands, get attributes for ds and a band        if len(ds_low.data_vars) == 0:            arcpy.AddError('Input NetDF must be a xr dataset.')            return        elif len(ds_low.data_vars) == 0:            arcpy.AddError('Input NetDF has no data/variables/bands.')            return                                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Getting Dataset attributes...')        arcpy.SetProgressorPosition(2)                        # get attributes from dataset        ds_low_attrs = ds_low.attrs        ds_low_band_attrs = ds_low[list(ds_low.data_vars)[0]].attrs        ds_low_spatial_ref_attrs = ds_low['spatial_ref'].attrs                        # check if expected band name exists (landsat/sentinel differences)        ds_low_mask_band = arc.get_name_of_mask_band(list(ds_low.data_vars))        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Removing invalid pixels and dates from low res NetCDF...')        arcpy.SetProgressorPosition(3)                # group duplicate times if exist and rechunk        ds_low = satfetcher.group_dupe_times(ds_low)        ds_low = ds_low.chunk({'time': -1})                # remove invalid pixels and empty scenes        ds_low = cog.remove_fmask_dates(ds=ds_low,                                         valid_class=in_fmask_flags,                                         max_invalid=in_max_cloud,                                         mask_band=ds_low_mask_band,                                         nodata_value=np.nan,                                         drop_fmask=True)                                                     # conform and prepare low res dataset         ds_low = vegfrax.prepare_raw_xr(ds_low,                                         dtype='float32',                                         conform_nodata_to=-999)                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Calculating tasselled cap...')        arcpy.SetProgressorPosition(4)                # get platform name from attributes, error if no attributes        in_platform = arc.get_platform_from_dea_attrs(ds_low_attrs)                # conform dea aws band names based on platform        ds_low = satfetcher.conform_dea_ard_band_names(ds=ds_low,                                                        platform=in_platform.lower())         # calculate tasselled cap index         ds_low = tools.calculate_indices(ds=ds_low,                                          index=['tcg', 'tcb', 'tcw'],                                          custom_name=None,                                          rescale=False,                                          drop=True)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Subsetting data to requested time range...')        arcpy.SetProgressorPosition(5)                        # restrict date range        ds_low = ds_low.where((ds_low['time'] >= in_from_date) &                               (ds_low['time'] <= in_to_date), drop=True)                                      # check if any data exists after subset         if len(ds_low['time']) == 0:            arcpy.AddError('Time range used to subset data returned nothing.')            return                    # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Aggregating low res data and computing into memory...')        arcpy.SetProgressorPosition(5)                # aggregate and compute        if in_aggregator.lower() == 'mean':            ds_low = ds_low.mean('time', keep_attrs=True).compute()        elif in_aggregator.lower() == 'median':            ds_low = ds_low.median('time', keep_attrs=True).compute()                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to low resolution dataset...')        arcpy.SetProgressorPosition(6)                        # append attrbutes on to dataset and bands        ds_low.attrs = ds_low_attrs        ds_low['spatial_ref'].attrs = ds_low_spatial_ref_attrs        for var in list(ds_low.data_vars):            ds_low[var].attrs = ds_low_band_attrs                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Loading and checking high res GeoTIFF...')        arcpy.SetProgressorPosition(7)                # load raster as an xarray dataset and set nodata to -128        ds_high = satfetcher.load_local_rasters(rast_path_list=in_high_res_tif,                                                 use_dask=True,                                                 conform_nodata_to=-128)                # check dataset has one band, get attributes for ds (i.e., band)        if isinstance(ds_high, xr.DataArray):            ds_high = ds_high.to_dataset(dim='variable')        elif len(ds_high) != 1:            arcpy.AddError('More than one band in high resolution input GeoTIFF.')            return                # do basic preparations (dtype, rename, checks)        ds_high = vegfrax.prepare_classified_xr(ds=ds_high)                 # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Clipping high resolution data lowest extent...')        arcpy.SetProgressorPosition(8)                # subset high to low extent        ds_high = tools.clip_xr_to_xr(ds_a=ds_high, ds_b=ds_low)                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Reclassifying all non-selected classes to 0...')        arcpy.SetProgressorPosition(9)                # add 0 if missing. chekc this, could be problematic        if 0 not in in_classes:            in_classes.append(0)                                      # reclassify all other classes to 0 (and leave nodata as is)        ds_high = vegfrax.reclassify_xr(ds=ds_high,                                         req_class=in_classes,                                        merge_classes=in_merge_classes,                                        inplace=True)                # get new list of current classes        in_classes = vegfrax.get_xr_classes(ds_high)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Computing high resolution data into memory...')        arcpy.SetProgressorPosition(10)                # load into memory now - we have values to modify!        ds_high = ds_high.compute()                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Generating random samples in overlap areas...')        arcpy.SetProgressorPosition(11)                # generate random samples within area overlap between raw and classified rasters        df_samples = vegfrax.generate_strat_random_samples(ds_raw=ds_low,                                                           ds_class=ds_high,                                                            req_class=in_classes,                                                           num_samples=in_num_samples)                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Extracting values from low res data...')        arcpy.SetProgressorPosition(12)                # extract pixel values from raw, low resolution rasters at each point        df_extract = tools.extract_xr_values(ds=ds_low,                                              coords=df_samples,                                              keep_xy=True)        # remove any points containing a nodata value        df_extract_clean = tools.remove_nodata_records(df_extract,                                                        nodata_value=ds_low.nodatavals)                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Building focal windows...')        arcpy.SetProgressorPosition(13)                # generate focal windows and extract pixels from class raster        df_windows = vegfrax.create_frequency_windows(ds_raw=ds_low,                                                       ds_class=ds_high,                                                       df_records=df_extract_clean)                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Converting window values to frequencies...')        arcpy.SetProgressorPosition(14)                        # transform raw focal window pixel class counts into frequencies        df_freqs = vegfrax.convert_window_counts_to_freqs(df_windows=df_windows,                                                           nodata_value=ds_high.nodatavals)                               # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Selecting requested classes for analysis...')        arcpy.SetProgressorPosition(15)                # convert classes to text. all classes in request list will be used        text_classes = [str(c) for c in in_classes]        # prepare data for analysis - prepare classes, nulls, normalise frequencies        df_data = vegfrax.prepare_freqs_for_analysis(ds_raw=ds_low,                                                      ds_class=ds_high,                                                      df_freqs=df_freqs,                                                      override_classes=text_classes)                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Performing Fractional Cover Analysis...')        arcpy.SetProgressorPosition(16)                # perform fca and show results in arcgis        f = StringIO()        with redirect_stdout(f):            ds_vegfrax = vegfrax.perform_fca(ds_raw=ds_low,                                              ds_class=ds_high,                                              df_data=df_data,                                              df_extract_clean=df_extract_clean,                                              n_estimators=in_num_estimators,                                             n_validations=in_num_validations)            arcpy.AddMessage(f.getvalue())                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(17)                # append top-level attributes        ds_vegfrax.attrs = ds_low_attrs                # append spatial ref back on        if 'spatial_ref' not in list(ds_vegfrax.coords):            crs = tools.get_xr_crs(ds_low)            ds_vegfrax = ds_vegfrax.assign_coords({'spatial_ref': crs})        ds_vegfrax['spatial_ref'].attrs = ds_low_spatial_ref_attrs                # add band-level attributes back on        for var in list(ds_vegfrax.data_vars):            ds_vegfrax[var].attrs = ds_low_band_attrs        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(18)                   # export netcdf file        tools.export_xr_as_nc(ds=ds_vegfrax, filename=out_nc)                                  # # # # #        # add multi-dim raster to current map        if in_add_result_to_map:                        # notify and increment progess bar            arcpy.SetProgressorLabel('Adding fractional cover classes to current ArcGIS map...')            arcpy.SetProgressorPosition(19)                                    # create output folder with dt            dt = datetime.datetime.now().strftime("%d%m%Y%H%M%S")            out_folder = os.path.join(os.path.dirname(out_nc), 'vegfrax' + '_' + dt)            os.makedirs(out_folder)                        try:                # try to get current map, fail if does not exist                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap	                                # enable auto-add to map                #arcpy.env.addOutputToMap = False                                # setup a group layer via template                grp_lyr = arcpy.mp.LayerFile(GRP_LYR_FILE)                grp = m.addLayer(grp_lyr)[0]                grp.name = 'vegfrax'                # loop each var and export a seperate crf                for var in list(ds_vegfrax.data_vars):                                    # create temporary netcdfil for one var (prevents 2.9 bug)                    with tempfile.NamedTemporaryFile() as tmp:                        tmp_nc = '{}_{}.nc'.format(tmp.name, var)                        ds_vegfrax[var].to_dataset().to_netcdf(tmp_nc)                                # build in-memory crf for temp netcdf                    out_crf = os.path.join(out_folder, 'temp_{}.crf'.format(var))                    lyr = arcpy.md.MakeMultidimensionalRasterLayer(in_multidimensional_raster=tmp_nc,                                                                    out_multidimensional_raster_layer=out_crf)                    # export final tif                    out_tif = os.path.join(out_folder, '{}.tif'.format(var))                    arcpy.management.CopyRaster(in_raster=lyr, out_rasterdataset=out_tif)                    # add to current map                    m.addDataFromPath(out_tif)                                        # apply symbology to layer                    sym = arc.apply_cmap(aprx=aprx,                                          lyr_name='{}.tif'.format(var),                                         cmap_name='Spectrum By Wavelength-Full Bright',                                         cutoff_pct=0.0)                    # rename lyr, add to group, remove second instance                        m.addLayerToGroup(grp, sym)                    m.removeLayer(sym)            except:                arcpy.AddWarning('Could not visualise output.')        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(20)                # close and del dataset        ds_low.close()        ds_high.close()        ds_vegfrax.close()        del ds_low, ds_high, ds_vegfrax        # notify user        arcpy.AddMessage('Generated fractional covers successfully.')        returnclass Ensemble_Sigmoider(object):    def __init__(self):        """        Initialise tool.        """                self.label = "Ensemble Sigmoider"        self.description = "Rescale raw NetCDF or GeoTiffs to 0-1 using advanced sigmoids."        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """        # input netcdf or geotiff file        par_in_file = arcpy.Parameter(                        displayName='Input evidence layer (NetCDF or GeoTiff',                        name='in_file',                        datatype='DEFile',                        parameterType='Required',                        direction='Input')        par_in_file.filter.list = ['nc', 'tif']                # output netcdf file        par_out_file = arcpy.Parameter(                         displayName='Output rescaled layer (NetCDF)',                         name='out_file',                         datatype='DEFile',                         parameterType='Required',                         direction='Output')        par_out_file.filter.list = ['nc']                # input variables        par_in_var = arcpy.Parameter(                       displayName='Variable',                       name='in_var',                       datatype='GPString',                       parameterType='Required',                       direction='Input')        par_in_var.filter.type = 'ValueList'              par_in_var.filter.list = []                    # input type        par_in_type = arcpy.Parameter(                        displayName='Membership type',                        name='in_type',                        datatype='GPString',                        parameterType='Required',                        direction='Input')        par_in_type.filter.type = 'ValueList'              par_in_type.filter.list = ['Increasing', 'Decreasing', 'Symmetric']          par_in_type.value = 'Increasing'                # input minimum (low inflection)        par_in_min = arcpy.Parameter(                       displayName='Low inflection point',                       name='in_minimum',                       datatype='GPDouble',                       parameterType='Required',                       direction='Input')                # input maximum (high inflection)        par_in_max = arcpy.Parameter(                       displayName='High inflection point',                       name='in_maximum',                       datatype='GPDouble',                       parameterType='Required',                       direction='Input')        # input add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True        params = [            par_in_file,             par_out_file,            par_in_var,            par_in_type,            par_in_min,            par_in_max,            par_add_result_to_map            ]        return params    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # imports        try:            import xarray as xr            import rasterio        except:            arcpy.AddError('Could not import Xarray or rasterio.')            return                    # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import satfetcher        except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return                    # globals        global ENSEMBLE_SIGMOIDS        # unpack global parameter values         curr_file = ENSEMBLE_SIGMOIDS.get('in_file')        curr_var = ENSEMBLE_SIGMOIDS.get('in_var')                # if input file added, run        if parameters[0].value is not None:                        # if global has no matching file (or first run), reload all            if curr_file != parameters[0].valueAsText:                if parameters[0].valueAsText.endswith('.nc'):                    try:                        ds = xr.open_dataset(parameters[0].valueAsText)                        data_vars = [var for var in ds]                        ds.close()                    except:                        data_vars = []                                elif parameters[0].valueAsText.endswith('.tif'):                    try:                        da = xr.open_rasterio(parameters[0].valueAsText)                        data_vars = ['{}'.format(var) for var in da['band'].values]                        da.close()                    except:                        data_vars = []                # populate var list with new vars                parameters[2].filter.list = data_vars                                # set var and min, max, mid to no selections                parameters[2].value = None                parameters[4].value = None                parameters[5].value = None                           # calc min, max, mid if var changed            elif curr_var != parameters[2].valueAsText:                new_var = parameters[2].valueAsText                new_type = parameters[3].value                                if parameters[0].valueAsText.endswith('.nc'):                    try:                        ds = xr.open_dataset(parameters[0].valueAsText)                        da = ds[new_var]                                                if hasattr(ds, 'nodatavals'):                            da = da.where(da != ds.nodatavals)                                                mins = round(float(da.min()), 3)                        maxs = round(float(da.max()), 3)                                                ds.close()                    except:                        mins, maxs = None, None                                elif parameters[0].valueAsText.endswith('.tif'):                    try:                        ds = xr.open_rasterio(parameters[0].valueAsText)                        da = ds.sel(band=int(new_var))                                                if hasattr(ds, 'nodatavals') and len(ds.nodatavals) == 1:                            da = da.where(da != ds.nodatavals)                        mins = round(float(da.min()), 3)                        maxs = round(float(da.max()), 3)                            ds.close()                    except:                        mins, maxs = None, None                # set min, max                parameters[4].value = mins                parameters[5].value = maxs                    # update global values        ENSEMBLE_SIGMOIDS = {            'in_file': parameters[0].valueAsText,            'in_var': parameters[2].value,        }        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""               return    def execute(self, parameters, messages):        """        Executes the Ensemble Sigmoider module.        """                # safe imports        import os             # arcgis comes with these        import datetime       # arcgis comes with these        # risky imports (not native to arcgis)        try:            import numpy as np            import xarray as xr        except:            arcpy.AddError('Python libraries xarray and dask not installed.')            return        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools            # module folder            sys.path.append(FOLDER_MODULES)            import canopy        except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return        # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        # grab parameter values         in_file = parameters[0].valueAsText          # input netcdf or geotiff        out_nc = parameters[1].valueAsText           # output netcdf        in_var = parameters[2].value                 # input variable        in_type = parameters[3].value                # input membership type        in_min = parameters[4].value                 # input minimum        in_max = parameters[5].value                 # input maximum        in_add_result_to_map = parameters[6].value   # input add result to map        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning Ensemble Sigmoider.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=7)        # prepare and check file extension        in_ext = os.path.splitext(in_file)[1]        if in_ext not in ['.nc', '.tif']:            arcpy.AddError('File type not supported.')            return        # check type        if in_type not in ['Increasing', 'Decreasing', 'Symmetric']:            arcpy.AddError('Membership type not supported.')            return                    # check values         if in_min is None or in_max is None:            arcpy.AddError('Low and high inflection points must not be empty.')            return        elif in_max <= in_min:            arcpy.AddError('High inflection point can not be <= low inflection.')            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading and checking input data...')        arcpy.SetProgressorPosition(1)        # do quick load depending on input type        try:            if in_ext == '.nc':                ds = xr.open_dataset(in_file)            else:                ds = xr.open_rasterio(in_file)                ds = ds.to_dataset(dim='band')                ds = tools.build_xr_attributes(ds)           except:            arcpy.AddError('Could not quick load input file.')            return        # check xr type, vars, coords, dims, attrs        if not isinstance(ds, xr.Dataset):            arcpy.AddError('Input NetCDF must be a xr dataset.')            return        elif 'time' in ds:            arcpy.AddError('Input NetCDF time dimension must not exist.')            return        elif len(ds) == 0:            arcpy.AddError('Input NetCDF has no data/variables/bands.')            return        elif in_ext == '.tif' and len(ds) > 1:            arcpy.AddError('Input GeoTiff must have only one band.')            return        elif 'x' not in list(ds.coords) or 'y' not in list(ds.coords):            arcpy.AddError('Input NetCDF must have x, y coords.')            return        elif 'spatial_ref' not in list(ds.coords):            arcpy.AddError('Input NetCDF must have a spatial_ref coord.')            return        elif 'x' not in list(ds.dims) or 'y' not in list(ds.dims):            arcpy.AddError('Input NetCDF must have x, y dimensions.')            return        elif len(ds['x']) == 0 or len(ds['y']) == 0:            arcpy.AddError('Input NetCDF must have at least one x, y index.')            return        elif ds.attrs == {}:            arcpy.AddError('NetCDF attributes not found. NetCDF must have attributes.')            return        elif not hasattr(ds, 'crs'):            arcpy.AddError('NetCDF CRS attribute not found. CRS required.')            return        elif ds.crs != 'EPSG:3577':            arcpy.AddError('NetCDF CRS is not EPSG:3577. EPSG:3577 required.')                        return        elif not hasattr(ds, 'nodatavals'):            arcpy.AddError('NetCDF nodatavals attribute not found.')                        return        # check if xr is all nan/0 via centroid pixel timeseries (saves full load)        if ds.to_array().isnull().all():            arcpy.AddError('Input data is completely null.')                        return         try:            # now, do proper open of file, set nodata to nan            if in_ext == '.nc':                ds = satfetcher.load_local_nc(nc_path=in_file,                                               use_dask=False,                                               conform_nodata_to=np.nan)            else:                ds = satfetcher.load_local_rasters(rast_path_list=in_file,                                                    use_dask=False,                                                    conform_nodata_to=np.nan)                ds = tools.build_xr_attributes(ds)         except:            arcpy.AddError('Could not properly load input file.')            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Getting file attributes...')        arcpy.SetProgressorPosition(2)        # get attributes from dataset        ds_attrs = ds.attrs        ds_band_attrs = ds[list(ds.data_vars)[0]].attrs        ds_spatial_ref_attrs = ds['spatial_ref'].attrs        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Subsetting data to specified variable...')        arcpy.SetProgressorPosition(3)        # check if requested var in netcdf (tif always has one)        if in_ext == '.nc' and in_var not in ds:            arcpy.AddError('Requested variable not found in NetCDF.')            return        # subset dataset if netcdf (tif always have one), set both as array        ds = ds[in_var] if in_ext == '.nc' else ds.to_array()        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Applying sigmoidal function to data...')        arcpy.SetProgressorPosition(3)                # get nan mask        ds_mask = xr.where(~ds.isnull(), True, False)        try:            # apply sigmoidal depending on user selection            if in_type == 'Increasing':                ds = canopy.inc_sigmoid(ds, a=in_min, b=in_max)            elif in_type == 'Decreasing':                ds = canopy.dec_sigmoid(ds, c=in_min, d=in_max)            elif in_type == 'Symmetric':                ds = canopy.bell_sigmoid(ds, a=in_min, bc=in_max, d=in_min)        except:            arcpy.AddError('Could not perform signoidal.')            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Cleaning up output dataset...')        arcpy.SetProgressorPosition(4)        # apply nan mask to be safe        ds = ds.where(ds_mask)        # name single variable to sigmoid        ds = ds.to_dataset(name='sigmoid').squeeze(drop=True)        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(5)        # append attrbutes on to dataset and bands        ds.attrs = ds_attrs        ds['spatial_ref'].attrs = ds_spatial_ref_attrs        for var in list(ds.data_vars):            ds[var].attrs = ds_band_attrs                    # now that we have forced nan as nodata, update attribute         ds.attrs.update({'nodatavals': np.nan})        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(6)           # export netcdf file        tools.export_xr_as_nc(ds=ds, filename=out_nc)        # # # # #        # add to map if requested        if in_add_result_to_map:            # notify and increment progress bar            arcpy.SetProgressorLabel('Adding sigmoidal to map...')            arcpy.SetProgressorPosition(8)            try:                # open current map                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap                                # remove sigmoidal layer if exists                for layer in m.listLayers():                    if layer.name == 'sigmoid.crf':                        m.removeLayer(layer)                # create output folder using datetime as name                dt = datetime.datetime.now().strftime('%d%m%Y%H%M%S')                out_folder = os.path.join(os.path.dirname(out_nc), 'sigmoid' + '_' + dt)                os.makedirs(out_folder)                # disable visualise on map temporarily                arcpy.env.addOutputsToMap = False                # create crf filename and copy it                out_file = os.path.join(out_folder, 'sigmoid.crf')                crf = arcpy.CopyRaster_management(in_raster=out_nc,                                                   out_rasterdataset=out_file)                # add to map                                  m.addDataFromPath(crf)               except:                arcpy.AddWarning('Could not visualise output. Aborting visualisation.')                            try:                # get symbology, update it                layer = m.listLayers('sigmoid.crf')[0]                sym = layer.symbology                                # if layer has stretch coloriser, apply color                if hasattr(sym, 'colorizer'):                    if sym.colorizer.type == 'RasterStretchColorizer':                        # apply percent clip type                        sym.colorizer.stretchType = 'PercentClip'                        sym.colorizer.minPercent = 0.01                        sym.colorizer.maxPercent = 0.99                        # apply color map                        cmap = aprx.listColorRamps('Spectrum By Wavelength-Full Bright')[0]                        sym.colorizer.colorRamp = cmap                        # apply other basic options                        sym.colorizer.invertColorRamp = False                        sym.colorizer.gamma = 1.0                        # update symbology                        layer.symbology = sym            except:                arcpy.AddWarning('Could not visualise layer GeoTiffs.')        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(7)        # close main dataset        ds.close()        del ds                # close mask dataset        ds_mask.close()        del ds_mask        # notify user        arcpy.AddMessage('Generated Sigmoidal successfully.')        returnclass Ensemble_Model(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = 'Ensemble Model'        self.description = 'Combine two or more evidence layers into ' \                           'belief/disbelief/plausability/confidence outputs .' \                           'The benefit of this is areas of certainty and ' \                           'uncertainty can be derived, providing a better ' \                           'understanding of where the model can be trusted.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set various ArcGIS Pro UI controls. Data validation        is enforced via ArcGIS Pro API.        """                # input input sigmoidals        par_layers = arcpy.Parameter(                       displayName='Input fuzzy evidence layers',                       name='in_layers',                       datatype='GPValueTable',                       parameterType='Required',                       direction='Input',                       multiValue=False)        par_layers.columns = [            ['DEFile', 'NetCDF File'],             ['GPString', 'Evidence Type']            ]        par_layers.filters[0].list = ['nc']        par_layers.filters[1].type = 'ValueList'        par_layers.filters[1].list = ['Belief', 'Disbelief']                # output netcdf file        par_out_nc = arcpy.Parameter(                       displayName='Output ensemble model (NetCDF)',                       name='out_nc',                       datatype='DEFile',                       parameterType='Required',                       direction='Output')        par_out_nc.filter.list = ['nc']                # input resample        par_resample = arcpy.Parameter(                         displayName='Resample resolution',                         name='in_resample',                         datatype='GPString',                         parameterType='Required',                         direction='Input',                         #category='Ensemble Options',                         multiValue=False)        resample_to = ['Highest Resolution', 'Lowest Resolution']        par_resample.filter.type = 'ValueList'              par_resample.filter.list = resample_to        par_resample.value = 'Lowest Resolution'                # smoothing window        par_in_win_size = arcpy.Parameter(                          displayName='Smoothing window size',                          name='in_win_size',                          datatype='GPLong',                          parameterType='Optional',                          direction='Input',                          multiValue=False)        par_in_win_size.filter.type = 'Range'        par_in_win_size.filter.list = [3, 99]        par_in_win_size.value = None                # input add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True                params = [            par_layers,            par_out_nc,            par_resample,            par_in_win_size,            par_add_result_to_map]        return params    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the Ensemble Model module.        """        # safe imports        import os             # arcgis comes with these        import datetime       # arcgis comes with these        import tempfile       # arcgis comes with these        # risky imports (not native to arcgis)        try:            import numpy as np            import xarray as xr        except:            arcpy.AddError('Python libraries xarray and dask not installed.')            return        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools            # module folder            sys.path.append(FOLDER_MODULES)            import canopy, ensemble        except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return        # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        # grab parameter values         in_layers = parameters[0].value              # input layers (value array)        out_nc = parameters[1].valueAsText           # output netcdf        in_resample = parameters[2].value            # resample resolution        in_win_size = parameters[3].value            # smoothing window size        in_add_result_to_map = parameters[4].value   # add result to map        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning Ensemble Modelling.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=9)        # check layers isnt empty        if len(in_layers) == 0:            arcpy.AddError('No fuzzy layers provided.')            return        # get all layers types and check        layer_types = [layer[1] for layer in in_layers]        if 'Belief' not in layer_types:            arcpy.AddError('Layers must have at least one Belief layer.')            return        elif 'Disbelief' not in layer_types:            arcpy.AddError('Layers must have at least one Disbelief layer.')            return        elif len(np.unique(layer_types)) != 2:            arcpy.AddError('Layers must have only contain Belief and Disbelief types.')            return                    # check resample        if in_resample not in ['Lowest Resolution', 'Highest Resolution']:            arcpy.AddWarning('Resample type not supported, setting to default.')            in_resample = 'Lowest Resolution'                    # check smooth window size (we support none, for no smoothing)        if in_win_size is not None:            if not isinstance(in_win_size, int):                arcpy.AddWarning('Smoothing window size must be integer, setting to default.')                in_win_size = 3            elif in_win_size < 3:                arcpy.AddWarning('Smoothing window size must be 3 or above, setting to default.')                in_win_size = 3        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Checking and loading input datasets...')        arcpy.SetProgressorPosition(1)        # iterate layers for check        ds_list = []        for layer in in_layers:                    try:                ds = xr.open_dataset(layer[0].value)                            except:                arcpy.AddError('Could not load input NetCDF {}.'.format(layer[0].value))                return                            # check xr type, vars, coords, dims, attrs            if not isinstance(ds, xr.Dataset):                arcpy.AddError('Input NetCDF must be a xr dataset.')                return            elif 'sigmoid' not in ds:                arcpy.AddError('Input NetCDF does not contain a sigmoid variable.')                return            elif 'time' in ds and ds['time'] > 1:                arcpy.AddError('Input NetCDF must not have a time dimension.')                return            elif len(ds.data_vars) == 0:                arcpy.AddError('Input NetCDF has no data/variables/bands.')                return            elif 'x' not in list(ds.coords) or 'y' not in list(ds.coords):                arcpy.AddError('Input NetCDF must have x, y coords.')                return            elif 'spatial_ref' not in list(ds.coords):                arcpy.AddError('Input NetCDF must have a spatial_ref coord.')                return            elif 'x' not in list(ds.dims) or 'y' not in list(ds.dims):                arcpy.AddError('Input NetCDF must have x, y dimensions.')                return            elif len(ds['x']) == 0 or len(ds['y']) == 0:                arcpy.AddError('Input NetCDF must have at least one x, y index.')                return            elif ds.attrs == {}:                arcpy.AddError('NetCDF attributes not found. NetCDF must have attributes.')                return            elif not hasattr(ds, 'crs'):                arcpy.AddError('NetCDF CRS attribute not found. CRS required.')                return            elif ds.crs != 'EPSG:3577':                arcpy.AddError('NetCDF CRS is not EPSG:3577. EPSG:3577 required.')                            return            elif not hasattr(ds, 'nodatavals'):                arcpy.AddError('NetCDF nodatavals attribute not found.')                            return                            # check if xr is all nan            if ds.to_array().isnull().all():                arcpy.AddError('Input data is completely null.')                            return            try:                # do proper load with dask, set nodata to nan                ds = satfetcher.load_local_nc(nc_path=layer[0].value,                                               use_dask=True,                                               conform_nodata_to=np.nan)            except:                arcpy.AddError('Could not properly load input file.')                return                            # add belief type to ds, will be striped later            ds.attrs.update({'evi_type': layer[1]})                            # add to dataset list            ds_list.append(ds)        # check at least one belief and disbelief layer captured        if len(ds_list) == 0:            arcpy.AddError('Insufficient processed datasets.')                        return                                            # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Checking dataset extents intersect...')        arcpy.SetProgressorPosition(2)                    # check extents overlap        if not tools.all_xr_intersect(ds_list):            arcpy.AddError('Not all input layers intersect.')                        return                                             # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Conforming datasets via resampling...')        arcpy.SetProgressorPosition(3)        # select target resolution dataset        ds_target = ensemble.get_target_res_xr(ds_list, in_resample)        # check if somethign returned        if ds_target is None:            arcpy.AddError('Could not obtain optimal resolution dataset.')                        return            try:            # resample all datasets to target dataset            for idx in range(len(ds_list)):                ds_list[idx] = tools.resample_xr(ds_from=ds_list[idx],                                                  ds_to=ds_target,                                                 resampling='nearest')                                                                 # squeeze                 ds_list[idx] = ds_list[idx].squeeze(drop=True)        except:            arcpy.AddError('Could not resmaple datasets.')            return         # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Smoothing datasets, if requested...')        arcpy.SetProgressorPosition(4)        # smooth each dataset, if requested        if in_win_size is not None:            try:                # smooth each dataset, eject if none (error)                for idx in range(len(ds_list)):                    ds_list[idx] = ensemble.smooth_xr_dataset(ds_list[idx], in_win_size)                                        if ds_list[idx] is None:                        arcpy.AddError('Could not smooth datasets.')                        return            except:                arcpy.AddError('Could not smooth datasets.')                return                                    # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading datasets into memory, please wait...')        arcpy.SetProgressorPosition(5)        # load each dataset        for ds in ds_list:             ds.load()        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Splitting datasets into belief and disbelief...')        arcpy.SetProgressorPosition(5)        # split datasets via evidence type attribute        beliefs = [ds for ds in ds_list if ds.evi_type == 'Belief']        disbeliefs = [ds for ds in ds_list if ds.evi_type == 'Disbelief']                      # check something was returned for each        if len(beliefs) == 0 or len(disbeliefs) == 0:            arcpy.AddError('Could not split datasets into belief and disbelief.')            return                    # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Performing ensemble modelling...')        arcpy.SetProgressorPosition(6)        try:            # perfom ensemble modelling            ds = ensemble.perform_modelling(belief=beliefs,                                            disbelief=disbeliefs)                        # create and add attributes to dataset            ds = tools.manual_create_xr_attrs(ds)                        # add nodatavals attribute, as we used nan             ds.attrs.update({'nodatavals': np.nan})                    except:            arcpy.AddError('Could not perform ensemble modelling.')            return        # check if dataset exists        if ds is None:            arcpy.AddError('No result was produced from ensemble modelling.')            return                                            # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(7)        # export netcdf file        tools.export_xr_as_nc(ds=ds, filename=out_nc)        # # # # #        # add multi-dim raster to current map        if in_add_result_to_map:            # notify and increment progess bar            arcpy.SetProgressorLabel('Adding outputs to current map...')            arcpy.SetProgressorPosition(8)            # create output folder with dt            dt = datetime.datetime.now().strftime("%d%m%Y%H%M%S")            out_folder = os.path.join(os.path.dirname(out_nc), 'ensemble' + '_' + dt)            os.makedirs(out_folder)            try:                # open current map                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap                                # remove existing ensemble layers if exist                for layer in m.listLayers():                    if layer.isGroupLayer and layer.name == 'ensemble':                        m.removeLayer(layer)                # setup a group layer via template                grp_lyr = arcpy.mp.LayerFile(GRP_LYR_FILE)                grp = m.addLayer(grp_lyr)[0]                grp.name = 'ensemble'                                # disable visual add to map                arcpy.env.addOutputsToMap = False                                        # loop each var and export a seperate crf                tif_list = []                for var in ds:                                                              # create temporary netcdfil for one var (prevents 2.9 bug)                    with tempfile.NamedTemporaryFile() as tmp:                        tmp_nc = '{}_{}.nc'.format(tmp.name, var)                        ds[[var]].to_netcdf(tmp_nc)                    # build in-memory crf for temp netcdf                    crf = arcpy.md.MakeMultidimensionalRasterLayer(in_multidimensional_raster=tmp_nc,                                                                    out_multidimensional_raster_layer=var)                                        # export temp tif                    tmp_tif = os.path.join(out_folder, '{}.tif'.format(var))                    arcpy.management.CopyRaster(in_raster=crf, out_rasterdataset=tmp_tif)                                        # add temp tif to map abd get as layer                    m.addDataFromPath(tmp_tif)                    layer = m.listLayers('{}.tif'.format(var))[0]                    # add layer to group and then remove outside layer                    m.addLayerToGroup(grp, layer, 'BOTTOM')                    m.removeLayer(layer)                                         # success, add store current layer for symbology below                    tif_list.append('{}.tif'.format(var))            except:                arcpy.AddWarning('Could not create layer GeoTiffs.')                            try:                       # iter tif layer names, get symbology, update it                for tif in tif_list:                    layer = m.listLayers(tif)[0]                    sym = layer.symbology                    # if layer has stretch coloriser, apply color                    if hasattr(sym, 'colorizer'):                        if sym.colorizer.type == 'RasterStretchColorizer':                            # apply percent clip type                            sym.colorizer.stretchType = 'PercentClip'                            sym.colorizer.minPercent = 0                            sym.colorizer.maxPercent = 1                            # apply color map                            cmap = aprx.listColorRamps('Spectrum By Wavelength-Full Bright')[0]                            sym.colorizer.colorRamp = cmap                            # apply other basic options                            sym.colorizer.invertColorRamp = False                            sym.colorizer.gamma = 1.0                            # update symbology                            layer.symbology = sym            except:                arcpy.AddWarning('Could not visualise layer GeoTiffs.')        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(9)                    # close ensemble dataset         ds.close()        del ds        # close all xr datasets in input lists        for ds in ds_list + beliefs + disbeliefs:            ds.close()        # notify user        arcpy.AddMessage('Performed Ensemble Modelling successfully.')        returnclass Ensemble_Masker(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = 'Ensemble Masker'        self.description = "Use another NetCDF or GeoTiff layer to mask " \                           "out areas from previously generated Ensemble modelling " \                           "outputs."        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """        # input netcdf or geotiff file        par_in_file = arcpy.Parameter(                        displayName='Input ensemble model (NetCDF)',                        name='in_file',                        datatype='DEFile',                        parameterType='Required',                        direction='Input')        par_in_file.filter.list = ['nc']                # output netcdf file        par_out_file = arcpy.Parameter(                         displayName='Output masked ensemble model (NetCDF)',                         name='out_file',                         datatype='DEFile',                         parameterType='Required',                         direction='Output')        par_out_file.filter.list = ['nc']                # input netcdf or geotiff file        par_in_mask_file = arcpy.Parameter(                             displayName='Input mask layer (NetCDF or GeoTiff)',                             name='in_mask',                             datatype='DEFile',                             parameterType='Required',                             direction='Input')        par_in_mask_file.filter.list = ['nc', 'tif']                # input variable        par_in_var = arcpy.Parameter(                       displayName='Mask variable',                       name='in_var',                       datatype='GPString',                       parameterType='Required',                       direction='Input')        par_in_var.filter.type = 'ValueList'              par_in_var.filter.list = []                    # input mask type        par_in_type = arcpy.Parameter(                        displayName='Mask type',                        name='in_type',                        datatype='GPString',                        parameterType='Required',                        direction='Input')        par_in_type.filter.type = 'ValueList'              par_in_type.filter.list = ['Binary', 'Range']          par_in_type.value = 'Binary'                # input binary mask value        par_in_bin = arcpy.Parameter(                             displayName='Mask out value',                             name='in_binary',                             datatype='GPDouble',                             parameterType='Optional',                             direction='Input')        # input range minimum        par_range_min = arcpy.Parameter(                                displayName='Minimum mask out value',                                name='in_range_min',                                datatype='GPDouble',                                parameterType='Optional',                                direction='Input')                                        # input range maximum        par_range_max = arcpy.Parameter(                                displayName='Maximum mask out value',                                name='in_range_max',                                datatype='GPDouble',                                parameterType='Optional',                                direction='Input')        # input add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True        params = [            par_in_file,             par_out_file,            par_in_mask_file,            par_in_var,            par_in_type,            par_in_bin,            par_range_min,            par_range_max,            par_add_result_to_map            ]        return params    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # imports        try:            import xarray as xr            import rasterio        except:            arcpy.AddError('Could not import Xarray or rasterio.')            return                    # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import satfetcher        except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return                    # globals        global ENSEMBLE_MASKER        # unpack global parameter values         curr_file = ENSEMBLE_MASKER.get('in_file')        curr_var = ENSEMBLE_MASKER.get('in_var')                # if input file added, run        if parameters[2].value is not None:                    # if global has no matching file (or first run), reload all            if curr_file != parameters[2].valueAsText:                if parameters[2].valueAsText.endswith('.nc'):                    try:                        ds = xr.open_dataset(parameters[2].valueAsText)                        data_vars = [var for var in ds]                        ds.close()                    except:                        data_vars = []                                elif parameters[2].valueAsText.endswith('.tif'):                    try:                        da = xr.open_rasterio(parameters[2].valueAsText)                        data_vars = ['{}'.format(var) for var in da['band'].values]                        da.close()                    except:                        data_vars = []                                          # populate var list with new vars                parameters[3].filter.list = data_vars                                # set var and bin, min, max to no selections                parameters[3].value = None                parameters[5].value = None                parameters[6].value = None                parameters[7].value = None              # calc min, max if var changed            elif curr_var != parameters[3].valueAsText:                new_var = parameters[3].valueAsText                new_type = parameters[4].value                if parameters[2].valueAsText.endswith('.nc'):                    try:                        ds = xr.open_dataset(parameters[2].valueAsText)                        da = ds[new_var]                                                if hasattr(ds, 'nodatavals'):                            da = da.where(da != ds.nodatavals)                                                mins = round(float(da.min()), 3)                        maxs = round(float(da.max()), 3)                                                ds.close()                    except:                        mins, maxs = None, None                                elif parameters[2].valueAsText.endswith('.tif'):                    try:                        ds = xr.open_rasterio(parameters[2].valueAsText)                        da = ds.sel(band=int(new_var))                                                if hasattr(ds, 'nodatavals') and len(ds.nodatavals) == 1:                            da = da.where(da != ds.nodatavals)                        mins = round(float(da.min()), 3)                        maxs = round(float(da.max()), 3)                            ds.close()                    except:                        mins, maxs = None, None                # set range min, max                parameters[6].value = mins                parameters[7].value = maxs                        # update global values        ENSEMBLE_MASKER = {            'in_file': parameters[2].valueAsText,            'in_var': parameters[3].value,        }        # enable binary or range parameters based on drop down        if parameters[4].value == 'Binary':            parameters[5].enabled = True             parameters[6].enabled = False             parameters[7].enabled = False         else:            parameters[5].enabled = False             parameters[6].enabled = True             parameters[7].enabled = True                 return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the Ensemble Masker module.        """        # safe imports        import os             # arcgis comes with these        import datetime       # arcgis comes with these        import tempfile       # arcgis comes with these        # risky imports (not native to arcgis)        try:            import numpy as np            import xarray as xr        except:            arcpy.AddError('Python libraries xarray and dask not installed.')            return        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools            # module folder            sys.path.append(FOLDER_MODULES)            import ensemble        except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return        # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        # grab parameter values         in_file = parameters[0].valueAsText          # input netcdf        out_nc = parameters[1].valueAsText           # output netcdf        in_mask_file = parameters[2].valueAsText     # input mask nc or tif        in_var = parameters[3].value                 # variable        in_type = parameters[4].value                # mask type        in_bin = parameters[5].value                 # binary value        in_range_min = parameters[6].value           # range minimum        in_range_max = parameters[7].value           # range maximum        in_add_result_to_map = parameters[8].value   # add result to map        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning Ensemble Masker.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=7)        # check variable        if in_var is None:            arcpy.AddError('No variable selected.')            return        # check type        if in_type not in ['Binary', 'Range']:            arcpy.AddError('Mask type not supported.')            return                    # check binary value        if in_type == 'Binary' and in_bin is None:            arcpy.AddError('Must provide a mask value when using binary type.')            return                    # check range values        if in_type == 'Range':            if in_range_min is None or in_range_max is None:                arcpy.AddError('Must provide a min and max value when using range type.')                return            elif in_range_max <= in_range_min:                arcpy.AddError('Range maximum can not be <= minimum.')                return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Checking input data...')        arcpy.SetProgressorPosition(1)        # check input ensemble and mask files        for file in [in_file, in_mask_file]:                        # get and check extension            in_ext = os.path.splitext(file)[1]            if in_ext not in ['.nc', '.tif']:                arcpy.AddError('File type not supported.')                return            try:                # do quick load depending on input type                if in_ext == '.nc':                    ds = xr.open_dataset(file)                else:                    ds = xr.open_rasterio(file)                    ds = ds.to_dataset(dim='band')                    ds = tools.build_xr_attributes(ds)               except:                arcpy.AddError('Could not quick load input file.')                return            # check xr type, vars, coords, dims, attrs            if not isinstance(ds, xr.Dataset):                arcpy.AddError('Input NetCDF must be a xr dataset.')                return            elif 'time' in ds:                arcpy.AddError('Input NetCDF must not have a time dimension.')                return            elif len(ds.data_vars) == 0:                arcpy.AddError('Input NetCDF has no data/variables/bands.')                return            elif in_ext == '.tif' and len(ds) > 1:                arcpy.AddError('Input GeoTiff must have only one band.')                return            elif 'x' not in list(ds.coords) or 'y' not in list(ds.coords):                arcpy.AddError('Input NetCDF must have x, y coords.')                return            elif 'spatial_ref' not in list(ds.coords):                arcpy.AddError('Input NetCDF must have a spatial_ref coord.')                return            elif 'x' not in list(ds.dims) or 'y' not in list(ds.dims):                arcpy.AddError('Input NetCDF must have x, y dimensions.')                return            elif len(ds['x']) == 0 or len(ds['y']) == 0:                arcpy.AddError('Input NetCDF must have at least one x, y index.')                return            elif ds.attrs == {}:                arcpy.AddError('NetCDF attributes not found. NetCDF must have attributes.')                return            elif not hasattr(ds, 'crs'):                arcpy.AddError('NetCDF CRS attribute not found. CRS required.')                return            elif ds.crs != 'EPSG:3577':                arcpy.AddError('NetCDF CRS is not EPSG:3577. EPSG:3577 required.')                            return            elif not hasattr(ds, 'nodatavals'):                arcpy.AddError('NetCDF nodatavals attribute not found.')                            return            # check if xr is all nan/0 via centroid pixel timeseries (saves full load)            if ds.to_array().isnull().all():                arcpy.AddError('Input data is completely null.')                            return                                         # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading input ensemble data...')        arcpy.SetProgressorPosition(2)        try:            # now, do proper load on ensemble netcdf, set nodata to nan            ds_ensemble = satfetcher.load_local_nc(nc_path=in_file,                                                    use_dask=False,                                                    conform_nodata_to=np.nan)        except:            arcpy.AddError('Could not properly load input ensemble NetCDF.')            return                    # check if required variables exist        for var in ds_ensemble:            if var not in ['belief', 'disbelief', 'plausability', 'interval']:                arcpy.AddError('Ensemble NetCDF is invalid does not have required variables.')                return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading input mask data...')        arcpy.SetProgressorPosition(3)        try:            # get mask file extension            in_mask_ext = os.path.splitext(in_mask_file)[1]                           # load mask file depending on file, set nodata to nan            if in_mask_ext == '.nc':                ds_mask = satfetcher.load_local_nc(nc_path=in_mask_file,                                                    use_dask=False,                                                    conform_nodata_to=np.nan)            else:                ds_mask = satfetcher.load_local_rasters(rast_path_list=in_mask_file,                                                         use_dask=False,                                                         conform_nodata_to=np.nan)                ds_mask = tools.build_xr_attributes(ds_mask)         except:            arcpy.AddError('Could not properly load input mask file.')            return                                            # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Getting ensemble file attributes...')        arcpy.SetProgressorPosition(4)        # get attributes from dataset        ds_attrs = ds_ensemble.attrs        ds_band_attrs = ds_ensemble[list(ds_ensemble.data_vars)[0]].attrs        ds_spatial_ref_attrs = ds_ensemble['spatial_ref'].attrs        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Checking dataset extents intersect...')        arcpy.SetProgressorPosition(5)        # check extents overlap        if not tools.all_xr_intersect([ds_ensemble, ds_mask]):            arcpy.AddError('Not all input layers intersect.')                        return                                 # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Conforming mask resolution via resampling...')        arcpy.SetProgressorPosition(6)        try:            # resample mask to ensemble dataset, if same, no change            ds_mask = tools.resample_xr(ds_from=ds_mask,                                         ds_to=ds_ensemble,                                        resampling='nearest')        except:            arcpy.AddError('Could not resamaple datasets.')            return         # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading datasets into memory, please wait...')        arcpy.SetProgressorPosition(7)        # load each dataset        ds_ensemble.load()        ds_mask.load()        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Preparing mask values...')        arcpy.SetProgressorPosition(8)               try:            # prepare mask depending on user choice            if in_type == 'Binary':                ds_mask = xr.where(ds_mask != in_bin, True, False)            else:                ds_mask = xr.where((ds_mask < in_range_min) |                                    (ds_mask > in_range_max), True, False)        except:            arcpy.AddError('Could not properly prepare mask dataset.')            return                                            # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Subsetting mask dataset to specified variable...')        arcpy.SetProgressorPosition(9)        # get mask file extension        in_mask_ext = os.path.splitext(in_mask_file)[1]                  # check if requested var in netcdf (tif always has one)        if in_mask_ext == '.nc' and in_var not in ds_mask:            arcpy.AddError('Requested variable not found in NetCDF.')            return        # subset dataset if netcdf (tif always have one), set both as array        ds_mask = ds_mask[in_var] if in_mask_ext == '.nc' else ds_mask.to_array()                                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Masking ensemble dataset via mask...')        arcpy.SetProgressorPosition(10)        # mask out any values under mask to nan and squeeze        ds_ensemble = ds_ensemble.where(ds_mask).squeeze(drop=True)        # check if any values exist in ensemble dataset        if ds_ensemble.to_array().isnull().all() == True:            arcpy.AddError('Ensemble dataset has no values after mask, check mask.')            return                                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(11)        # append attrbutes on to dataset and bands        ds_ensemble.attrs = ds_attrs        ds_ensemble['spatial_ref'].attrs = ds_spatial_ref_attrs        for var in ds_ensemble:            ds_ensemble[var].attrs = ds_band_attrs                    # ensure nodata is nan, update attribute         ds_ensemble.attrs.update({'nodatavals': np.nan})                                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(12)           # export netcdf file        tools.export_xr_as_nc(ds=ds_ensemble, filename=out_nc)        # # # # #        # add multi-dim raster to current map        if in_add_result_to_map:            # notify and increment progess bar            arcpy.SetProgressorLabel('Adding outputs to current map...')            arcpy.SetProgressorPosition(13)            # create output folder with dt            dt = datetime.datetime.now().strftime("%d%m%Y%H%M%S")            out_folder = os.path.join(os.path.dirname(out_nc), 'ensemble_mask' + '_' + dt)            os.makedirs(out_folder)            try:                # open current map                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap                # remove existing ensemble layers if exist                for layer in m.listLayers():                    if layer.isGroupLayer and layer.name == 'ensemble_mask':                        m.removeLayer(layer)                # setup a group layer via template                grp_lyr = arcpy.mp.LayerFile(GRP_LYR_FILE)                grp = m.addLayer(grp_lyr)[0]                grp.name = 'ensemble_mask'                # disable visual add to map                arcpy.env.addOutputsToMap = False                # loop each var and export a seperate crf                tif_list = []                for var in ds_ensemble:                          # create temporary netcdfil for one var (prevents 2.9 bug)                    with tempfile.NamedTemporaryFile() as tmp:                        tmp_nc = '{}_{}.nc'.format(tmp.name, var)                        ds_ensemble[[var]].to_netcdf(tmp_nc)                    # build in-memory crf for temp netcdf                    crf = arcpy.md.MakeMultidimensionalRasterLayer(in_multidimensional_raster=tmp_nc,                                                                    out_multidimensional_raster_layer=var)                    # export temp tif                    tmp_tif = os.path.join(out_folder, '{}_mask.tif'.format(var))                    arcpy.management.CopyRaster(in_raster=crf, out_rasterdataset=tmp_tif)                    # add temp tif to map abd get as layer                    m.addDataFromPath(tmp_tif)                    layer = m.listLayers('{}_mask.tif'.format(var))[0]                    # add layer to group and then remove outside layer                    m.addLayerToGroup(grp, layer, 'BOTTOM')                    m.removeLayer(layer)                     # success, add store current layer for symbology below                    tif_list.append('{}_mask.tif'.format(var))            except:                arcpy.AddWarning('Could not create layer GeoTiffs.')            try:                       # iter tif layer names, get symbology, update it                for tif in tif_list:                    layer = m.listLayers(tif)[0]                    sym = layer.symbology                    # if layer has stretch coloriser, apply color                    if hasattr(sym, 'colorizer'):                        if sym.colorizer.type == 'RasterStretchColorizer':                            # apply percent clip type                            sym.colorizer.stretchType = 'PercentClip'                            sym.colorizer.minPercent = 0                            sym.colorizer.maxPercent = 1                            # apply color map                            cmap = aprx.listColorRamps('Spectrum By Wavelength-Full Bright')[0]                            sym.colorizer.colorRamp = cmap                            # apply other basic options                            sym.colorizer.invertColorRamp = False                            sym.colorizer.gamma = 1.0                            # update symbology                            layer.symbology = sym            except:                arcpy.AddWarning('Could not visualise layer GeoTiffs.')                                        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(9)        # close ensemble dataset         ds_ensemble.close()        del ds_ensemble        # close mask dataset        ds_mask.close()        del ds_mask        # notify user        arcpy.AddMessage('Performed Ensemble Masking successfully.')        returnclass NRT_Create_Project(object):    def __init__(self):        """        Initialise tool.        """        self.label = "NRT Create Project"        self.description = "Create a new geodatabase to hold monitoring areas."        self.canRunInBackground = False    def getParameterInfo(self):        """        Set various ArcGIS Pro UI controls. Data validation        is enforced via ArcGIS Pro API.        """                # output gdb folder        par_out_folder = arcpy.Parameter(                           displayName='Output Project Folder',                           name='out_folder',                           datatype='DEFolder',                           parameterType='Required',                           direction='Input')                                     # combine parameters        parameters = [par_out_folder]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the NRT Create Project module.        """                # safe imports        import os        import arcpy                        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc                    # module folder            sys.path.append(FOLDER_MODULES)            import nrt        except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return                 # grab parameter values         out_folder = parameters[0].valueAsText      # output gdb folder path                # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning NRT Create Project.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=7)                                    # check inputs are not none and strings        if out_folder is None:            arcpy.AddError('Blank project folder provided.')            return        elif not isinstance(out_folder, str):            arcpy.AddError('Project Folder is not text type.')            return        # check if monitoring area gdb already exists         gdb_path = os.path.join(out_folder, 'monitoring_areas.gdb')        if os.path.exists(gdb_path):            arcpy.AddError('Monitoring area project already exists, use a different folder.')            return                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Creating new monitoring project database...')        arcpy.SetProgressorPosition(1)        # build project geodatbase and empty json file        out_filepath = arcpy.management.CreateFileGDB(out_folder, 'monitoring_areas.gdb')                    # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Generating database feature class...')        arcpy.SetProgressorPosition(2)        # temporarily disable auto-visual of outputs        arcpy.env.addOutputsToMap = False        # create feature class and aus albers spatial ref sys        srs = arcpy.SpatialReference(3577)        out_feat = arcpy.management.CreateFeatureclass(out_path=out_filepath,                                                        out_name='monitoring_areas',                                                        geometry_type='POLYGON',                                                       spatial_reference=srs)        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Generating database domains...')        arcpy.SetProgressorPosition(3)        # create platform domain        arcpy.management.CreateDomain(in_workspace=out_filepath,                                       domain_name='dom_platforms',                                       domain_description='Platform name (Landsat or Sentinel)',                                      field_type='TEXT',                                       domain_type='CODED')        # generate coded values to platform domain        dom_values = {'Landsat': 'Landsat', 'Sentinel': 'Sentinel'}        for dom_value in dom_values:            arcpy.management.AddCodedValueToDomain(in_workspace=out_filepath,                                                    domain_name='dom_platforms',                                                    code=dom_value,                                                    code_description=dom_values.get(dom_value))        # create year domain        arcpy.management.CreateDomain(in_workspace=out_filepath,                                       domain_name='dom_years',                                       domain_description='Training years (1980 - 2050)',                                      field_type='LONG',                                       domain_type='RANGE')        # generate range values to year domain        arcpy.management.SetValueForRangeDomain(in_workspace=out_filepath,                                                 domain_name='dom_years',                                                 min_value=1980,                                                 max_value=2050)        # create index domain        arcpy.management.CreateDomain(in_workspace=out_filepath,                                       domain_name='dom_indices',                                       domain_description='Vegetation index name',                                      field_type='TEXT',                                       domain_type='CODED')        # generate coded values to index domain        dom_values = {'NDVI': 'NDVI', 'MAVI': 'MAVI', 'kNDVI': 'kNDVI'}        for dom_value in dom_values:            arcpy.management.AddCodedValueToDomain(in_workspace=out_filepath,                                                    domain_name='dom_indices',                                                    code=dom_value,                                                    code_description=dom_values.get(dom_value))        # create persistence domain        arcpy.management.CreateDomain(in_workspace=out_filepath,                                       domain_name='dom_persistence',                                       domain_description='Vegetation persistence (0.001 - 9.999)',                                      field_type='FLOAT',                                       domain_type='RANGE')        # generate range values to persistence domain        arcpy.management.SetValueForRangeDomain(in_workspace=out_filepath,                                                 domain_name='dom_persistence',                                                 min_value=0.001,                                                 max_value=9.999)        # create rule 1 min consequtives domain        arcpy.management.CreateDomain(in_workspace=out_filepath,                                       domain_name='dom_rule_1_consequtives',                                       domain_description='Rule 1 Consequtives (0 - 999)',                                      field_type='LONG',                                       domain_type='RANGE')        # generate range values to consequtives domain        arcpy.management.SetValueForRangeDomain(in_workspace=out_filepath,                                                 domain_name='dom_rule_1_consequtives',                                                 min_value=0,                                                 max_value=999)        # create rule 2 min stdv domain        arcpy.management.CreateDomain(in_workspace=out_filepath,                                       domain_name='dom_rule_2_min_stdv',                                       domain_description='Rule 2 Minimum Stdvs (1 - 99)',                                      field_type='LONG',                                       domain_type='RANGE')        # generate range values to consequtives domain        arcpy.management.SetValueForRangeDomain(in_workspace=out_filepath,                                                 domain_name='dom_rule_2_min_stdv',                                                 min_value=1,                                                 max_value=99)        # create rule 3 num zones domain        arcpy.management.CreateDomain(in_workspace=out_filepath,                                       domain_name='dom_rule_3_num_zones',                                       domain_description='Rule 3 Num Zones (1 - 12)',                                      field_type='LONG',                                       domain_type='RANGE')        # generate range values to consequtives domain        arcpy.management.SetValueForRangeDomain(in_workspace=out_filepath,                                                 domain_name='dom_rule_3_num_zones',                                                 min_value=1,                                                 max_value=11)        # create ruleset domain        arcpy.management.CreateDomain(in_workspace=out_filepath,                                       domain_name='dom_ruleset',                                       domain_description='Various rulesets',                                      field_type='TEXT',                                       domain_type='CODED')        # generate coded values to ruleset domain           dom_values = {            '1':     '1 Only',            '2':     '2 Only',            '3':     '3 Only',            '1&2':   '1 and 2',            '1&3':   '1 and 3',            '2&3':   '2 and 3',            '1|2':   '1 or 2',            '1|3':   '1 or 3',            '2|3':   '2 or 3',            '1&2&3': '1 and 2 and 3',            '1|2&3': '1 or 2 and 3',            '1&2|3': '1 and 2 or 3',            '1|2|3': '1 or 2 or 3'            }              for dom_value in dom_values:            arcpy.management.AddCodedValueToDomain(in_workspace=out_filepath,                                                    domain_name='dom_ruleset',                                                    code=dom_value,                                                    code_description=dom_values.get(dom_value))                                                    # create alert method domain        arcpy.management.CreateDomain(in_workspace=out_filepath,                                       domain_name='dom_alert_method',                                       domain_description='Change detection alert method',                                      field_type='TEXT',                                       domain_type='CODED')        # generate coded values to alert method domain         dom_values = {'static': 'Static', 'dynamic': 'Dynamic'}        for dom_value in dom_values:            arcpy.management.AddCodedValueToDomain(in_workspace=out_filepath,                                                    domain_name='dom_alert_method',                                                    code=dom_value,                                                    code_description=dom_values.get(dom_value))                # create alert direction domain        arcpy.management.CreateDomain(in_workspace=out_filepath,                                       domain_name='dom_alert_direction',                                       domain_description='Alert directions',                                      field_type='TEXT',                                       domain_type='CODED')        # generate coded values to boolean domain        dom_values = {'Incline Only': 'Incline Only', 'Decline Only': 'Decline Only', 'Both': 'Both'}        for dom_value in dom_values:            arcpy.management.AddCodedValueToDomain(in_workspace=out_filepath,                                                    domain_name='dom_alert_direction',                                                    code=dom_value,                                                    code_description=dom_values.get(dom_value))        # create boolean domain        arcpy.management.CreateDomain(in_workspace=out_filepath,                                       domain_name='dom_boolean',                                       domain_description='Boolean (Yes or No)',                                      field_type='TEXT',                                       domain_type='CODED')        # generate coded values to boolean domain        dom_values = {'Yes': 'Yes', 'No': 'No'}        for dom_value in dom_values:            arcpy.management.AddCodedValueToDomain(in_workspace=out_filepath,                                                    domain_name='dom_boolean',                                                    code=dom_value,                                                    code_description=dom_values.get(dom_value))        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Generating database fields...')        arcpy.SetProgressorPosition(4)        # add area id field to featureclass           arcpy.management.AddField(in_table=out_feat,                                   field_name='area_id',                                   field_type='TEXT',                                   field_alias='Area ID',                                  field_length=200,                                  field_is_required='REQUIRED')        # add platforms field to featureclass           arcpy.management.AddField(in_table=out_feat,                                   field_name='platform',                                   field_type='TEXT',                                   field_alias='Platform',                                  field_length=20,                                  field_is_required='REQUIRED',                                  field_domain='dom_platforms')            # add s_year field to featureclass           arcpy.management.AddField(in_table=out_feat,                                   field_name='s_year',                                   field_type='LONG',                                   field_alias='Start Year of Training Period',                                  field_is_required='REQUIRED',                                  field_domain='dom_years')        # add e_year field to featureclass           arcpy.management.AddField(in_table=out_feat,                                   field_name='e_year',                                   field_type='LONG',                                   field_alias='End Year of Training Period',                                  field_is_required='REQUIRED',                                  field_domain='dom_years')        # add index field to featureclass           arcpy.management.AddField(in_table=out_feat,                                   field_name='index',                                   field_type='TEXT',                                   field_alias='Vegetation Index',                                  field_length=20,                                  field_is_required='REQUIRED',                                  field_domain='dom_indices')        # add persistence field to featureclass           arcpy.management.AddField(in_table=out_feat,                                   field_name='persistence',                                   field_type='FLOAT',                                   field_alias='Vegetation Persistence',                                  field_is_required='REQUIRED',                                  field_domain='dom_persistence')        # add rule 1 min consequtives field to featureclass           arcpy.management.AddField(in_table=out_feat,                                   field_name='rule_1_min_conseqs',                                   field_type='LONG',                                   field_alias='Rule 1 Minimum Consequtives',                                  field_is_required='REQUIRED',                                  field_domain='dom_rule_1_consequtives')        # add include plateaus field to featureclass           arcpy.management.AddField(in_table=out_feat,                                   field_name='rule_1_inc_plateaus',                                   field_type='TEXT',                                   field_alias='Rule 1 Include Pleateaus',                                  field_length=20,                                  field_is_required='REQUIRED',                                  field_domain='dom_boolean')        # add rule 2 min stdv field to featureclass           arcpy.management.AddField(in_table=out_feat,                                   field_name='rule_2_min_stdv',                                   field_type='LONG',                                   field_alias='Rule 2 Minimum Std Dev',                                  field_is_required='REQUIRED',                                  field_domain='dom_rule_2_min_stdv')        # add rule 2 bidirectional field to featureclass           arcpy.management.AddField(in_table=out_feat,                                   field_name='rule_2_bidirectional',                                   field_type='TEXT',                                   field_alias='Rule 2 Bidirectional',                                  field_length=20,                                  field_is_required='REQUIRED',                                  field_domain='dom_boolean')        # add rule 3 num zones field to featureclass           arcpy.management.AddField(in_table=out_feat,                                   field_name='rule_3_num_zones',                                   field_type='LONG',                                   field_alias='Rule 3 Number of Zones',                                  field_is_required='REQUIRED',                                  field_domain='dom_rule_3_num_zones')                                      # add ruleset field to featureclass           arcpy.management.AddField(in_table=out_feat,                                   field_name='ruleset',                                   field_type='TEXT',                                   field_alias='Ruleset',                                  field_length=20,                                  field_is_required='REQUIRED',                                  field_domain='dom_ruleset')              # add alert field to featureclass           arcpy.management.AddField(in_table=out_feat,                                   field_name='alert',                                   field_type='TEXT',                                   field_alias='Alert via Email',                                  field_is_required='REQUIRED',                                  field_domain='dom_boolean')           # add method field to featureclass           arcpy.management.AddField(in_table=out_feat,                                   field_name='method',                                   field_type='TEXT',                                   field_alias='Alert via Method',                                  field_length=20,                                  field_is_required='REQUIRED',                                  field_domain='dom_alert_method')           # add alert direction field to featureclass           arcpy.management.AddField(in_table=out_feat,                                   field_name='alert_direction',                                   field_type='TEXT',                                   field_alias='Change Direction to Trigger Alert',                                  field_is_required='REQUIRED',                                  field_domain='dom_alert_direction')        # add email field to featureclass           arcpy.management.AddField(in_table=out_feat,                                   field_name='email',                                   field_type='TEXT',                                   field_alias='Email of User',                                  field_is_required='REQUIRED')        # add ignore field to featureclass           arcpy.management.AddField(in_table=out_feat,                                   field_name='ignore',                                   field_type='TEXT',                                   field_alias='Ignore When Run',                                  field_is_required='REQUIRED',                                  field_domain='dom_boolean')                                             # add zone_color field to featureclass           arcpy.management.AddField(in_table=out_feat,                                   field_name='color',                                   field_type='LONG',                                   field_alias='Color Code',                                  field_is_required='REQUIRED')          # add global_id field to featureclass           arcpy.management.AddField(in_table=out_feat,                                   field_name='global_id',                                   field_type='TEXT',                                   field_alias='Global ID',                                  field_length=200,                                  field_is_required='REQUIRED')        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Generating database defaults...')        arcpy.SetProgressorPosition(5)        # set default platform        arcpy.management.AssignDefaultToField(in_table=out_feat,                                               field_name='platform',                                              default_value='Landsat')           # set default index        arcpy.management.AssignDefaultToField(in_table=out_feat,                                               field_name='index',                                              default_value='MAVI')          # set default persistence        arcpy.management.AssignDefaultToField(in_table=out_feat,                                               field_name='persistence',                                              default_value=1.0)        # set default min conseqs        arcpy.management.AssignDefaultToField(in_table=out_feat,                                               field_name='rule_1_min_conseqs',                                              default_value=3)        # set default inc plateaus        arcpy.management.AssignDefaultToField(in_table=out_feat,                                               field_name='rule_1_inc_plateaus',                                              default_value='No')        # set default min stdvs        arcpy.management.AssignDefaultToField(in_table=out_feat,                                               field_name='rule_2_min_stdv',                                              default_value=1)        # set default bidirectional        arcpy.management.AssignDefaultToField(in_table=out_feat,                                               field_name='rule_2_bidirectional',                                              default_value='Yes')        # set default num zones        arcpy.management.AssignDefaultToField(in_table=out_feat,                                               field_name='rule_3_num_zones',                                              default_value=1)        # set default ruleset        arcpy.management.AssignDefaultToField(in_table=out_feat,                                               field_name='ruleset',                                              default_value='1&2|3')        # set default alert        arcpy.management.AssignDefaultToField(in_table=out_feat,                                               field_name='alert',                                              default_value='No')                   # set default alert method        arcpy.management.AssignDefaultToField(in_table=out_feat,                                               field_name='method',                                              default_value='static')                  # set default alert direction        arcpy.management.AssignDefaultToField(in_table=out_feat,                                               field_name='alert_direction',                                              default_value='Both')           # set default ignore        arcpy.management.AssignDefaultToField(in_table=out_feat,                                               field_name='ignore',                                              default_value='No')                                                        # set default color        arcpy.management.AssignDefaultToField(in_table=out_feat,                                               field_name='color',                                              default_value=0)         # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Adding data to current map...')        arcpy.SetProgressorPosition(7)        # enable auto-visual of outputs        arcpy.env.addOutputsToMap = True        try:            # add layer to map            p = arcpy.mp.ArcGISProject('CURRENT')            m = p.activeMap            m.addDataFromPath(out_feat)        except:            arcpy.AddWarning('Could not find active map. Add monitor areas manually.')                    try:            # apply symbology to new layer            p = arcpy.mp.ArcGISProject('CURRENT')            m = p.activeMap            for layer in m.listLayers('monitoring_areas'):                arc.apply_monitoring_area_symbology(layer)        except:            arcpy.AddWarning('Could not apply symbology to montiroing areas. Skipping.')                            # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(7)        # finish up        arcpy.AddMessage('Created new NRT Project.')        returnclass NRT_Create_Monitoring_Areas(object):    def __init__(self):        """        Initialise tool.        """        self.label = "NRT Create Monitoring Areas"        self.description = "Create new NRT monitoring area features."        self.canRunInBackground = False    def getParameterInfo(self):        """        Set various ArcGIS Pro UI controls. Data validation        is enforced via ArcGIS Pro API.        """                # set input feature class        par_in_feat = arcpy.Parameter(                        displayName='Input Monitoring Area Featureclass',                        name='in_feat',                        datatype='GPFeatureLayer',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_in_feat.filter.list = ['Polygon']                # set input feature class        par_in_new_feat = arcpy.Parameter(                            displayName='Draw new monitoring area feature',                            name='in_new_feat',                            datatype='GPFeatureRecordSetLayer',                            parameterType='Required',                            direction='Input',                            multiValue=False)        par_in_new_feat.filter.list = ['Polygon']        par_in_new_feat.enabled = False                # set monitoring area        par_in_set_area = arcpy.Parameter(                            displayName='Set the monitoring area identifier',                            name='in_area_id',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            multiValue=False)        par_in_set_area.enabled = False                # platform        par_in_platform = arcpy.Parameter(                            displayName='Set satellite platform',                            name='in_platform',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            multiValue=False)        par_in_platform.filter.type = 'ValueList'        par_in_platform.filter.list = ['Landsat', 'Sentinel']        par_in_platform.value = 'Landsat'        par_in_platform.enabled = False                # start year        par_in_s_year = arcpy.Parameter(                          displayName='Set pre-impact start year',                          name='in_s_year',                          datatype='GPLong',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_in_s_year.filter.type = 'Range'        par_in_s_year.filter.list = [1980, 2050]        par_in_s_year.enabled = False                # end year        par_in_e_year = arcpy.Parameter(                          displayName='Set pre-impact end year',                          name='in_e_year',                          datatype='GPLong',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_in_e_year.filter.type = 'Range'        par_in_e_year.filter.list = [1980, 2050]        par_in_e_year.enabled = False        # vegetation index        par_in_veg_idx = arcpy.Parameter(                           displayName='Set vegetation index',                           name='in_veg_idx',                           datatype='GPString',                           parameterType='Required',                           direction='Input',                           multiValue=False)        par_in_veg_idx.filter.type = 'ValueList'        par_in_veg_idx.filter.list = ['NDVI', 'MAVI', 'kNDVI']        par_in_veg_idx.value = 'MAVI'        par_in_veg_idx.enabled = False                # persistence        par_in_persistence = arcpy.Parameter(                               displayName='Vegetation persistence',                               name='in_persistence',                               datatype='GPDouble',                               parameterType='Required',                               direction='Input',                               multiValue=False)        par_in_persistence.filter.type = 'Range'        par_in_persistence.filter.list = [0.001, 9.999]        par_in_persistence.value = 1.0        par_in_persistence.enabled = False               # rule 1 min conseqs        par_in_min_conseqs = arcpy.Parameter(                               displayName='Rule 1: Minimum consequtives',                               name='in_min_conseqs',                               datatype='GPLong',                               parameterType='Optional',                               direction='Input',                               category='Rules',                               multiValue=False)        par_in_min_conseqs.filter.type = 'Range'        par_in_min_conseqs.filter.list = [0, 99]        par_in_min_conseqs.value = 3        par_in_min_conseqs.enabled = False                # rule 1 include plateaus        par_in_inc_plateaus = arcpy.Parameter(                                displayName='Rule 1: Include plateaus',                                name='in_inc_plateaus',                                datatype='GPString',                                parameterType='Optional',                                direction='Input',                                category='Rules',                                multiValue=False)        par_in_inc_plateaus.filter.type = 'ValueList'        par_in_inc_plateaus.filter.list = ['Yes', 'No']        par_in_inc_plateaus.value = 'No'        par_in_inc_plateaus.enabled = False                # rule 2 minimum standard deviation        par_in_min_stdv = arcpy.Parameter(                            displayName='Rule 2: Minimum Std. Dev.',                            name='in_min_stdv',                            datatype='GPLong',                            parameterType='Optional',                            category='Rules',                            direction='Input',                            multiValue=False)        par_in_min_stdv.filter.type = 'Range'        par_in_min_stdv.filter.list = [0, 99]        par_in_min_stdv.value = 1        par_in_min_stdv.enabled = False                # rule 2 bidirectional        par_in_bidirectional = arcpy.Parameter(                                 displayName='Rule 2: Bidirectional',                                 name='in_bidirectional',                                 datatype='GPString',                                 parameterType='Optional',                                 category='Rules',                                 direction='Input',                                 multiValue=False)        par_in_bidirectional.filter.type = 'ValueList'        par_in_bidirectional.filter.list = ['Yes', 'No']        par_in_bidirectional.value = 'No'        par_in_bidirectional.enabled = False                      # rule 3 number of zones        par_in_num_zones = arcpy.Parameter(                             displayName='Rule 3: Number of zones jumped',                             name='in_num_zones',                             datatype='GPLong',                             parameterType='Optional',                             category='Rules',                             direction='Input',                             multiValue=False)        par_in_num_zones.filter.type = 'Range'        par_in_num_zones.filter.list = [0, 99]        par_in_num_zones.value = 1        par_in_num_zones.enabled = False                       # ruleset        par_in_ruleset = arcpy.Parameter(                           displayName='Ruleset',                           name='in_ruleset',                           datatype='GPString',                           parameterType='Optional',                           direction='Input',                           category='Rules',                           multiValue=False)        par_in_ruleset.filter.type = 'ValueList'        ruleset = [            '1 Only',            '2 Only',            '3 Only',            '1 and 2',            '1 and 3',            '2 and 3',            '1 or 2',            '1 or 3',            '2 or 3',            '1 and 2 and 3',            '1 or 2 and 3',            '1 and 2 or 3',            '1 or 2 or 3'            ]         par_in_ruleset.filter.list = ruleset        par_in_ruleset.value = '1 and 2 or 3'        par_in_ruleset.enabled = False               # alert user         par_in_alert_user = arcpy.Parameter(                              displayName='Set alert user',                              name='in_alert_user',                              datatype='GPString',                              parameterType='Required',                              direction='Input',                              category='Alerts and Email',                              multiValue=False)        par_in_alert_user.filter.type = 'ValueList'        par_in_alert_user.filter.list = ['Yes', 'No']        par_in_alert_user.value = 'No'        par_in_alert_user.enabled = False             # alert method        par_in_alert_method = arcpy.Parameter(                           displayName='Set alert method',                           name='in_alert_method',                           datatype='GPString',                           parameterType='Required',                           direction='Input',                           category='Alerts and Email',                           multiValue=False)        par_in_alert_method.filter.type = 'ValueList'        par_in_alert_method.filter.list = ['Static', 'Dynamic']        par_in_alert_method.value = 'Static'        par_in_alert_method.enabled = False                        # alert direction         par_in_alert_direction = arcpy.Parameter(                                   displayName='Direction of change for alert',                                   name='in_alert_direction',                                   datatype='GPString',                                   parameterType='Required',                                   direction='Input',                                   category='Alerts and Email',                                   multiValue=False)        par_in_alert_direction.filter.type = 'ValueList'        par_in_alert_direction.filter.list = ['Incline Only', 'Decline Only', 'Both']        par_in_alert_direction.value = 'Both'        par_in_alert_direction.enabled = False            # email         par_in_email = arcpy.Parameter(                         displayName='Set user email address',                         name='in_email',                         datatype='GPString',                         parameterType='Optional',                         direction='Input',                         category='Alerts and Email',                         multiValue=False)        par_in_email.enabled = False           # ignore         par_in_ignore = arcpy.Parameter(                          displayName='Ignore during monitoring',                          name='in_ignore_user',                          datatype='GPString',                          parameterType='Required',                          direction='Input',                          category='Other',                          multiValue=False)        par_in_ignore.filter.type = 'ValueList'        par_in_ignore.filter.list = ['Yes', 'No']        par_in_ignore.value = 'No'        par_in_ignore.enabled = False          # combine parameters        parameters = [            par_in_feat,            par_in_new_feat,            par_in_set_area,            par_in_platform,            par_in_s_year,            par_in_e_year,            par_in_veg_idx,            par_in_persistence,            par_in_min_conseqs,            par_in_inc_plateaus,            par_in_min_stdv,            par_in_bidirectional,            par_in_num_zones,            par_in_ruleset,            par_in_alert_user,            par_in_alert_method,            par_in_alert_direction,            par_in_email,            par_in_ignore        ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when         controls are changed on ArcGIS Pro panel.        """                 # set required fields         required_fields = ['area_id', 'platform', 's_year', 'e_year',                            'index', 'persistence', 'rule_1_min_conseqs',                            'rule_1_inc_plateaus', 'rule_2_min_stdv',                            'rule_2_bidirectional', 'rule_3_num_zones',                           'ruleset', 'alert', 'method', 'alert_direction',                            'email', 'ignore', 'color', 'global_id']                 # check existing feature input        if parameters[0].value is not None:            feat_desc = arcpy.Describe(parameters[0].value)            in_feat = os.path.join(feat_desc.path, feat_desc.name)                        # safe grab field list and check            with arcpy.da.SearchCursor(in_feat, field_names=['*']) as cursor:                fields = cursor.fields                        # check if all fields in feat            if not all(f in cursor.fields for f in required_fields):                arcpy.AddError('Incompatible monitoring areas feature class.')                return                       # if all columns exist, enable other parameters            parameters[1].enabled = True   # feature drawer            parameters[2].enabled = True   # monitoring area id            parameters[3].enabled = True   # platform            parameters[4].enabled = True   # start year            parameters[5].enabled = True   # end year            parameters[6].enabled = True   # vegetation index            parameters[7].enabled = True   # persistence            parameters[8].enabled = True   # rule 1 min conseqs            parameters[9].enabled = True   # rule 1 inc plateaus            parameters[10].enabled = True  # rule 2 num stdvs            parameters[11].enabled = True  # rule 2 bidirectional            parameters[12].enabled = True  # rule 3 num zones             parameters[13].enabled = True  # ruleset                    parameters[14].enabled = True  # alert user            parameters[15].enabled = True  # alert method            parameters[16].enabled = True  # alert direction            parameters[17].enabled = True  # email            parameters[18].enabled = True  # ignore                    return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the NRT Create Monitoring Areas tool.        """                # safe imports        import uuid        import arcpy                # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc                    # module folder            sys.path.append(FOLDER_MODULES)            import nrt        except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return                   # grab parameter values         in_exist_feat = parameters[0]                     # input monitoring areas feature        in_new_feat = parameters[1]                       # input new monitoring areas feature        in_set_area = parameters[2].value                 # input monitoring area id        in_platform = parameters[3].value                 # input platform        in_s_year = parameters[4].value                   # input start year        in_e_year = parameters[5].value                   # input end year        in_veg_idx = parameters[6].value                  # input vegetation index        in_persistence = parameters[7].value              # input persistence        in_rule_1_min_conseqs = parameters[8].value       # input rule 1 min conseqs        in_rule_1_inc_plateaus = parameters[9].value      # input rule 1 include plateaus        in_rule_2_min_stdv = parameters[10].value         # input rule 2 min stdvs        in_rule_2_bidirectional = parameters[11].value    # input rule 2 bidirectional        in_rule_3_num_zones = parameters[12].value        # input rule 3 num zones         in_ruleset = parameters[13].value                 # input rulesets        in_alert_user = parameters[14].value              # input alert user         in_alert_method = parameters[15].value            # input alert method         in_alert_direction = parameters[16].value         # input alert direction        in_email = parameters[17].value                   # input email        in_ignore = parameters[18].value                  # input ignore                        # # # # #        # notify user and set up progress bar         arcpy.AddMessage('Beginning NRT Create Monitoring Areas.')        arcpy.SetProgressor(type='step',                            message='Preparing parameters...',                            min_range=0, max_range=6)        # get full path to existing monitoring areas        exist_feat_desc = arcpy.Describe(parameters[0].value)        in_exist_feat = os.path.join(exist_feat_desc.path, exist_feat_desc.name)                # validate existing feature class        if not nrt.validate_monitoring_areas(in_exist_feat):            arcpy.AddError('Monitoring areas feature is invalid.')            return                    # check if temp layer is as expected         if 'NRT Create Monitoring Areas Draw' not in parameters[1].valueAsText:            arcpy.AddError('New monitoring area feature layer incompatible.')            return                # get path to new layer         new_feat_desc = arcpy.Describe(in_new_feat)        in_new_feat = os.path.join(new_feat_desc.path, new_feat_desc.name)                    # check training period variables         if in_s_year >= in_e_year:            arcpy.AddError('End pre-impact year must be higher than start year.')            return         elif in_e_year - in_s_year < 2:            arcpy.AddError('Must have two or more years worth of pre-impact period.')            return                     # check if years are supported in sentinel        if in_platform == 'Sentinel':            if in_s_year < 2016:                arcpy.AddError('Starting year for Sentinel data must be >= 2016.')                return                    # check if email provided if alert is yes and has syntax        if in_alert_user == 'Yes' and in_email is None:            arcpy.AddError('Must provide an email if alert is set to Yes.')            return        elif in_email is not None and ('@' not in in_email or '.' not in in_email):            arcpy.AddError('Email address is invalid.')            return                    # check alert method         if in_alert_method not in ['Static', 'Dynamic']:            arcpy.AddError('Alert method must be Static or Dynamic.')            return        # # # # #        # notify user and increment progress bar         arcpy.SetProgressorLabel('Preparing new monitoring area...')         arcpy.SetProgressorPosition(1)                 try:            # reset symbology (do before insert else error...)            p = arcpy.mp.ArcGISProject('CURRENT')            m = p.activeMap            for layer in m.listLayers('monitoring_areas'):                arc.apply_monitoring_area_symbology(layer)        except:            arcpy.AddWarning('Could not apply symbology to montiroing areas. Skipping.')        # check if new feature has only one feature, if so, get geom, else get frist        with arcpy.da.SearchCursor(in_new_feat, field_names=['SHAPE@']) as cursor:            count = len([row for row in cursor])            if count >= 1:                            # get raw geometry from first row                                    cursor.reset()                row = cursor.next()                geom = row[0]                                # convert to a wgs84 polygon object                srs = arcpy.SpatialReference(4326)                arr = arcpy.Array(geom)                pol = arcpy.Polygon(arr, srs)                                if count > 1:                    arcpy.AddWarning('Multiple monitoring area polygons were drawn, used first only.')            else:                arcpy.AddError('A new monitoring area was not drawn. Please draw one.')                return        # # # # #        # notify user and increment progress bar         arcpy.SetProgressorLabel('Preparing existing geodatabase data...')         arcpy.SetProgressorPosition(2)                         # get all rows as list of tuples with specific field order        with arcpy.da.SearchCursor(in_exist_feat, field_names=['area_id']) as cursor:            for row in cursor:                if in_set_area == row[0]:                    arcpy.AddError('This monitoring area identifier already exists. Set another.')                    return                                # # # # #        # notify user and increment progress bar         arcpy.SetProgressorLabel('Updating existing geodatabase with new area data...')         arcpy.SetProgressorPosition(3)                         # create required fields         fields = ['area_id', 'platform', 's_year', 'e_year',                   'index', 'persistence', 'rule_1_min_conseqs',                   'rule_1_inc_plateaus', 'rule_2_min_stdv',                   'rule_2_bidirectional', 'rule_3_num_zones',                  'ruleset', 'alert', 'method', 'alert_direction',                   'email', 'ignore', 'color', 'global_id', 'SHAPE@']                                   # create color and global id values        in_color = 0        guid = uuid.uuid4().hex                # create row structure with parameters        row = [in_set_area, in_platform, in_s_year, in_e_year,                in_veg_idx, in_persistence, in_rule_1_min_conseqs,               in_rule_1_inc_plateaus, in_rule_2_min_stdv,               in_rule_2_bidirectional, in_rule_3_num_zones,               in_ruleset, in_alert_user, in_alert_method, in_alert_direction,               in_email, in_ignore, in_color, guid, pol]                # take inputs and geom and insert into existing monitoring areas         try:            with arcpy.da.InsertCursor(in_exist_feat, field_names=fields) as cursor:                inserted = cursor.insertRow(row)        except:            arcpy.AddError('Could not insert new area into geodatabase. Do you have it open?')            return                        # # # # #        # notify user and increment progress bar         arcpy.SetProgressorLabel('Refreshing spatial extent...')         arcpy.SetProgressorPosition(4)                        # recalculate extent         try:            arcpy.AddSpatialIndex_management(in_exist_feat)        except:            arcpy.AddWarning('Could not recalculate extent, skipping.')                        # # # # #        # notify user and increment progress bar         arcpy.SetProgressorLabel('Removing temporary new monitoring area from map...')         arcpy.SetProgressorPosition(5)                 # remove temporary layer if exists        if inserted is not None:            try:                           # remove temp layer                 p = arcpy.mp.ArcGISProject('CURRENT')                m = p.activeMap                for layer in m.listLayers():                    if 'NRT Create Monitoring Areas Draw' in layer.name:                        m.removeLayer(layer)            except:                arcpy.AddWarning('Could not remove temporary layer. Skipping.')                            # hack: if first feat, remove it, add back to map, re-symbolise            try:                p = arcpy.mp.ArcGISProject('CURRENT')                m = p.activeMap                layer = m.listLayers('monitoring_areas')[0]                count = arcpy.GetCount_management(layer).getOutput(0)                if int(count) == 1:                    m.removeLayer(layer)                    m.addDataFromPath(in_exist_feat)                    layer = m.listLayers('monitoring_areas')[0]                    arc.apply_monitoring_area_symbology(layer)            except:                arcpy.AddWarning('Could not visualise first feature. Skipping.')        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(6)        # notify user        arcpy.AddMessage('Created new NRT Monitoring Area.')                returnclass NRT_Modify_Monitoring_Areas(object):    def __init__(self):        """        Initialise tool.        """        self.label = "NRT Modify Monitoring Areas"        self.description = "Modify existing NRT monitoring area features."        self.canRunInBackground = False    def getParameterInfo(self):        """        Set various ArcGIS Pro UI controls. Data validation        is enforced via ArcGIS Pro API.        """                # set input feature class        par_in_feat = arcpy.Parameter(                        displayName='Input Monitoring Area Featureclass',                        name='in_feat',                        datatype='GPFeatureLayer',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_in_feat.filter.list = ['Polygon']                # set monitoring area        par_in_set_area = arcpy.Parameter(                            displayName='Select the monitoring area to modify',                            name='in_set_area',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            multiValue=False)        par_in_set_area.filter.type = 'ValueList'        par_in_set_area.enabled = False                # platform        par_in_platform = arcpy.Parameter(                            displayName='Set satellite platform',                            name='in_platform',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            multiValue=False)        par_in_platform.filter.type = 'ValueList'        par_in_platform.filter.list = ['Landsat', 'Sentinel']        par_in_platform.value = 'Landsat'        par_in_platform.enabled = False                # start year        par_in_s_year = arcpy.Parameter(                          displayName='Set pre-impact start year',                          name='in_s_year',                          datatype='GPLong',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_in_s_year.filter.type = 'Range'        par_in_s_year.filter.list = [1980, 2050]        par_in_s_year.enabled = False                # end year        par_in_e_year = arcpy.Parameter(                          displayName='Set pre-impact end year',                          name='in_e_year',                          datatype='GPLong',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_in_e_year.filter.type = 'Range'        par_in_e_year.filter.list = [1980, 2050]        par_in_e_year.enabled = False        # vegetation index        par_in_veg_idx = arcpy.Parameter(                           displayName='Set vegetation index',                           name='in_veg_idx',                           datatype='GPString',                           parameterType='Required',                           direction='Input',                           multiValue=False)        par_in_veg_idx.filter.type = 'ValueList'        par_in_veg_idx.filter.list = ['NDVI', 'MAVI', 'kNDVI']        par_in_veg_idx.value = 'MAVI'        par_in_veg_idx.enabled = False                # persistence        par_in_persistence = arcpy.Parameter(                               displayName='Vegetation persistence',                               name='in_persistence',                               datatype='GPDouble',                               parameterType='Required',                               direction='Input',                               multiValue=False)        par_in_persistence.filter.type = 'Range'        par_in_persistence.filter.list = [0.001, 9.999]        par_in_persistence.value = 0.5        par_in_persistence.enabled = False               # rule 1 min conseqs        par_in_min_conseqs = arcpy.Parameter(                               displayName='Rule 1: Minimum consequtives',                               name='in_min_conseqs',                               datatype='GPLong',                               parameterType='Optional',                               direction='Input',                               category='Rules',                               multiValue=False)        par_in_min_conseqs.filter.type = 'Range'        par_in_min_conseqs.filter.list = [0, 99]        par_in_min_conseqs.value = 2        par_in_min_conseqs.enabled = False                # rule 1 include plateaus        par_in_inc_plateaus = arcpy.Parameter(                                displayName='Rule 1: Include plateaus',                                name='in_inc_plateaus',                                datatype='GPString',                                parameterType='Optional',                                direction='Input',                                category='Rules',                                multiValue=False)        par_in_inc_plateaus.filter.type = 'ValueList'        par_in_inc_plateaus.filter.list = ['Yes', 'No']        par_in_inc_plateaus.value = 'No'        par_in_inc_plateaus.enabled = False                # rule 2 minimum standard deviation        par_in_min_stdv = arcpy.Parameter(                            displayName='Rule 2: Minimum Std. Dev.',                            name='in_min_stdv',                            datatype='GPLong',                            parameterType='Optional',                            category='Rules',                            direction='Input',                            multiValue=False)        par_in_min_stdv.filter.type = 'Range'        par_in_min_stdv.filter.list = [0, 99]        par_in_min_stdv.value = 1        par_in_min_stdv.enabled = False                # rule 2 bidirectional        par_in_bidirectional = arcpy.Parameter(                                 displayName='Rule 2: Bidirectional',                                 name='in_bidirectional',                                 datatype='GPString',                                 parameterType='Optional',                                 category='Rules',                                 direction='Input',                                 multiValue=False)        par_in_bidirectional.filter.type = 'ValueList'        par_in_bidirectional.filter.list = ['Yes', 'No']        par_in_bidirectional.value = 'No'        par_in_bidirectional.enabled = False                      # rule 3 number of zones        par_in_num_zones = arcpy.Parameter(                             displayName='Rule 3: Number of zones jumped',                             name='in_num_zones',                             datatype='GPLong',                             parameterType='Optional',                             category='Rules',                             direction='Input',                             multiValue=False)        par_in_num_zones.filter.type = 'Range'        par_in_num_zones.filter.list = [0, 99]        par_in_num_zones.value = 1        par_in_num_zones.enabled = False                       # ruleset        par_in_ruleset = arcpy.Parameter(                           displayName='Ruleset',                           name='in_ruleset',                           datatype='GPString',                           parameterType='Optional',                           direction='Input',                           category='Rules',                           multiValue=False)        par_in_ruleset.filter.type = 'ValueList'        ruleset = [            '1 Only',            '2 Only',            '3 Only',            '1 and 2',            '1 and 3',            '2 and 3',            '1 or 2',            '1 or 3',            '2 or 3',            '1 and 2 and 3',            '1 or 2 and 3',            '1 and 2 or 3',            '1 or 2 or 3'            ]         par_in_ruleset.filter.list = ruleset        par_in_ruleset.value = '1 and 2 or 3'        par_in_ruleset.enabled = False               # alert user         par_in_alert_user = arcpy.Parameter(                              displayName='Set alert user',                              name='in_alert_user',                              datatype='GPString',                              parameterType='Required',                              direction='Input',                              category='Alerts and Email',                              multiValue=False)        par_in_alert_user.filter.type = 'ValueList'        par_in_alert_user.filter.list = ['Yes', 'No']        par_in_alert_user.value = 'No'        par_in_alert_user.enabled = False                        # alert method        par_in_alert_method = arcpy.Parameter(                           displayName='Set alert method',                           name='in_alert_method',                           datatype='GPString',                           parameterType='Required',                           direction='Input',                           category='Alerts and Email',                           multiValue=False)        par_in_alert_method.filter.type = 'ValueList'        par_in_alert_method.filter.list = ['Static', 'Dynamic']        par_in_alert_method.value = 'Static'        par_in_alert_method.enabled = False                  # alert direction         par_in_alert_direction = arcpy.Parameter(                                   displayName='Direction of change for alert',                                   name='in_alert_direction',                                   datatype='GPString',                                   parameterType='Required',                                   direction='Input',                                   category='Alerts and Email',                                   multiValue=False)        par_in_alert_direction.filter.type = 'ValueList'        par_in_alert_direction.filter.list = ['Incline Only', 'Decline Only', 'Both']        par_in_alert_direction.value = 'Both'        par_in_alert_direction.enabled = False            # email         par_in_email = arcpy.Parameter(                         displayName='Set user email address',                         name='in_email',                         datatype='GPString',                         parameterType='Optional',                         direction='Input',                         category='Alerts and Email',                         multiValue=False)        par_in_email.enabled = False           # ignore         par_in_ignore = arcpy.Parameter(                          displayName='Ignore during analysis',                          name='in_ignore_user',                          datatype='GPString',                          parameterType='Required',                          direction='Input',                          category='Other',                          multiValue=False)        par_in_ignore.filter.type = 'ValueList'        par_in_ignore.filter.list = ['Yes', 'No']        par_in_ignore.value = 'No'        par_in_ignore.enabled = False           # combine parameters        parameters = [            par_in_feat,            par_in_set_area,            par_in_platform,            par_in_s_year,            par_in_e_year,            par_in_veg_idx,            par_in_persistence,            par_in_min_conseqs,            par_in_inc_plateaus,            par_in_min_stdv,            par_in_bidirectional,            par_in_num_zones,            par_in_ruleset,            par_in_alert_user,            par_in_alert_method,            par_in_alert_direction,            par_in_email,            par_in_ignore        ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when         controls are changed on ArcGIS Pro panel.        """                # set global var (not nice, but gets around arcgis limits easily), shorten        global NRT_MOD_MON_AREAS_LAST_AREA                       # set required fields         required_fields = ['area_id', 'platform', 's_year', 'e_year',                            'index', 'persistence', 'rule_1_min_conseqs',                            'rule_1_inc_plateaus', 'rule_2_min_stdv',                            'rule_2_bidirectional', 'rule_3_num_zones',                           'ruleset', 'alert', 'method', 'alert_direction',                            'email', 'ignore', 'color', 'global_id']                                    # if not first run or no change to area id, skip        last_area = NRT_MOD_MON_AREAS_LAST_AREA        if last_area is not None and last_area == parameters[1].value:            return                 # check feature input        if parameters[0].value is not None:            feat_desc = arcpy.Describe(parameters[0].value)            in_feat = os.path.join(feat_desc.path, feat_desc.name)                        # safe grab field list and check            with arcpy.da.SearchCursor(in_feat, field_names=['*']) as cursor:                fields = cursor.fields                        # check if all fields in feat            if not all(f in cursor.fields for f in required_fields):                arcpy.AddError('Incompatible monitoring areas file.')                return                                # get all rows as list of tuples with specific field order            with arcpy.da.SearchCursor(in_feat, field_names=required_fields) as cursor:                rows = [rec for rec in cursor]                        # if first time, get first row values, else user selected            row = None            if parameters[1].value is None:                row = rows[0]            else:                for row in rows:                    if row[0] == parameters[1].value:                        break            # if row exists, enable + populate controls with first row values             if row is not None:                # enable, populate and set area selector value list                parameters[1].enabled = True                parameters[1].filter.list = [rec[0] for rec in rows]                parameters[1].value = row[0]                                # enable, populate and set platform                parameters[2].enabled = True                parameters[2].value = row[1]                # enable, start year                parameters[3].enabled = True                parameters[3].value = row[2]                                # enable, end year                parameters[4].enabled = True                parameters[4].value = row[3]                                # enable, vegetation index                parameters[5].enabled = True                parameters[5].value = row[4]                                # enable, persistence                parameters[6].enabled = True                parameters[6].value = row[5]                 # enable, rule 1 min conseqs                parameters[7].enabled = True                parameters[7].value = row[6]                                 # enable, rule 1 inc plateaus                parameters[8].enabled = True                parameters[8].value = row[7]                                                 # enable, rule 2 num stdvs                parameters[9].enabled = True                parameters[9].value = row[8]                           # enable, rule 2 bidirectional                parameters[10].enabled = True                parameters[10].value = row[9]                     # enable, rule 3 num zones                parameters[11].enabled = True                parameters[11].value = row[10]                       # enable, ruleset                parameters[12].enabled = True                parameters[12].value = row[11]                            # enable, alert user                parameters[13].enabled = True                parameters[13].value = row[12]                                # enable, alert user                parameters[14].enabled = True                parameters[14].value = row[13]                # enable, alert direction                parameters[15].enabled = True                parameters[15].value = row[14]                                           # enable, email                parameters[16].enabled = True                parameters[16].value = row[15]                                # enable, ignore                parameters[17].enabled = True                parameters[17].value = row[16]                                        # update global to current area id                NRT_MOD_MON_AREAS_LAST_AREA = row[0]        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the NRT Modify Monitoring Areas tool.        """                # safe imports        import arcpy                # risky imports        try:            import xarray as xr        except:            arcpy.AddError('Could not import xarray.')            return            # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc                        # module folder            sys.path.append(FOLDER_MODULES)            import nrt        except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return                            # grab parameter values         in_feat = parameters[0]                           # input monitoring areas feature        in_set_area = parameters[1].value                 # input monitoring area id        in_platform = parameters[2].value                 # input platform        in_s_year = parameters[3].value                   # input start year        in_e_year = parameters[4].value                   # input end year        in_veg_idx = parameters[5].value                  # input vegetation index        in_persistence = parameters[6].value              # input persistence        in_rule_1_min_conseqs = parameters[7].value       # input rule 1 min conseqs        in_rule_1_inc_plateaus = parameters[8].value      # input rule 1 include plateaus        in_rule_2_min_stdv = parameters[9].value          # input rule 2 min stdvs        in_rule_2_bidirectional = parameters[10].value    # input rule 2 bidirectional        in_rule_3_num_zones = parameters[11].value        # input rule 3 num zones         in_ruleset = parameters[12].value                 # input rulesets        in_alert_user = parameters[13].value              # input alert user         in_alert_method = parameters[14].value            # input alert method         in_alert_direction = parameters[15].value         # input alert direction        in_email = parameters[16].value                   # input email        in_ignore = parameters[17].value                  # input ignore                        # # # # #        # notify user and set up progress bar         arcpy.AddMessage('Beginning NRT Modify Monitoring Areas.')        arcpy.SetProgressor(type='step',                            message='Preparing parameters...',                            min_range=0, max_range=3)                # get full path to existing monitoring areas        exist_feat_desc = arcpy.Describe(parameters[0].value)        in_exist_feat = os.path.join(exist_feat_desc.path, exist_feat_desc.name)                       # validate existing feature class        if not nrt.validate_monitoring_areas(in_exist_feat):            arcpy.AddError('Monitoring areas feature is invalid.')            return                    # check training period variables         if in_s_year >= in_e_year:            arcpy.AddError('End pre-impact year must be higher than start year.')            return         elif in_e_year - in_s_year < 2:            arcpy.AddError('Must have two or more years worth of pre-impact period.')            return                     # check if years are supported in sentinel        if in_platform == 'Sentinel':            if in_s_year < 2016:                arcpy.AddError('Starting year for Sentinel data must be >= 2016.')                return                    # check if email provided if alert is yes and has syntax        if in_alert_user == 'Yes' and in_email is None:            arcpy.AddError('Must provide an email if alert is set to Yes.')            return        elif in_email is not None and ('@' not in in_email or '.' not in in_email):            arcpy.AddError('Email address is invalid.')            return        # # # # #        # notify user and increment progress bar         arcpy.SetProgressorLabel('Updating monitoring area values...')         arcpy.SetProgressorPosition(1)                 try:            # reset symbology (do before insert else error...)            p = arcpy.mp.ArcGISProject('CURRENT')            m = p.activeMap            for layer in m.listLayers('monitoring_areas'):                arc.apply_monitoring_area_symbology(layer)        except Exception as e:            #arcpy.AddWarning('Could not apply symbology to montiroing areas. Skipping.')            arcpy.AddWarning(e)                # set required fields         fields = ['area_id', 'platform', 's_year', 'e_year',                   'index', 'persistence', 'rule_1_min_conseqs',                   'rule_1_inc_plateaus', 'rule_2_min_stdv',                   'rule_2_bidirectional', 'rule_3_num_zones',                  'ruleset', 'alert', 'method', 'alert_direction',                   'email', 'ignore']        try:            # open feature class and update if current id            with arcpy.da.UpdateCursor(in_exist_feat, field_names=fields) as cursor:                for row in cursor:                                        # where id matches, update                    if row[0] == in_set_area:                        row[1] = in_platform                        row[2] = in_s_year                        row[3] = in_e_year                        row[4] = in_veg_idx                        row[5] = in_persistence                        row[6] = in_rule_1_min_conseqs                        row[7] = in_rule_1_inc_plateaus                        row[8] = in_rule_2_min_stdv                        row[9] = in_rule_2_bidirectional                        row[10] = in_rule_3_num_zones                        row[11] = in_ruleset                        row[12] = in_alert_user                        row[13] = in_alert_method                        row[14] = in_alert_direction                        row[15] = in_email                        row[16] = in_ignore                        # update row                        cursor.updateRow(row)        except:            arcpy.AddError('Could not update monitoring area. Is the geodatabase open?')            return                              # # # # #        # notify user and increment progress bar         arcpy.SetProgressorLabel('Refreshing spatial extent...')         arcpy.SetProgressorPosition(2)                        # recalculate extent         try:            arcpy.AddSpatialIndex_management(in_exist_feat)        except:            arcpy.AddWarning('Could not recalculate extent, skipping.')        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(3)        # notify user        arcpy.AddMessage('Modified existing NRT Monitoring Area.')                returnclass NRT_Delete_Monitoring_Areas(object):    def __init__(self):        """        Initialise tool.        """        self.label = "NRT Delete Monitoring Areas"        self.description = "Delete existing NRT monitoring area features."        self.canRunInBackground = False    def getParameterInfo(self):        """        Set various ArcGIS Pro UI controls. Data validation        is enforced via ArcGIS Pro API.        """                # set input feature class        par_in_feat = arcpy.Parameter(                        displayName='Input Monitoring Area Featureclass',                        name='in_feat',                        datatype='GPFeatureLayer',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_in_feat.filter.list = ['Polygon']                # set monitoring area        par_in_set_area = arcpy.Parameter(                            displayName='Select monitoring area to delete',                            name='in_area_id',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            multiValue=False)        par_in_set_area.enabled = False                # combine parameters        parameters = [            par_in_feat,            par_in_set_area        ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when         controls are changed on ArcGIS Pro panel.        """                 # set required fields         required_fields = ['area_id', 'platform', 's_year', 'e_year',                            'index', 'persistence', 'rule_1_min_conseqs',                            'rule_1_inc_plateaus', 'rule_2_min_stdv',                            'rule_2_bidirectional', 'rule_3_num_zones',                           'ruleset', 'alert', 'method', 'alert_direction',                            'email', 'ignore']                 # check feature input        if parameters[0].value is not None:            feat_desc = arcpy.Describe(parameters[0].value)            in_feat = os.path.join(feat_desc.path, feat_desc.name)                        # safe grab field list and check            with arcpy.da.SearchCursor(in_feat, field_names=['*']) as cursor:                fields = cursor.fields                        # check if all fields in feat            if not all(f in cursor.fields for f in required_fields):                arcpy.AddError('Incompatible monitoring areas file.')                return                                # get all rows as list of tuples with specific field order            with arcpy.da.SearchCursor(in_feat, field_names=required_fields) as cursor:                rows = [rec for rec in cursor]                        # if first time, get first row values, else user selected            row = None            if parameters[1].value is None:                row = rows[0]            else:                for row in rows:                    if row[0] == parameters[1].value:                        break            # if row exists, enable + populate controls with first row values             if row is not None:                parameters[1].enabled = True                parameters[1].filter.list = [rec[0] for rec in rows]                parameters[1].value = row[0]                            else:                parameters[1].enabled = True                parameters[1].filter.list = []                parameters[1].value = None                        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the NRT Create Monitoring Areas tool.        """                # safe imports        import os        import arcpy                # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc                        # module folder            sys.path.append(FOLDER_MODULES)            import nrt        except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return                   # grab parameter values         in_exist_feat = parameters[0]                     # input monitoring areas feature        in_set_area = parameters[1].value                 # input monitoring area id                # # # # #        # notify user and set up progress bar         arcpy.AddMessage('Beginning NRT Delete Monitoring Areas.')        arcpy.SetProgressor(type='step',                            message='Preparing parameters...',                            min_range=0, max_range=3)                # get full path to existing monitoring areas        feat_desc = arcpy.Describe(parameters[0].value)        in_feat = os.path.join(feat_desc.path, feat_desc.name)                # validate existing feature class        if not nrt.validate_monitoring_areas(in_feat):            arcpy.AddError('Monitoring areas feature is invalid.')            return        # # # # #        # notify user and increment progress bar         arcpy.SetProgressorLabel('Deleting monitoring area feature...')         arcpy.SetProgressorPosition(1)                        try:            # reset symbology (do before insert else error...)            p = arcpy.mp.ArcGISProject('CURRENT')            m = p.activeMap            for layer in m.listLayers('monitoring_areas'):                arc.apply_monitoring_area_symbology(layer)        except:            arcpy.AddWarning('Could not apply symbology to montiroing areas. Skipping.')                # delete user selected feature         with arcpy.da.UpdateCursor(in_feat, field_names=['area_id']) as cursor:            for row in cursor:                if row[0] == in_set_area:                    cursor.deleteRow()                               # # # # #        # notify user and increment progress bar         arcpy.SetProgressorLabel('Refreshing spatial extent...')         arcpy.SetProgressorPosition(2)                        # recalculate extent         try:            arcpy.AddSpatialIndex_management(in_feat)        except:            arcpy.AddWarning('Could not recalculate extent, skipping.')                    # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(3)        # notify user        arcpy.AddMessage('Deleted existing NRT Monitoring Area.')                return# metaclass NRT_Build_Graphs(object):    def __init__(self):        """Define the tool (tool name is the name of the class)."""        self.label = "NRT Build Graphs"        self.description = "Helper tool to build html graphs for UI popups."        self.canRunInBackground = True    def getParameterInfo(self):        """Define parameter definitions"""                # input change cube path        par_cube_path = arcpy.Parameter(                          displayName='Filepath to a single change cube',                          name='in_cube_path',                          datatype='GPString',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_cube_path.value = ''                # input monitoring area parameters        par_area_params = arcpy.Parameter(                            displayName='String of monitoring areas parameters.',                            name='in_area_params',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            multiValue=False)        par_area_params.value = ''                params = [par_cube_path, par_area_params]        return params    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the NRT Create Monitoring Areas tool.        """                # safe imports        import os        import arcpy                # risky imports        try:            import xarray as xr        except:            arcpy.AddMessage('<h3>Could not generate graph information.</h3>')            return                 # set up non-chart html code         overview_template = """        <html>            <head>                <style type="text/css">                    h4 {                        margin-top: 0px;                        margin-bottom: 0px;                    }                    p {                        margin-top: 0px;                        margin-bottom: 0px;                    }                </style>            </head>            <body>                <center>                    <h3>Overview of Selected Area</h3>                </center>                <h4>Area identifier</h4>                <p>//data.OvrAreaId</p>                <br />                <h4>Satellite platform</h4>                <p>//data.OvrPlatform</p>                <br />                <h4>Starting year of pre-impact period</h4>                <p>//data.OvrSYear</p>                <br />                <h4>Ending year of pre-impact period</h4>                <p>//data.OvrEYear</p>                <br />                <h4>Vegetation index</h4>                <p>//data.OvrIndex</p>                <br />                <h4>Model Persistence</h4>                <p>//data.OvrPersistence</p>                <br />                <h4>Rule 1: Minimum consequtives</h4>                <p>//data.OvrRule1MinConseq</p>                <br />                <h4>Rule 1: Include Plateaus</h4>                <p>//data.OvrRule1IncPlateaus</p>                <br />                <h4>Rule 2: Minimum Std. Dev.</h4>                <p>//data.OvrRule2MinStdv</p>                <br />                <h4>Rule 2: Bidirectional</h4>                <p>//data.OvrRule2Bidirectional</p>                <br />                <h4>Rule 3: Number of Zones</h4>                <p>//data.OvrRule3NumZones</p>                <br />                <h4>Ruleset</h4>                <p>//data.OvrRuleset</p>                <br />                <h4>Alert via email</h4>                <p>//data.OvrAlert</p>                <br />                <h4>Alert direction</h4>                <p>//data.OvrAlertDirection</p>                <br />                <h4>User email</h4>                <p>//data.OvrEmail</p>                <br />                <h4>Ignore during monitoring</h4>                <p>//data.OvrIgnore</p>                <br />            </body>        </html>        """        # set up table html template code        table_template = """            <html>                <head>                    <script type="text/javascript" src="https://www.gstatic.com/charts/loader.js"></script>                    <script type="text/javascript">                        google.charts.load('current', { packages: ['table'] });                        google.charts.setOnLoadCallback(drawBasic);                        function drawBasic() {                            var data = new google.visualization.DataTable();                            data.addColumn('string', 'Date');                            data.addColumn('number', 'Zone');                            //data.addRows                                                        data.addRows([                            ['2010-01-01', {v: -2, f: '-2'}],                            ['2010-05-11', {v: -3, f: '-3'}],                            ['2010-09-19', {v: -5, f: '-5'}],                            ]);                            var options = {                                //legend: {                                    //position: 'none',                                //},                                //chartArea: {                                    //width: '100%',                                     //height: '100%',                                    //top: 25,                                    //bottom: 100,                                    //left: 75,                                    //right: 25                                    //},                                //hAxis: {                                    //title: '//data.xAxisTitle',                                    //textStyle: {fontSize: '9'},                                    //slantedText: true,                                     //slantedTextAngle: 90                                //},                                //vAxis: {                                    //title: '//data.vAxisTitle',                                    //textStyle: {fontSize: '9'},                                //},                                //colors: ['//data.LineColor'],                            };                            var table = new google.visualization.Table(document.getElementById('table_div'));                            table.draw(data, options);                        }                    </script>                </head>                <body>                    <center>                        <h3>Overview</h3>                        <div id="table_div" style="width: 100%; height: 90%"></div>                    </center>                                    </body>            </html>        """        # set up line html template code        line_template = """            <html>                <head>                    <script type="text/javascript" src="https://www.gstatic.com/charts/loader.js"></script>                    <script type="text/javascript">                        google.charts.load('current', { packages: ['corechart', 'line'] });                        google.charts.setOnLoadCallback(drawBasic);                        function drawBasic() {                            var data = new google.visualization.DataTable();                            data.addColumn('string', 'x');                            data.addColumn('number', 'y');                            //data.addRows                            var options = {                                legend: {                                    position: 'none',                                },                                chartArea: {                                    width: '100%',                                     height: '100%',                                    top: 25,                                    bottom: 100,                                    left: 75,                                    right: 25                                    },                                hAxis: {                                    title: '//data.xAxisTitle',                                    textStyle: {fontSize: '9'},                                    slantedText: true,                                     slantedTextAngle: 90                                },                                vAxis: {                                    title: '//data.vAxisTitle',                                    textStyle: {fontSize: '9'},                                },                                colors: ['//data.LineColor'],                            };                            var chart = new google.visualization.LineChart(document.getElementById('curve_chart'));                            chart.draw(data, options);                        }                    </script>                </head>                <body>                    <center>                        <h3>//data.PopupTitle</h3>                        <div id="curve_chart" style="width: 100%; height: 90%"></div>                    </center>                                    </body>            </html>        """                # set up column html template code        column_template = """            <html>                <head>                    <script type="text/javascript" src="https://www.gstatic.com/charts/loader.js"></script>                    <script type="text/javascript">                        google.charts.load('current', { packages: ['corechart', 'bar'] });                        google.charts.setOnLoadCallback(drawBasic);                        function drawBasic() {                            //data.addRows                            var options = {                                legend: {                                    position: 'none',                                },                                chartArea: {                                    width: '100%',                                     height: '100%',                                    top: 25,                                    bottom: 100,                                    left: 75,                                    right: 25                                    },                                hAxis: {                                    title: '//data.xAxisTitle',                                    textStyle: {fontSize: '9'},                                    slantedText: true,                                     slantedTextAngle: 90                                },                                vAxis: {                                    title: '//data.vAxisTitle',                                    textStyle: {fontSize: '9'},                                },                            };                            var chart = new google.visualization.ColumnChart(document.getElementById('column_chart'));                            chart.draw(data, options);                        }                    </script>                </head>                <body>                    <center>                        <h3>//data.PopupTitle</h3>                        <div id="column_chart" style="width: 100%; height: 90%"></div>                    </center>                                    </body>            </html>        """                # set up calendar html template code        calendar_template = """            <html>                <head>                    <script type="text/javascript" src="https://www.gstatic.com/charts/loader.js"></script>                    <script type="text/javascript">                        google.charts.load('current', { packages: ['calendar'] });                        google.charts.setOnLoadCallback(drawBasic);                        function drawBasic() {                            var data = new google.visualization.DataTable();                            data.addColumn('date', 'Date');                            data.addColumn('number', 'Zone');                            //data.addRows                            var options = {                                title: "Alert History",                                //legend: {                                    //position: 'none',                                //},                                //chartArea: {                                    //width: '100%',                                     //height: '100%',                                    //top: 25,                                    //bottom: 100,                                    //left: 75,                                    //right: 25                                    //},                                //hAxis: {                                    //title: '//data.xAxisTitle',                                    //textStyle: {fontSize: '9'},                                    //slantedText: true,                                     //slantedTextAngle: 90                                //},                                //vAxis: {                                    //title: '//data.vAxisTitle',                                    //textStyle: {fontSize: '9'},                                //},                                //colors: ['//data.LineColor'],                            };                            var chart = new google.visualization.Calendar(document.getElementById('calendar_basic'));                            chart.draw(data, options);                        }                    </script>                </head>                <body>                    <center>                        <h3>Alert History</h3>                        <div id="calendar_basic" style="width: 100%; height: 90%"></div>                    </center>                                    </body>            </html>        """        # set up legend html template code        legend_template = """            <html>              <head>                <style>                  td, th {                    border: 1px solid transparent;                    text-align: left;                    padding: 0px;                  }                </style>              </head>              <body>                <center>                  <h3>Zone Legend</h3>                </center>                <table style="width: 100%;">                  <colgroup>                    <col span="1" style="width: 15%;">                    <col span="1" style="width: 15%;">                    <col span="1" style="width: 70%;">                  </colgroup>                  <tr>                    <th>Symbology</th>                    <th>Zone</th>                    <th>Description</th>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid red; background-color: #FF7F7F"></div>                      </div>                    </td>                    <td>-11</td>                    <td>Change deviation is below -19. Decline.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid red; background-color: #FFA77F"></div>                      </div>                    </td>                    <td>-10</td>                    <td>Change deviation is between -17 and -19. Decline.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid red; background-color: #FFD37F"></div>                      </div>                    </td>                    <td>-9</td>                    <td>Change deviation is between -15 and -17. Decline.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid red; background-color: #FFFF73"></div>                      </div>                    </td>                    <td>-8</td>                    <td>Change deviation is between -13 and -15. Decline.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid red; background-color: #D1FF73"></div>                      </div>                    </td>                    <td>-7</td>                    <td>Change deviation is between -11 and -13. Decline.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid red; background-color: #A3FF73"></div>                      </div>                    </td>                    <td>-6</td>                    <td>Change deviation is between -9 and -11. Decline.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid red; background-color: #73FFDF"></div>                      </div>                    </td>                    <td>-5</td>                    <td>Change deviation is between -7 and -9. Decline.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid red; background-color: #73DFFF"></div>                      </div>                    </td>                    <td>-4</td>                    <td>Change deviation is between -5 and -7. Decline.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid red; background-color: #73B2FF"></div>                      </div>                    </td>                    <td>-3</td>                    <td>Change deviation is between -3 and -5. Decline.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid red; background-color: #DF73FF"></div>                      </div>                    </td>                    <td>-2</td>                    <td>Change deviation is between -1 and -3. Decline.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid red; background-color: #FF73DF"></div>                      </div>                    </td>                    <td>-1</td>                    <td>Change deviation is between 0 and -1. Decline.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid black; background-color: white"></div>                      </div>                    </td>                    <td>0</td>                    <td>No change in either direction.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid blue; background-color: #FF73DF"></div>                      </div>                    </td>                    <td>1</td>                    <td>Change deviation is between 0 and 1. Growth.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid blue; background-color: #DF73FF"></div>                      </div>                    </td>                    <td>2</td>                    <td>Change deviation is between 1 and 3. Growth.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid blue; background-color: #73B2FF"></div>                      </div>                    </td>                    <td>3</td>                    <td>Change deviation is between 3 and 5. Growth.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid blue; background-color: #73DFFF"></div>                      </div>                    </td>                    <td>4</td>                    <td>Change deviation is between 5 and 7. Growth.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid blue; background-color: #73FFDF"></div>                      </div>                    </td>                    <td>5</td>                    <td>Change deviation is between 7 and 9. Growth.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid blue; background-color: #A3FF73"></div>                      </div>                    </td>                    <td>6</td>                    <td>Change deviation is between 9 and 11. Growth.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid blue; background-color: #D1FF73"></div>                      </div>                    </td>                    <td>7</td>                    <td>Change deviation is between 11 and 13. Growth.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid blue; background-color: #FFFF73"></div>                      </div>                    </td>                    <td>8</td>                    <td>Change deviation is between 13 and 15. Growth.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid blue; background-color: #FFD37F"></div>                      </div>                    </td>                    <td>9</td>                    <td>Change deviation is between 15 and 17. Growth.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid blue; background-color: #FFA77F"></div>                      </div>                    </td>                    <td>10</td>                    <td>Change deviation is between 17 and 19. Growth.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid blue; background-color: #FF7F7F"></div>                      </div>                    </td>                    <td>11</td>                    <td>Change deviation is above 19. Growth.</td>                  </tr>                </table>              </body>            </html>        """                # # # # # #        # get cube filepath from c#        in_cube_path = parameters[0].value       # full path with filename to change cube from c#        in_area_params = parameters[1].value     # string of area params seperated by ";"                # unpack parameters         params = in_area_params.split(';')                       # check if inputs exist        if in_cube_path == '' or in_area_params == '':            arcpy.AddMessage('<h3>Could not generate graph information.</h3>')            return        elif in_cube_path is None or in_area_params is None:            arcpy.AddMessage('<h3>Could not generate graph information.</h3>')            return        # check if cube file exists         if not os.path.exists(in_cube_path):            arcpy.AddMessage('<h3>Could not generate graph information.</h3>')            return                # # # # # #        # safe open current dataset        try:            ds = xr.open_dataset(in_cube_path)            ds = ds.load()             ds.close()         except:            arcpy.AddMessage('<h3>Could not generate graph information.</h3>')            return                            # # # # # #        # do basic checks on dataset         if 'time' not in ds: #or 'x' not in ds or 'y' not in ds:            arcpy.AddMessage('<h3>Could not generate graph information.</h3>')            return        elif len(ds['time']) == 0:            arcpy.AddMessage('<h3>Could not generate graph information.</h3>')            return                    # reduce to single value per time and remove last date         try:            #ds = ds.mean(['x', 'y'])            ds = ds.isel(time=slice(0, -1))  # -1 when slice, -2 when not        except:            arcpy.AddMessage('<h3>Could not generate graph information.</h3>')            return                # remove all dates before training period         s_year = int(params[2])        ds = ds.where(ds['time.year'] >= s_year, drop=True)                # remove all coinciding indices where nans        # fix this up for when certain vars are empty         #if ds.to_array().isnull().any():            #nan_dts = ds.where(ds.isnull(), drop=True).time            #ds = ds.drop_sel(time=nan_dts)                                # check if anything exists after removal of nan        if len(ds['time']) == 0:            arcpy.AddMessage('<h3>Could not generate graph information.</h3>')            return                #with open(r"C:\Users\Lewis\Desktop\temp.txt", 'a') as f:            #f.write(str(ds.load()))                        # # # # # #        # check if required variables exist         #for field in ['veg_idx', 'static', 'zones', 'cands_inc', 'cands_dec', 'consq_inc', 'consq_dec']:            #if field not in ds:                #arcpy.AddMessage('<h3>Could not generate graph information.</h3>')                #return                # get value vectors for time, veg, change, etc...        try:            dts = ds['time'].dt.strftime('%Y-%m-%d').values        # datetimes as string            veg = ds['veg_clean'].values                           # mean veg index            stc = ds['static_clean'].values                        # static change deviations            zns = ds['static_zones'].values                        # static zones                        # prepare zone data where canidates exist             #zns = ds['zones'].where((ds['cands_inc'] == 1) | (ds['cands_dec'] == 1), 0)            #zns = zns.values                except:            arcpy.AddMessage('<h3>Could not generate graph information.</h3>')            return        # construct html template for overview data        if params != '' or params is not None:            ovr_html = overview_template.replace('//data.OvrAreaId', params[0])            ovr_html = ovr_html.replace('//data.OvrPlatform', params[1])            ovr_html = ovr_html.replace('//data.OvrSYear', params[2])            ovr_html = ovr_html.replace('//data.OvrEYear', params[3])            ovr_html = ovr_html.replace('//data.OvrIndex', params[4])            ovr_html = ovr_html.replace('//data.OvrPersistence', params[5])            ovr_html = ovr_html.replace('//data.OvrRule1MinConseq', params[6])            ovr_html = ovr_html.replace('//data.OvrRule1IncPlateaus', params[7])            ovr_html = ovr_html.replace('//data.OvrRule2MinStdv', params[8])            ovr_html = ovr_html.replace('//data.OvrRule2Bidirectional', params[9])            ovr_html = ovr_html.replace('//data.OvrRule3NumZones', params[10])            ovr_html = ovr_html.replace('//data.OvrRuleset', params[11])            ovr_html = ovr_html.replace('//data.OvrAlert', params[12])            ovr_html = ovr_html.replace('//data.OvrAlertDirection', params[13])            ovr_html = ovr_html.replace('//data.OvrEmail', params[14])            ovr_html = ovr_html.replace('//data.OvrIgnore', params[15])            arcpy.AddMessage(ovr_html)        else:            arcpy.AddMessage('<h3>Could not generate overview information.</h3>')        # construct html template for vegetation index data        if len(dts) == len(veg):            veg_data = [[dts[i], veg[i]] for i in range(len(dts))]              veg_data_block = "data.addRows(" + str(veg_data) + ");"            veg_html = line_template.replace('//data.addRows', veg_data_block)            veg_html = veg_html.replace('//data.PopupTitle', 'Raw Vegetation')            veg_html = veg_html.replace('//data.xAxisTitle', 'Date')            veg_html = veg_html.replace('//data.vAxisTitle', 'Raw Vegetation')            veg_html = veg_html.replace('//data.LineColor',  '#129632')            arcpy.AddMessage(veg_html)        else:            arcpy.AddMessage('<h3>Could not generate vegetation graph.</h3>')                    # construct html template for change data        if len(dts) == len(stc):                        stc_data = [[dts[i], stc[i]] for i in range(len(dts))]              stc_data_block = "data.addRows(" + str(stc_data) + ");"            stc_html = line_template.replace('//data.addRows', stc_data_block)            stc_html = stc_html.replace('//data.PopupTitle', 'Change Deviation')            stc_html = stc_html.replace('//data.xAxisTitle', 'Date')            stc_html = stc_html.replace('//data.vAxisTitle', 'Change Deviation')            stc_html = stc_html.replace('//data.LineColor',  '#961212')            arcpy.AddMessage(stc_html)                    else:            arcpy.AddMessage('<h3>Could not generate change deviation graph.</h3>')                    # construct html template for conseq data        if len(dts) == len(zns):                            # create cmap             cmap = {                -12: "black",                -11: "#FF7F7F",                -10: "#FFA77F",                -9:  "#FFD37F",                -8:  "#FFFF73",                -7:  "#D1FF73",                -6:  "#A3FF73",                -5:  "#73FFDF",                -4:  "#73DFFF",                -3:  "#73B2FF",                -2:  "#DF73FF",                -1:  "#FF73DF",                0:   "#FFFFFF",                1:   "#FF73DF",                2:   "#DF73FF",                3:   "#73B2FF",                4:   "#73DFFF",                5:   "#73FFDF",                6:   "#A3FF73",                7:   "#D1FF73",                8:   "#FFFF73",                9:   "#FFD37F",                10:  "#FFA77F",                11:  "#FF7F7F",                12: "black"}                    # quick check to see if cmap values in cube            try:                [cmap[v] for v in zns]            except:                arcpy.AddMessage('<h3>Could not generate change deviation graph.</h3>')                return            # prepare data statement and header row             zns_data_block = "var data = google.visualization.arrayToDataTable(["            zns_data_block += "['X', 'Y', {role: 'style'}],"                    # construct data array with colors, stringify, append            zns_data = [[dts[i], zns[i], cmap.get(zns[i])] for i in range(len(dts))]              zns_data = ','.join([str(s) for s in zns_data])            zns_data_block += zns_data + ']);'                        # prepare data            zns_html = column_template.replace('//data.addRows', zns_data_block)            zns_html = zns_html.replace('//data.PopupTitle', 'Alert History')            zns_html = zns_html.replace('//data.xAxisTitle', 'Date')            zns_html = zns_html.replace('//data.vAxisTitle', 'Zone when alert triggered.')            arcpy.AddMessage(zns_html)                   else:            arcpy.AddMessage('<h3>Could not generate alert history graph.</h3>')                # construct html template for legend         arcpy.AddMessage(legend_template)         return# needs tidying up for progressor, qolclass NRT_Monitor_Areas(object):    def __init__(self):        """Define the tool (tool name is the name of the class)."""        self.label = "NRT Monitor Areas"        self.description = "Monitor designated monitoring areas."        self.canRunInBackground = False    def getParameterInfo(self):        """        Set various ArcGIS Pro UI controls. Data validation        is enforced via ArcGIS Pro API.        """                # input monitoring area features        par_in_feat = arcpy.Parameter(                        displayName='Input monitoring area features',                        name='in_feat',                        datatype='GPFeatureLayer',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_in_feat.filter.list = ['Polygon']                # input continuous monitoring        par_ongoing = arcpy.Parameter(                        displayName='Continuously monitor areas',                        name='in_ongoing',                        datatype='GPBoolean',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_ongoing.value = False                # time interval months        par_time_interval = arcpy.Parameter(                              displayName='Set hours between monitoring checks',                              name='in_time_interval',                              datatype='GPLong',                              parameterType='Required',                              direction='Input',                              multiValue=False)        par_time_interval.filter.type = 'Range'        par_time_interval.filter.list = [1, 10000]        par_time_interval.value = 12        # email from         par_email_from = arcpy.Parameter(                           displayName='Host email address',                           name='in_email_from',                           datatype='GPString',                           parameterType='Optional',                           direction='Input',                           category='Email Alert Service',                           multiValue=False)        par_email_from.value = 'mrlewie@outlook.com'        # smtp email server        par_smtp_server = arcpy.Parameter(                           displayName='SMTP server address',                           name='in_smtp_server',                           datatype='GPString',                           parameterType='Optional',                           direction='Input',                           category='Email Alert Service',                           multiValue=False)        par_smtp_server.value = 'smtp.office365.com'                # smtp port        par_smtp_port = arcpy.Parameter(                           displayName='SMTP server port',                           name='in_smtp_port',                           datatype='GPLong',                           parameterType='Optional',                           direction='Input',                           category='Email Alert Service',                           multiValue=False)        par_smtp_port.value = 587        # smtp username        par_smtp_username = arcpy.Parameter(                              displayName='SMTP server username',                              name='in_smtp_username',                              datatype='GPString',                              parameterType='Optional',                              direction='Input',                              category='Email Alert Service',                              multiValue=False)        par_smtp_username.value = 'mrlewie@outlook.com'                # smtp username        par_smtp_password = arcpy.Parameter(                              displayName='SMTP server password',                              name='in_smtp_password',                              datatype='GPString',                              parameterType='Optional',                              direction='Input',                              category='Email Alert Service',                              multiValue=False)        par_smtp_password.value = '***'        # # output epsg        # par_output_epsg = arcpy.Parameter(                            # displayName='Output EPSG',                            # name='in_output_epsg',                            # datatype='GPLong',                            # parameterType='Required',                            # direction='Input',                            # category='Warping Options',                            # multiValue=False)        # par_output_epsg.filter.list = [3577]        # par_output_epsg.values = 3577                # # set oa class values        # par_fmask_flags = arcpy.Parameter(displayName='Include pixels flags',                            # name='in_fmask_flags',                            # datatype='GPString',                            # parameterType='Required',                            # direction='Input',                            # category='Satellite Quality Options',                            # multiValue=True)        # flags = [            # 'NoData',             # 'Valid',             # 'Cloud',             # 'Shadow',             # 'Snow',             # 'Water'            # ]        # par_fmask_flags.filter.type = 'ValueList'                # par_fmask_flags.filter.list = flags        # par_fmask_flags.values = ['Valid', 'Snow', 'Water']                # # max cloud cover        # par_max_cloud = arcpy.Parameter(                          # displayName='Maximum cloud cover',                          # name='in_max_cloud',                          # datatype='GPDouble',                          # parameterType='Required',                          # direction='Input',                          # category='Satellite Quality Options',                          # multiValue=False)        # par_max_cloud.filter.type = 'Range'        # par_max_cloud.filter.list = [0.0, 100.0]        # par_max_cloud.value = 0.0                # # input interpolate        # par_interpolate = arcpy.Parameter(                            # displayName='Interpolate NoData pixels',                            # name='in_interpolate',                            # datatype='GPBoolean',                            # parameterType='Required',                            # direction='Input',                            # category='Satellite Quality Options',                            # multiValue=False)        # par_interpolate.value = True                # combine parameters        parameters = [            par_in_feat,            par_ongoing,            par_time_interval,            par_email_from,            par_smtp_server,            par_smtp_port,            par_smtp_username,            par_smtp_password,            #par_output_epsg,            #par_fmask_flags,            #par_max_cloud,            #par_interpolate        ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the NRT Monitor Areas module.        """                # set gdal global environ        import os        os.environ['GDAL_DISABLE_READDIR_ON_OPEN'] = 'EMPTY_DIR'        os.environ['CPL_VSIL_CURL_ALLOWED_EXTENSIONS '] = 'tif'        os.environ['VSI_CACHE '] = 'TRUE'        os.environ['GDAL_HTTP_MULTIRANGE '] = 'YES'        os.environ['GDAL_HTTP_MERGE_CONSECUTIVE_RANGES '] = 'YES'                # also set rasterio env variables        rasterio_env = {            'GDAL_DISABLE_READDIR_ON_OPEN': 'EMPTY_DIR',            'CPL_VSIL_CURL_ALLOWED_EXTENSIONS':'tif',            'VSI_CACHE': True,            'GDAL_HTTP_MULTIRANGE': 'YES',            'GDAL_HTTP_MERGE_CONSECUTIVE_RANGES': 'YES'        }        # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)                # safe imports        import sys                      # arcgis comes with these        import shutil                   # arcgis comes with these        import datetime                 # arcgis comes with these        import numpy as np              # arcgis comes with these        import arcpy                    # arcgis comes with these        import tempfile                 # arcgis comes with these        from datetime import datetime   # arcgis comes with these                # risky imports (not native to arcgis)        try:            import xarray as xr            import dask            import rasterio            import pystac_client            import osr            from odc import stac            from osgeo import gdal            from osgeo import ogr            from osgeo import osr        except:            arcpy.AddError('Python libraries xarray, dask, rasterio, pystac, or odc not installed.')            return                    # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools            # module folder            sys.path.append(FOLDER_MODULES)            import nrt, cog_odc, cog        except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return           # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        warnings.simplefilter(action='ignore', category=UserWarning)        warnings.simplefilter(action='ignore', category=dask.array.core.PerformanceWarning)                             # grab parameter values         in_feat = parameters[0]                            # input monitoring area polygon features        in_ongoing = parameters[1].value                   # perform ongoing monitoring        in_time_interval = parameters[2].value             # hours between checks        in_email_from = parameters[3].value                # email from         in_smtp_server = parameters[4].value               # email smtp server         in_smtp_port = parameters[5].value                 # email smtp port         in_smtp_username = parameters[6].value             # email smtp username         in_smtp_password = parameters[7].value             # email smtp password         #in_epsg = parameters[8].value                      # input epsg        #in_fmask_flags = parameters[9].valueAsText         # fmask flag values        #in_max_cloud = parameters[10].value                # max cloud percentage        #in_interpolate = parameters[11].value              # interpolate missing pixels        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning NRT Monitoring of areas.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=20)        # convert fmask as text to numeric code equivalents              #in_fmask_flags = [e for e in in_fmask_flags.split(';')]                #in_fmask_flags = arc.convert_fmask_codes(in_fmask_flags)                # check if cloud cover is valid        #if in_max_cloud < 0 or in_max_cloud > 100:            #arcpy.AddError('Cloud cover must be between 0 and 100.')            #raise                    # check if time interval is > 0        in_time_interval = in_time_interval * 60 * 60        if in_time_interval <= 0:            arcpy.AddError('Time interval must be above 0 hours.')            return                    # get and check feat count        #feat_count = lyr.GetFeatureCount()        #if feat_count == 0:            #print('No monitoring areas found in feature, flagging as invalid.')            #is_valid = False        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Checking parameters...')        arcpy.SetProgressorPosition(1)        # prepare features shapefile        shp_desc = arcpy.Describe(in_feat)        in_feat = os.path.join(shp_desc.path, shp_desc.name)                                    # validate monitoring area feature class        if not nrt.validate_monitoring_areas(in_feat):            arcpy.AddError('Monitoring areas feature is invalid.')            return                    # get input featureclass file, get dir and filename        in_name = os.path.basename(in_feat)     # name of monitor fc        in_gdb = os.path.dirname(in_feat)       # path of gdb                # check gdv extension        if not in_gdb.endswith('.gdb'):            arcpy.AddError('Feature class is not in a geodatabase.')            return        else:            in_path = os.path.splitext(in_gdb)[0]   # path of gdb without ext            in_data_path = in_path + '_' + 'cubes'  # associated cube data folder                    # check if cubes folder exists        if not os.path.exists(in_data_path):            arcpy.AddError('Could not find cube folder for selected monitoring areas.')            return                        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading monitoring area features...')        arcpy.SetProgressorPosition(2)                # get features (will always have at least one, as we validated earlier)        try:            driver = ogr.GetDriverByName('OpenFileGDB')             data_source = driver.Open(os.path.dirname(in_feat), 0)            feats = data_source.GetLayer('monitoring_areas')        except:            arcpy.AddError('Could not open monitoring areas feature.')            return                        # # # # #        # notify and set progress bar to defaukt        arcpy.SetProgressor(type='default', message='Iterating through monitoring areas...')                        # begin monitoring process iteration        continue_monitoring = True        while continue_monitoring:            for feat in feats:                            # # # # #                # notify                 arcpy.AddMessage('Preparing parameters for area: {}...'.format(feat['area_id']))                                # set up expected netcdf file name and path                out_nc = os.path.join(in_data_path, 'cube' + '_' + feat['area_id'] + '.nc')                                # get start year and latest year for end from dataset                 s_year = '{}-01-01'.format(feat['s_year'])                e_year = '{}-12-31'.format(datetime.now().year)  # now                                # get parameters for platform                # note, this does not consider sentinel nrt at moment.                 # see nrt.sync_nrt_cube for temp solution                # change when coll 3 sentinel avail                #params = nrt.get_satellite_params(platform=feat['platform'])                                                 # transform from albers to wgs84 if needed                geom = feat.geometry()                geom_prj = ogr.CreateGeometryFromWkb(geom.ExportToIsoWkb())  # quick copy                if 'GDA_1994_Australia_Albers' not in geom_prj.ExportToWkt():                    geom_prj = nrt.reproject_ogr_geom(geom=geom_prj, from_epsg=3577, to_epsg=4326)                                    # get bbox in wgs84 from geometry                bbox = geom_prj.GetEnvelope()                bbox = [bbox[0], bbox[2], bbox[1], bbox[3]]                                                   # # # # #                # notify                 arcpy.AddMessage('Checking monitoring area: {}...'.format(feat['area_id']))                                                # check if feature is valid                is_valid = nrt.validate_monitoring_area(area_id=feat['area_id'],                                                        platform=feat['platform'],                                                         s_year=feat['s_year'],                                                         e_year=feat['e_year'],                                                         index=feat['index'])                                                                        # check if monitoring area is valid                if not is_valid:                    arcpy.AddWarning('Invalid monitoring area: {}, skipping.'.format(feat['area_id']))                    continue                # # # # #                # notify                 arcpy.AddMessage('Creating / Syncing monitoring area cube: {}...'.format(feat['area_id']))                                # get parameters for platform                # note, this does not consider sentinel nrt at moment.                 # see nrt.sync_nrt_cube for temp solution                # change when coll 3 sentinel avail                params = nrt.get_satellite_params(platform=feat['platform'])                                  # open existing cube if exists, get date information                ds_existing = None                if os.path.exists(out_nc):                    try:                        ds_existing = xr.open_dataset(out_nc)                        num_times = len(ds_existing['time'])                        s_year = ds_existing.isel(time=-1)                        s_year = str(s_year['time'].dt.strftime('%Y-%m-%d').values)                    except:                        arcpy.AddWarning('Could not open cube: {}, skipping.'.format(feat['area_id']))                        continue                                # sync cube to now. combines old cube with new images if exist                ds = nrt.sync_nrt_cube(out_nc=out_nc,                                       collections=params.get('collections'),                                       bands=params.get('bands'),                                       start_dt=s_year,                                       end_dt=e_year,                                       bbox=bbox,                                       in_epsg=3577,  # always albers                                       slc_off=False,                                       resolution=params.get('resolution'),                                       ds_existing=ds_existing,                                       chunks={})                                                                                         # download, overwrite existing, and compute                with rasterio.Env(**rasterio_env):                    tools.export_xr_as_nc(ds=ds.astype('int16'), filename=out_nc)                    ds = ds.compute()                                                        # if existingt was found, notify and close                 if ds_existing is not None:                    arcpy.AddMessage('Added {} images to cube.'.format(len(ds['time']) - num_times))                # # # # #                # notify                 arcpy.AddMessage('Extracting attributes from cube...')                                # get dataset and band attributes                ds_attrs = ds.attrs                ds_band_attrs = ds[list(ds.data_vars)[0]].attrs                ds_spatial_ref_attrs = ds['spatial_ref'].attrs                                    # # # # #                # notify                 arcpy.AddMessage('Removing invalid pixels and empty dates...')                                # check if expected band name exists                mask_band = arc.get_name_of_mask_band(list(ds.data_vars))                # remove invalid pixels and empty scenes                ds = cog.remove_fmask_dates(ds=ds,                                             valid_class=[1, 4, 5],    # always valid, snow, water                                            max_invalid=0,            # always cloudless images only                                            mask_band=mask_band,                                             nodata_value=np.nan,                                             drop_fmask=True)                # # # # #                # notify                 arcpy.AddMessage('Calculating vegetation index: {}...'.format(feat['index']))                                # conform dea aws band names based on platform                ds = satfetcher.conform_dea_ard_band_names(ds=ds, platform=feat['platform'].lower())                                 # calculate vegetation index                 ds = tools.calculate_indices(ds=ds,                                              index=feat['index'].lower(),                                              custom_name='veg_idx',                                              rescale=False,                                              drop=True)                # append original attributes on to new band                ds['veg_idx'].attrs = ds_band_attrs                # # # # #                # notify                 arcpy.AddMessage('Building and applying edge mask...')                                                # convert feature to layer and use to mask                geom = ogr.Open(feat.ExportToJson(), 0)                lyr = geom.GetLayer()                mask = nrt.mask_xr_via_polygon(geom=lyr,                                                x=ds['x'].data,                                                y=ds['y'].data,                                                bbox=ds.geobox.extent.boundingbox,                                                transform=ds.geobox.transform,                                                ncols=len(ds['x']),                                                nrows=len(ds['y']),                                                mask_value=1)                                                               # apply mask to current dataset, set everything outside to nan                ds = ds.where(mask)                                # # # # #                # notify                 arcpy.AddMessage('Performing static and dynamic change detection...')                                                # perform static, dynamic and summary methods                 ds_change = nrt.build_change_cube(ds,                                                   training_start_year=int(feat['s_year']),                                                   training_end_year=int(feat['e_year']))                                # # # # #                # notify                 arcpy.AddMessage('Cleaning up static and dynamic change detection outputs...')                # re-mask summary and change cubes as nan pixels now set to non-nan                ds_change = ds_change.where(mask)                                # append attributes back on to dataset                 ds_change.attrs = ds_attrs                ds_change['spatial_ref'].attrs = ds_spatial_ref_attrs                for var in list(ds_change.data_vars):                    ds_change[var].attrs = ds_band_attrs                                    # temp                               ds_change.to_netcdf(r'C:\Users\Lewis\Desktop\test_change\change_{}.nc'.format(feat['area_id']))                                                # # # # #                # notify                 arcpy.AddMessage('Applying traffic light algorithm...')                # todo                 # todo                # todo                                                 # # # # #                # email user alert if detected and requested                if feat['alert'] == 'Yes':                                        # if traffic light == True:                                    # notify                     arcpy.AddMessage('Emailing alrert...')                                        #in_email_from = parameters[3].value                # email from                     #in_smtp_server = parameters[4].value               # email smtp server                     #in_smtp_port = parameters[5].value                 # email smtp port                     #in_smtp_username = parameters[6].value             # email smtp username                     #in_smtp_password = parameters[7].value             # email smtp password                                         # send email                    #nrt.send_email_alert(sent_from='mrlewie@outlook.com',                                          #sent_to='mrlewie@outlook.com',                                          #subject='Area ID: {} has been flagged.'.format(feat['area_id']),                                          #body_text='Area ID: {} has been found to have a decline. Please check.'.format(feat['area_id']),                                          #smtp_server='smtp.office365.com',                                          #smtp_port=587,                                          #username='mrlewie@outlook.com',                                          #password='halfLife1985micr')                # close dataset                  ds.close()                ds_change.close()                del ds                del ds_change                                # safely close existing                if ds_existing is not None:                    ds_existing.close()                    del ds_existing                                                                        # set processing to false if not requested to end this            in_time_interval = 0.01                                    if in_ongoing:                arcpy.AddMessage('Hibernating for {} hours. Press stop to cancel.'.format(in_time_interval / 60 / 60))                time.sleep(in_time_interval)            else:                continue_monitoring = False                            # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(3)        # notify user        arcpy.AddMessage('Monitoring areas synced successfully.')        returnclass Tool(object):    def __init__(self):        """Define the tool (tool name is the name of the class)."""        self.label = "Tool"        self.description = "Tool Template"        self.canRunInBackground = False    def getParameterInfo(self):        """Define parameter definitions"""                params = None        return params    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """The source code of the tool."""                            return# testingclass Test(object):    def __init__(self):        """Define the tool (tool name is the name of the class)."""        self.label = "Test"        self.description = "Tool Template"        self.canRunInBackground = False    def getParameterInfo(self):        """Define parameter definitions"""                param1 = arcpy.Parameter(                   displayName='Baseline years',                   name='in_baseline_years',                   datatype='GPValueTable',                   parameterType='Required',                   multiValue=False,                   direction='Input')                        param1.columns = [['GPLong', 'Start year'], ['GPLong', 'End year']]        param1.filters[1].type = 'ValueList'        param1.filters[1].list = [2000, 2001, 2002, 2003, 2004]        #param1.values = [['NAME', 'SUM']]                                        params = [param1]        return params    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """The source code of the tool."""                import time         import arcpy        #import arcgisscripting                # shared folder        sys.path.append(FOLDER_SHARED)        import arc                            # get current project and active map        p = arcpy.mp.ArcGISProject('CURRENT')        m = p.activeMap        # remove temp layer         for layer in m.listLayers():            if 'NRT Create Monitoring Areas Draw' in layer.name:                m.removeLayer(layer)        # apply symbology        for layer in m.listLayers('monitoring_areas'):            try:                arc.apply_monitoring_area_symbology(layer)            except Exception as e:                arcpy.AddMessage(e)                                    # # set transparency            # alpha = 80                            # # get symbology, update renderer, target color field            # sym = layer.symbology            # sym.updateRenderer('UniqueValueRenderer')            # sym.renderer.fields = ['color']                        # # iter group items and colorize            # for grp in sym.renderer.groups:                # for itm in grp.items:                    # try:                        # # get class value and convert to int                        # val = int(itm.values[0][0])                        # # apply fill color                        # if val == 0:                            # itm.symbol.color = {'RGB': [255, 255, 255, alpha]}                        # elif abs(val) == 1:                            # itm.symbol.color = {'RGB': [255, 115, 223, alpha]}                         # elif abs(val) == 2:                            # itm.symbol.color = {'RGB': [115, 178, 255, alpha]}                        # elif abs(val) == 3:                            # itm.symbol.color = {'RGB': [115, 178, 255, alpha]}                        # elif abs(val) == 4:                            # itm.symbol.color = {'RGB': [115, 223, 255, alpha]}                        # elif abs(val) == 5:                            # itm.symbol.color = {'RGB': [115, 255, 223, alpha]}                        # elif abs(val) == 6:                            # itm.symbol.color = {'RGB': [163, 255, 115, alpha]}                        # elif abs(val) == 7:                            # itm.symbol.color = {'RGB': [209, 255, 115, alpha]}                        # elif abs(val) == 8:                            # itm.symbol.color = {'RGB': [255, 255, 115, alpha]}                        # elif abs(val) == 9:                            # itm.symbol.color = {'RGB': [255, 211, 127, alpha]}                        # elif abs(val) == 10:                            # itm.symbol.color = {'RGB': [255, 167, 127, alpha]}                        # elif abs(val) == 11:                            # itm.symbol.color = {'RGB': [255, 127, 127, alpha]}                        # # apply border style now for stable, incline, decline                        # if val == 0:                            # itm.symbol.size = 2                            # itm.symbol.outlineColor = {'RGB': [0, 0, 0, alpha]}                         # elif val > 0:                            # itm.symbol.size = 2                            # itm.symbol.outlineColor = {'RGB': [0, 112, 255, alpha]}                        # elif val < 0:                            # itm.symbol.size = 2                            # itm.symbol.outlineColor = {'RGB': [255, 0, 0, alpha]}                             # except:                        # print('Symbology class value not supported, skipping.')                        # continue                                    # # finally, apply the symbology            # layer.symbology = sym                                                return        