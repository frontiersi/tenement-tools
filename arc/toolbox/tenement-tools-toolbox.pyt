# -*- coding: utf-8 -*-# importsimport osimport certifiimport arcpy# set default gdal and ertifi envs (dev)#os.environ['GDAL_DATA']  = r'C:\Program Files\ArcGIS\Pro\Resources\pedata\gdaldata'#os.environ.setdefault("CURL_CA_BUNDLE", certifi.where())# set default gdal and ertifi envs (non-dev)try:    install_dir = arcpy.GetInstallInfo().get('InstallDir')  # get arcgis install dir    os.environ['GDAL_DATA'] = os.path.join(install_dir, 'Resources\pedata\gdaldata')  # join to gdal install    os.environ.setdefault("CURL_CA_BUNDLE", certifi.where())  # set certifiexcept:    arcpy.AddError('Could not get install directory for ArcGIS Pro or set certifi.')    raise# get location of tenement-tool toolboxtbx_filename = os.path.realpath(__file__)tbx_folder = os.path.dirname(tbx_filename)folder = os.path.dirname(tbx_folder)# globals (non-dev)#FOLDER_MODULES = os.path.join(folder, 'modules')#FOLDER_SHARED = os.path.join(folder, 'shared')#GRP_LYR_FILE = os.path.join(folder, r'arc\lyr\group_template.lyrx')#MON_LYR_FILE = os.path.join(folder, r'arc\lyr\monitor_template.lyrx')# globals (dev)FOLDER_MODULES = r'C:\Users\Lewis\Documents\GitHub\tenement-tools\modules'  FOLDER_SHARED = r'C:\Users\Lewis\Documents\GitHub\tenement-tools\shared'GRP_LYR_FILE = r'C:\Users\Lewis\Documents\GitHub\tenement-tools\arc\lyr\group_template.lyrx'# globals (dea aws)STAC_ENDPOINT = 'https://explorer.sandbox.dea.ga.gov.au/stac/search'STAC_ENDPOINT_ODC = 'https://explorer.sandbox.dea.ga.gov.au/stac'AWS_KEY = ''AWS_SECRET = ''RESULT_LIMIT = 250# check arcgis versionif arcpy.GetInstallInfo().get('Version') not in ['2.8', '2.9']:    arcpy.AddWarning('Not running ArcGIS Pro ver >= 2.8. Plugin only tested on >=2.8.')class Toolbox(object):    def __init__(self):        """Define the toolbox (the name of the toolbox is the name of the        .pyt file)."""           self.label = "Toolbox"        self.alias = "toolbox"        # list of tool classes associated with this toolbox        self.tools = [            COG_Fetch,             COG_Fetch_ODC,            COG_Sync,             COG_Explore,             GDVSpectra_Likelihood,             GDVSpectra_Threshold,             GDVSpectra_Trend,            GDVSpectra_CVA,            Phenolopy_Metrics,            Nicher_SDM,            Nicher_Masker,            VegFrax_Fractional_Cover,            Ensemble_Model,            Ensemble_Masker,            NRT_Create_Project,            NRT_Monitor_Areas            ]class COG_Fetch(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = "COG Fetch"        self.description = "COG contains functions that " \                           "allow for efficient download of " \                           "analysis ready data (ARD) Landsat " \                           "5, 7, 8 or Sentinel 2A, 2B images " \                           "from the Digital Earth Australia " \                           "(DEA) public AWS server."        self.canRunInBackground = False    def getParameterInfo(self):                # input study area shapefile        par_studyarea_feat = arcpy.Parameter(                                displayName="Input study area feature",                                name="in_studyarea_feat",                                datatype="GPFeatureLayer",                                parameterType="Required",                                direction="Input"                                )                                        # set study area to be polygon only        par_studyarea_feat.filter.list = ['Polygon']                                        # output file location        par_out_nc_path = arcpy.Parameter(                                displayName="Output NetCDF file",                                name="out_nc_path",                                datatype="DEFile",                                parameterType="Required",                                direction="Output"                                )                                        # set file type to be netcdf only        par_out_nc_path.filter.list = ['nc']        # in_platform        par_platform = arcpy.Parameter(                            displayName="Satellite platform",                            name="in_platform",                            datatype="GPString",                            parameterType="Required",                            direction="Input",                            multiValue=False                            )                                    # set default platform        par_platform.values = 'Landsat'        par_platform.filter.list = ['Landsat', 'Sentinel']                # in_from_date        par_date_from = arcpy.Parameter(                            displayName="Date from",                            name="in_from_date",                            datatype="GPDate",                            parameterType="Required",                            direction="Input",                            multiValue=False                            )                                    # set in_from_date value        par_date_from.values = '2015/01/01'                # in_to_date        par_date_to = arcpy.Parameter(                        displayName="Date to",                        name="in_to_date",                        datatype="GPDate",                        parameterType="Required",                        direction="Input",                        multiValue=False                        )        # set in_from_date value        par_date_to.values = '2020/12/31'        # set bands        par_bands = arcpy.Parameter(                        displayName="Bands",                        name="in_bands",                        datatype="GPString",                        parameterType="Required",                        direction="Input",                        category='Satellite Bands',                        multiValue=True                        )                         # set landsat bands        bands = [            'Blue',             'Green',             'Red',             'NIR',             'SWIR1',             'SWIR2',             'OA_Mask'            ]                # set default bands        par_bands.filter.type = "ValueList"                par_bands.filter.list = bands        par_bands.values = bands                # set slc-off        par_slc_off = arcpy.Parameter(                        displayName="SLC Off",                        name="in_slc_off",                        datatype="GPBoolean",                        parameterType="Required",                        direction="Input",                        multiValue=False                        )                # set slc-off value        par_slc_off.value = False                       # set output datatype        par_output_dtype = arcpy.Parameter(                            displayName="Output data type",                            name="in_output_dtype",                            datatype="GPString",                            parameterType="Required",                            direction="Input",                            category='Warping Options',                            multiValue=False                            )                                    # set default platform        par_output_dtype.filter.list = ['int8', 'int16', 'float32', 'float64']        par_output_dtype.values = 'int16'                # todo make this changeh when sent/landsat changed        # set output resolution        par_output_res = arcpy.Parameter(                            displayName="Output pixel resolution",                            name="in_output_res",                            datatype="GPLong",                            parameterType="Required",                            direction="Input",                            category='Warping Options',                            multiValue=False                            )                                    # set default platform        par_output_res.filter.type = 'Range'        par_output_res.filter.list = [0, 1000]        par_output_res.value = 30         # todo allow this to handle np.nan        # set output nodata value        par_output_fill_value = arcpy.Parameter(                                    displayName="Output NoData value",                                    name="in_output_fill_value",                                    datatype="GPString",                                    parameterType="Required",                                    direction="Input",                                    category='Warping Options',                                    multiValue=False                                    )                                    # set default nodata value        par_output_fill_value.value = "-999"                # set output epsg        par_output_epsg = arcpy.Parameter(                            displayName="Output EPSG",                            name="in_output_epsg",                            datatype="GPLong",                            parameterType="Required",                            direction="Input",                            category='Warping Options',                            multiValue=False                            )                                    # set default epsg        par_output_epsg.filter.list = [3577]        par_output_epsg.values = 3577                # set resampling type        par_output_resampling = arcpy.Parameter(                            displayName="Resampling type",                            name="in_output_resampling",                            datatype="GPString",                            parameterType="Required",                            direction="Input",                            category='Warping Options',                            multiValue=False                            )                                    # set default resampling        par_output_resampling.filter.list = ['Nearest', 'Bilinear']        par_output_resampling.values = 'Nearest'                # set snap boundary        par_output_snap = arcpy.Parameter(                            displayName="Snap boundaries",                            name="in_snap_bounds",                            datatype="GPBoolean",                            parameterType="Required",                            direction="Input",                            category='Warping Options',                            multiValue=False                            )                # set snap boundary value        par_output_snap.value = True                # set rescale        par_output_rescale = arcpy.Parameter(                        displayName="Rescale",                        name="in_rescale",                        datatype="GPBoolean",                        parameterType="Required",                        direction="Input",                        category='Warping Options',                        multiValue=False                        )                # set rescale value        par_output_rescale.value = True                # set cell alignment        par_output_cell_align = arcpy.Parameter(                            displayName="Cell alignment",                            name="in_output_cell_align",                            datatype="GPString",                            parameterType="Required",                            direction="Input",                            category='Warping Options',                            multiValue=False                            )                                    # set default cell align        par_output_cell_align.filter.list = ['Top-left', 'Center']        par_output_cell_align.values = 'Top-left'                # set chunks        par_output_chunk_size = arcpy.Parameter(                            displayName="Chunk size",                            name="in_output_chunk_size",                            datatype="GPLong",                            parameterType="Required",                            direction="Input",                            category='Parallelisation',                            multiValue=False                            )                                    # set default chunksize        par_output_chunk_size.value = -1                # set dea aws stac url        par_output_stac_url = arcpy.Parameter(                                displayName="Digital Earth Australia STAC URL",                                name="in_output_stac_url",                                datatype="GPString",                                parameterType="Required",                                direction="Input",                                category='STAC Options',                                multiValue=False                                )                 # set default dea aws stac url        par_output_stac_url.value = STAC_ENDPOINT        # set dea aws key        par_output_aws_key = arcpy.Parameter(                                displayName="Digital Earth Australia AWS Key",                                name="in_output_aws_key",                                datatype="GPString",                                parameterType="Optional",                                direction="Input",                                category='STAC Options',                                multiValue=False                                )                 # set default dea aws key value        par_output_aws_key.value = AWS_KEY         # set dea aws secret        par_output_aws_secret = arcpy.Parameter(                                displayName="Digital Earth Australia AWS Secret Key",                                name="in_output_aws_secret",                                datatype="GPString",                                parameterType="Optional",                                direction="Input",                                category='STAC Options',                                multiValue=False                                )                 # set default dea aws secret value        par_output_aws_secret.value = AWS_SECRET                # combine parameters        parameters = [            par_studyarea_feat,            par_out_nc_path,            par_platform,            par_date_from,            par_date_to,            par_bands,            par_slc_off,            par_output_dtype,            par_output_res,            par_output_fill_value,            par_output_epsg,            par_output_resampling,            par_output_snap,            par_output_rescale,            par_output_cell_align,            par_output_chunk_size,            par_output_stac_url,            par_output_aws_key,            par_output_aws_secret        ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # modify bands list when platform changed        if parameters[2].value == 'Landsat' and not parameters[2].hasBeenValidated:                    # enable slc-off control            parameters[6].enabled = True                        # update landsat band list            bands = [                'Blue',                 'Green',                 'Red',                 'NIR',                 'SWIR1',                 'SWIR2',                 'OA_Mask'                ]                        # update bands and set to default resolution            parameters[5].filter.list = bands            parameters[5].values = bands            parameters[8].value = 30        elif parameters[2].value == 'Sentinel' and not parameters[2].hasBeenValidated:                    # disable slc-off control            parameters[6].enabled = False                        # update sentinel band list            bands = [                'Blue',                 'Green',                 'Red',                 'NIR1',                 'SWIR2',                 'SWIR3',                 'OA_Mask'                ]                        # update values in control            parameters[5].filter.list = bands            parameters[5].values = bands                        # set resolution to original 10x10m            parameters[8].value = 10        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        """                                # imports        import os, sys        import io        import time        import numpy as np        import xarray as xr        import dask        import dask.array as da        import arcpy        # import tools        sys.path.append(FOLDER_SHARED)        import arc, tools, satfetcher                # import gdvspectra module        sys.path.append(FOLDER_MODULES)        import cog                # notify         arcpy.AddMessage('Performing COG Fetch.')                                                    # grab parameter values         in_studyarea_feat = parameters[0].value         # study area feat         out_nc = parameters[1].valueAsText              # output nc         in_platform = parameters[2].value               # platform name        in_from_date = parameters[3].value              # from date        in_to_date = parameters[4].value                # to date        in_bands = parameters[5].valueAsText            # bands        in_slc_off = parameters[6].value                # slc off         in_dtype = parameters[7].value                  # output pixel dtype        in_res = parameters[8].value                    # output pixel resolution        in_fill_value = parameters[9].value             # todo processing string to int, float or np.nan        in_epsg = parameters[10].value                  # output epsg         in_resampling = parameters[11].value            # output resampler method         in_snap = parameters[12].value                  # output snap alignment         in_rescale = parameters[13].value               # output rescale        in_cell_align = parameters[14].value            # output cell alignmnent             in_chunk_size = parameters[15].value            # chunk size        in_stac_endpoint = parameters[16].value         # stac endpoint        in_aws_key = parameters[17].value               # dea aws key        in_aws_secret = parameters[18].value            # dea aws secret                # let user know that aws key and secret not yet implemented        if in_aws_key is not None or in_aws_secret is not None:            arcpy.AddWarning('AWS Credentials not yet supported. Using DEA AWS.')                # set up progess bar        arcpy.SetProgressor(type='default', message='Preparing query parameters...')                # get minimum bounding geom from input         bbox = arc.get_selected_layer_extent(in_studyarea_feat)                # get collections based on platform         collections = arc.prepare_collections_list(in_platform)                    # prepare start and end date times        in_from_date = arc.datetime_to_string(in_from_date)        in_to_date = arc.datetime_to_string(in_to_date)                # fetch stac data         arcpy.SetProgressorLabel('Performing STAC query...')        feats = cog.fetch_stac_data(stac_endpoint=in_stac_endpoint,                                     collections=collections,                                     start_dt=in_from_date,                                     end_dt=in_to_date,                                     bbox=bbox,                                    slc_off=in_slc_off,                                    limit=RESULT_LIMIT)                # count number of items        arcpy.AddMessage('Found {} {} scenes.'.format(len(feats), in_platform))        # prepare band (i.e. stac assets) names        assets = arc.prepare_band_names(in_bands=in_bands,                                         in_platform=in_platform)                                                            # convert raw stac into dict with coord reproject, etc.        arcpy.SetProgressorLabel('Converting STAC data into useable format...')        meta, asset_table = cog.prepare_data(feats,                                              assets=assets,                                             bounds_latlon=bbox,                                              bounds=None,                                              epsg=in_epsg,                                              resolution=in_res,                                              snap_bounds=in_snap,                                             force_dea_http=True)                                                     # prepare resample and fill value types        resampling = in_resampling.lower()        fill_value = arc.prepare_fill_value_type(in_fill_value)                                                                                                  # convert assets to dask array        arcpy.SetProgressorLabel('Parallelising data...')        darray = cog.convert_to_dask(meta=meta,                                      asset_table=asset_table,                                      chunksize=in_chunk_size,                                     resampling=resampling,                                      dtype=in_dtype,                                      fill_value=fill_value,                                      rescale=in_rescale)                                             # prepare alignment type        cell_align = arc.prepare_cell_align_type(in_cell_align)        # generate coordinates and dimensions from metadata        arcpy.SetProgressorLabel('Building dataset metadata...')        coords, dims = cog.build_coords(feats=feats,                                        assets=assets,                                         meta=meta,                                        pix_loc=cell_align)                # build final xarray data array        arcpy.SetProgressorLabel('Finalising dataset...')        ds_name = 'stac-' + dask.base.tokenize(darray)        ds = xr.DataArray(darray,                          coords=coords,                          dims=dims,                          name=ds_name                          )                                 # comvert to cleaner xarray dataset        ds = ds.to_dataset(dim='band')                # append attributes onto dataset        ds = cog.build_attributes(ds=ds,                                  meta=meta,                                   collections=collections,                                   bands=assets,                                  slc_off=in_slc_off,                                   bbox=bbox,                                  dtype=in_dtype,                                  snap_bounds=in_snap,                                  fill_value=fill_value,                                   rescale=in_rescale,                                  cell_align=in_cell_align,                                  resampling=in_resampling)                                             # set up proper progress bar        arcpy.SetProgressor(type='step',                             message='Preparing data download...',                             min_range=0,                             max_range=len(ds.data_vars) + 1)        # get list of dataset vars and iterate compute on each        for counter, data_var in enumerate(list(ds.data_vars), start=1):                    # start clock            start = time.time()                    # update progress bar            arcpy.SetProgressorLabel('Downloading band: {}...'.format(data_var))            arcpy.SetProgressorPosition(counter)                    # compute!            ds[data_var] = ds[data_var].compute()                        # notify time             duration = round(time.time() - start, 2)            arcpy.AddMessage('Band: {} took: {}s to download.'.format(data_var, duration))                                          # wrap up         arcpy.SetProgressorLabel('Exporting NetCDF...')        arcpy.SetProgressorPosition(counter + 1)                        # export netcdf to output folder        tools.export_xr_as_nc(ds=ds, filename=out_nc)        # notify finish        arcpy.AddMessage('COG Fetch completed successfully.')                return# implement resamplingclass COG_Fetch_ODC(object):    def __init__(self):        """        Initialise tool.        """            # set tool name, description, options        self.label = "COG Fetch ODC"        self.description = "COG contains functions that " \                           "allow for efficient download of " \                           "analysis ready data (ARD) Landsat " \                           "5, 7, 8 or Sentinel 2A, 2B images " \                           "from the Digital Earth Australia " \                           "(DEA) public AWS server."        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """                # input study area shapefile        par_studyarea_feat = arcpy.Parameter(                               displayName='Input study area feature',                               name='in_studyarea_feat',                               datatype='GPFeatureLayer',                               parameterType='Required',                               direction='Input')        par_studyarea_feat.filter.list = ['Polygon']                                        # output file location        par_out_nc_path = arcpy.Parameter(                            displayName='Output NetCDF file',                            name='out_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_path.filter.list = ['nc']        # satellite platform        par_platform = arcpy.Parameter(                         displayName='Satellite platform',                         name='in_platform',                         datatype='GPString',                         parameterType='Required',                         direction='Input',                         multiValue=False)        par_platform.filter.list = ['Landsat', 'Sentinel']        par_platform.values = 'Landsat'                # start date        par_date_from = arcpy.Parameter(                          displayName='Date from',                          name='in_from_date',                          datatype='GPDate',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_date_from.values = '2015/01/01'                # end date        par_date_to = arcpy.Parameter(                        displayName='Date to',                        name='in_to_date',                        datatype='GPDate',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_date_to.values = '2020/12/31'        # satellite bands        par_bands = arcpy.Parameter(                      displayName='Bands',                      name='in_bands',                      datatype='GPString',                      parameterType='Required',                      direction='Input',                      category='Satellite bands',                      multiValue=True)        bands = [            'Blue',             'Green',             'Red',             'NIR',             'SWIR1',             'SWIR2',             'OA_Mask'            ]        par_bands.filter.type = 'ValueList'                par_bands.filter.list = bands        par_bands.values = bands                # slc-off        par_slc_off = arcpy.Parameter(                        displayName='SLC Off',                        name='in_slc_off',                        datatype='GPBoolean',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_slc_off.value = False                       # output datatype        par_output_dtype = arcpy.Parameter(                             displayName='Output data type',                             name='in_output_dtype',                             datatype='GPString',                             parameterType='Required',                             direction='Input',                             category='Warping Options',                             multiValue=False)        par_output_dtype.filter.list = ['int16', 'float32']        par_output_dtype.values = 'int16'                # output resolution        par_output_res = arcpy.Parameter(                           displayName='Output pixel resolution',                           name='in_output_res',                           datatype='GPLong',                           parameterType='Required',                           direction='Input',                           category='Warping Options',                           multiValue=False)        par_output_res.filter.type = 'Range'        par_output_res.filter.list = [1, 1000]        par_output_res.value = 30         # output nodata value        par_output_fill_value = arcpy.Parameter(                                  displayName='Output NoData value',                                  name='in_output_fill_value',                                  datatype='GPString',                                  parameterType='Required',                                  direction='Input',                                  category='Warping Options',                                  multiValue=False)        par_output_fill_value.value = '-999'                # output epsg        par_output_epsg = arcpy.Parameter(                            displayName='Output EPSG',                            name='in_output_epsg',                            datatype='GPLong',                            parameterType='Required',                            direction='Input',                            category='Warping Options',                            multiValue=False)        par_output_epsg.filter.list = [3577]        par_output_epsg.values = 3577                # output resampling type        par_output_resampling = arcpy.Parameter(                            displayName='Resampling type',                            name='in_output_resampling',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Warping Options',                            multiValue=False)        par_output_resampling.filter.list = ['Nearest', 'Bilinear']        par_output_resampling.values = 'Nearest'                # output snap boundary        #par_output_snap = arcpy.Parameter(                            #displayName='Snap boundaries',                            #name='in_snap_bounds',                            #datatype='GPBoolean',                            #parameterType='Required',                            #direction='Input',                            #category='Warping Options',                            #multiValue=False)        #par_output_snap.value = True                # output rescale        #par_output_rescale = arcpy.Parameter(                        #displayName='Rescale',                        #name='in_rescale',                        #datatype='GPBoolean',                        #parameterType='Required',                        #direction='Input',                        #category='Warping Options',                        #multiValue=False)        #par_output_rescale.value = True                # output cell alignment        #par_output_cell_align = arcpy.Parameter(                                  #displayName='Cell alignment',                                  #name='in_output_cell_align',                                  #datatype='GPString',                                  #parameterType='Required',                                  #direction='Input',                                  #category='Warping Options',                                  #multiValue=False)        #par_output_cell_align.filter.list = ['Top-left', 'Center']        #par_output_cell_align.values = 'Top-left'                # dask chunks        par_output_chunk_size = arcpy.Parameter(                                  displayName='Chunk size',                                  name='in_output_chunk_size',                                  datatype='GPLong',                                  parameterType='Required',                                  direction='Input',                                  category='Parallelisation',                                  multiValue=False)        par_output_chunk_size.value = -1                # dea aws stac url        par_output_stac_url = arcpy.Parameter(                                displayName='Digital Earth Australia STAC URL',                                name='in_output_stac_url',                                datatype='GPString',                                parameterType='Required',                                direction='Input',                                category='STAC Options',                                multiValue=False)        par_output_stac_url.value = STAC_ENDPOINT_ODC        # dea aws key        par_output_aws_key = arcpy.Parameter(                                displayName='Digital Earth Australia AWS Key',                                name='in_output_aws_key',                                datatype='GPString',                                parameterType='Optional',                                direction='Input',                                category='STAC Options',                                multiValue=False)        par_output_aws_key.value = AWS_KEY         # dea aws secret        par_output_aws_secret = arcpy.Parameter(                                displayName='Digital Earth Australia AWS Secret Key',                                name='in_output_aws_secret',                                datatype='GPString',                                parameterType='Optional',                                direction='Input',                                category='STAC Options',                                multiValue=False)        par_output_aws_secret.value = AWS_SECRET                # output cell alignment        par_save_type = arcpy.Parameter(                          displayName='Save type',                          name='in_save_type',                          datatype='GPString',                          parameterType='Required',                          direction='Input',                          category='Output Options',                          multiValue=False)        par_save_type.filter.list = ['Memory', 'Local']        par_save_type.values = 'Memory'                # combine parameters        parameters = [            par_studyarea_feat,            par_out_nc_path,            par_platform,            par_date_from,            par_date_to,            par_bands,            par_slc_off,            par_output_dtype,            par_output_res,            par_output_fill_value,            par_output_epsg,            par_output_resampling,            #par_output_snap,            #par_output_rescale,            #par_output_cell_align,            par_output_chunk_size,            par_output_stac_url,            par_output_aws_key,            par_output_aws_secret,            par_save_type        ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # modify bands list when platform changed        if parameters[2].value == 'Landsat' and not parameters[2].hasBeenValidated:                    # enable slc-off control            parameters[6].enabled = True                        # update landsat band list            bands = [                'Blue',                 'Green',                 'Red',                 'NIR',                 'SWIR1',                 'SWIR2',                 'OA_Mask'                ]                        # update bands and set to default resolution            parameters[5].filter.list = bands            parameters[5].values = bands            parameters[8].value = 30        elif parameters[2].value == 'Sentinel' and not parameters[2].hasBeenValidated:                    # disable slc-off control            parameters[6].enabled = False                        # update sentinel band list            bands = [                'Blue',                 'Green',                 'Red',                 'NIR1',                 'SWIR2',                 'SWIR3',                 'OA_Mask'                ]                        # update bands and set to default resolution            parameters[5].filter.list = bands            parameters[5].values = bands            parameters[8].value = 10        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the COG Fetch (ODC) module.        """                # set gdal global environ        import os        os.environ['GDAL_DISABLE_READDIR_ON_OPEN'] = 'EMPTY_DIR'        os.environ['CPL_VSIL_CURL_ALLOWED_EXTENSIONS '] = 'tif'        os.environ['VSI_CACHE '] = 'TRUE'        os.environ['GDAL_HTTP_MULTIRANGE '] = 'YES'        os.environ['GDAL_HTTP_MERGE_CONSECUTIVE_RANGES '] = 'YES'                # also set rasterio env variables        rasterio_env = {            'GDAL_DISABLE_READDIR_ON_OPEN': 'EMPTY_DIR',            'CPL_VSIL_CURL_ALLOWED_EXTENSIONS':'tif',            'VSI_CACHE': True,            'GDAL_HTTP_MULTIRANGE': 'YES',            'GDAL_HTTP_MERGE_CONSECUTIVE_RANGES': 'YES'        }        # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)                # safe imports        import sys                  # arcgis comes with these        import datetime                 # arcgis comes with these        import numpy as np              # arcgis comes with these        import arcpy                    # arcgis comes with these        from datetime import datetime   # arcgis comes with these                # risky imports (not native to arcgis)        try:            import xarray as xr            import dask            import rasterio            import pystac_client            from odc import stac        except:            arcpy.AddError('Python libraries xarray, dask, rasterio, pystac, or odc not installed.')            raise                        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                    # module folder            sys.path.append(FOLDER_MODULES)            import cog_odc        except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            raise        # grab parameter values        in_studyarea_feat = parameters[0].value         # study area feat         out_nc = parameters[1].valueAsText              # output nc         in_platform = parameters[2].value               # platform name        in_from_date = parameters[3].value              # from date        in_to_date = parameters[4].value                # to date        in_bands = parameters[5].valueAsText            # bands        in_slc_off = parameters[6].value                # slc off         in_dtype = parameters[7].value                  # output pixel dtype        in_res = parameters[8].value                    # output pixel resolution        in_fill_value = parameters[9].value             # todo processing string to int, float or np.nan        in_epsg = parameters[10].value                  # output epsg         in_resampling = parameters[11].value            # output resampler method         #in_snap = parameters[12].value                 # output snap alignment         #in_rescale = parameters[13].value              # output rescale        #in_cell_align = parameters[14].value           # output cell alignmnent             in_chunk_size = parameters[12].value            # chunk size        in_stac_endpoint = parameters[13].value         # stac endpoint        in_aws_key = parameters[14].value               # dea aws key        in_aws_secret = parameters[15].value            # dea aws secret        in_save_type = parameters[16].value             # save type                        # # # # #        # notify user and set proressbar        arcpy.SetProgressor(type='default',                             message='Preparing parameters...')                # aws keys not implemented yet        if in_aws_key is not None or in_aws_secret is not None:            arcpy.AddWarning('AWS Credentials not yet supported. Using DEA AWS.')                # get minimum bounding geom from input         bbox = arc.get_selected_layer_extent(in_studyarea_feat)                # get collections based on platform         collections = arc.prepare_collections_list(in_platform)                    # prepare start and end date times        in_from_date = arc.datetime_to_string(in_from_date)        in_to_date = arc.datetime_to_string(in_to_date)                # prepare band (i.e. stac assets) names        bands = arc.prepare_band_names(in_bands=in_bands,                                        in_platform=in_platform)                                               # prepare resample and fill value types        resampling = in_resampling.lower()        fill_value = arc.prepare_fill_value_type(in_fill_value)        # # # # #        # notify user and set progress        arcpy.SetProgressor(type='default',                             message='Performing STAC query and obtaining items...')                try:                   # fetch stac items            items = cog_odc.fetch_stac_items_odc(stac_endpoint=in_stac_endpoint,                                                  collections=collections,                                                  start_dt=in_from_date,                                                  end_dt=in_to_date,                                                  bbox=bbox,                                                 slc_off=in_slc_off,                                                 limit=RESULT_LIMIT)                        # count number of items            arcpy.AddMessage('Found {} {} scenes.'.format(len(items), in_platform))                    except:            arcpy.AddError('Could not obtain DEA AWS STAC items.')            raise                # # # # #        # notify user and set progress        arcpy.SetProgressor(type='default',                             message='Replacing s3 url prefix with https...')                # replace s3 prefix with https for each band - arcgis doesnt like s3        items = cog_odc.replace_items_s3_to_https(items=items,                                                   from_prefix='s3://dea-public-data',                                                   to_prefix='https://data.dea.ga.gov.au')        # # # # #        # notify user and set progress        arcpy.SetProgressor(type='default',                             message='Converting STAC data into xr dataset via odc-stac...')        # construct an xr of items (lazy)        try:            ds = cog_odc.build_xr_odc(items=items,                                      bbox=bbox,                                      bands=bands,                                      crs=in_epsg,                                      resolution=in_res,                                      group_by='solar_day',                                      skip_broken_datasets=True,                                      like=None,                                      chunks={})                    except:            arcpy.AddError('Could not build xr dataset via odc-stac.')            raise                                 # # # # #        # notify user and set progress        arcpy.SetProgressor(type='default',                             message='Preparing xr dataset for ArcGIS compatibility...')                                    # convert whole dataset from uint16 to int16 (to handle -999 nodata)        ds = cog_odc.convert_type(ds=ds, to_type=in_dtype)                # change nodata value from 0 to -999 to align with original cog method        ds = cog_odc.change_nodata_odc(ds=ds,                                       orig_value=0,                                        fill_value=fill_value)                # correct xr dataset datetimes for arcgis compatiability        ds = cog_odc.fix_xr_time_for_arc_cog(ds)                        # # # # #        # notify user and set progress        arcpy.SetProgressor(type='default',                             message='Appending original query attributes on to dataset...')                      # append query attributes on        ds = cog_odc.append_query_attrs_odc(ds=ds,                                            bbox=bbox,                                            collections=collections,                                             bands=bands,                                             resolution=in_res,                                            dtype=in_dtype,                                             fill_value=fill_value,                                            slc_off=in_slc_off,                                             resampling=resampling)          # # # # #        # download, compute / save netcdf!        if in_save_type == 'Memory':                        # set up proper progress bar            arcpy.SetProgressor(type='step',                                 message='Beginning to download data cube... This can take awhile.',                                 min_range=0,                                 max_range=len(ds.data_vars) + 1)            try:                for counter, data_var in enumerate(list(ds.data_vars), start=1):                                    # start clock                    start = time.time()                                    # update progress bar                    arcpy.SetProgressorLabel('Downloading band: {}...'.format(data_var))                    arcpy.SetProgressorPosition(counter)                                    # compute!                    with rasterio.Env(**rasterio_env):                        ds[data_var] = ds[data_var].compute()                                        # notify time                     duration = round((time.time() - start) / 60, 2)                    arcpy.AddMessage('Band: {} took: {} min to download.'.format(data_var, duration))                                # wrap up                 arcpy.SetProgressorLabel('Exporting NetCDF...')                arcpy.SetProgressorPosition(counter + 1)                                    # export netcdf to output folder                tools.export_xr_as_nc(ds=ds, filename=out_nc)                        except Exception as e:                arcpy.AddError(e)                raise                    else:            # notify user and set progress            arcpy.SetProgressor(type='default',                                 message='Beginning to download data cube... This can take awhile.')                                            # start clock            start = time.time()                        try:                # export netcdf to output folder                with rasterio.Env(**rasterio_env):                    tools.export_xr_as_nc(ds=ds, filename=out_nc)                        except Exception as e:                arcpy.AddError(e)                raise            # notify time             duration = round((time.time() - start) / 60, 2)            arcpy.AddMessage('NetCDF took: {} min to download.'.format(duration))                    # notify finish        arcpy.AddMessage('COG Fetch completed successfully.')                returnclass COG_Sync(object):    def __init__(self):            # set tool name        self.label = "COG Sync"                # set tool description        self.description = "Sync COG to update cube with latest " \                           "data."                                   # set false for pro        self.canRunInBackground = False    def getParameterInfo(self):            # input netcdf file        par_nc_file = arcpy.Parameter(                        displayName="Input NetCDF file",                        name="in_nc_file",                        datatype="DEFile",                        parameterType="Required",                        direction="Input"                        )                                        # set options        par_nc_file.filter.list = ['nc']                # combine parameters        parameters = [            par_nc_file            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):                # imports        import os, sys        #import io        #import time        import pandas as pd        import numpy as np        import xarray as xr        import arcpy        # import tools        sys.path.append(FOLDER_SHARED)        import arc, tools, satfetcher                # import gdvspectra module        sys.path.append(FOLDER_MODULES)        import cog                # globals         AWS_KEY = ''        AWS_SECRET = ''        STAC_ENDPOINT = 'https://explorer.sandbox.dea.ga.gov.au/stac/search'        RESULT_LIMIT = 250                # notify         arcpy.AddMessage('Performing COG Sync.')                                                    # grab parameter values         in_nc = parameters[0].valueAsText      # raw netcdf        # set up progess bar        arcpy.SetProgressor(type='default', message='Loading and checking netcdf...')                # load netcdf file as xr        ds = satfetcher.load_local_nc(nc_path=in_nc,                                       use_dask=True,                                       conform_nodata_to=np.nan)  #nodatavals?                # checks        if 'time' not in ds:            arcpy.AddError('No time dimension detected.')                #tod other checks                 # get original query attributes         arcpy.SetProgressorLabel('Getting original query parameters...')        # check attributes        in_bands = list(ds.data_vars)        collections = list(ds.orig_collections)        bbox = list(ds.orig_bbox)        in_res = ds.res # use get xr res method        crs = ds.crs        in_slc_off = ds.orig_slc_off        resampling = ds.orig_resample        nodatavals = ds.nodatavals        # need to do        in_epsg = int(crs.split(':')[1])        in_platform = 'Landsat'        dtype = 'int16'        fill_value = -999        in_snap = True        rescale = True        cell_align = 'Top-left'        chunk_size = -1                # get datetimes        arcpy.SetProgressorLabel('Assessing dates...')        # get now, earliest, latest datetimes in dataset        dt_now = np.datetime64('now')        dt_first = ds['time'].isel(time=0).values        dt_last = ds['time'].isel(time=-1).values        # conver to stac format        in_from_date = arc.datetime_to_string(pd.Timestamp(dt_last))        in_to_date = arc.datetime_to_string(pd.Timestamp(dt_now))        # check if xr dt less than now (will be for now, but not if override)        if dt_last < dt_now:                        # fetch cog            arcpy.SetProgressorLabel('Performing STAC query...')            feats = cog.fetch_stac_data(stac_endpoint=STAC_ENDPOINT,                                         collections=collections,                                         start_dt=in_from_date,                                         end_dt=in_to_date,                                         bbox=bbox,                                        slc_off=in_slc_off,                                        limit=RESULT_LIMIT)                                                    # count number of items            arcpy.AddMessage('Found {} {} scenes.'.format(len(feats), in_platform))                                # prepare band (i.e. stac assets) names            assets = in_bands            #assets = arc.prepare_band_names(in_bands=in_bands,                                             #in_platform=in_platform)                            # convert raw stac into dict with coord reproject, etc.            arcpy.SetProgressorLabel('Converting STAC data into useable format...')            meta, asset_table = cog.prepare_data(feats,                                                  assets=assets,                                                 bounds_latlon=bbox,                                                  bounds=None,                                                  epsg=in_epsg,                                                  resolution=in_res,                                                  snap_bounds=in_snap,                                                 force_dea_http=True)                                                                                    else:            arcpy.AddMessage('No new scenes available. No sync required.')            returnclass COG_Explore(object):    def __init__(self):        self.label = "COG Explore"        self.description = "Explore an existing multidimensional " \                            "raster layer downloaded using COG " \                            "Fetcher."        self.canRunInBackground = False    def getParameterInfo(self):            # input netcdf file        par_nc_file = arcpy.Parameter(                        displayName="Input NetCDF file",                        name="in_nc_file",                        datatype="DEFile",                        parameterType="Required",                        direction="Input"                        )                                        # set options        par_nc_file.filter.list = ['nc']        # input vegetation index         par_veg_idx = arcpy.Parameter(                        displayName="Set vegetation index",                        name="in_veg_idx",                        datatype="GPString",                        parameterType="Required",                        direction="Input",                        multiValue=False                        )                                # set options        par_veg_idx.filter.type = 'ValueList'        par_veg_idx.filter.list = [            'NDVI',            'EVI',             'SAVI',            'MSAVI',            'SLAVI',            'MAVI',            'kNDVI',            'TCG'        ]        par_veg_idx.value = 'MAVI'                               # input interpolate        par_interpolate = arcpy.Parameter(                            displayName="Interpolate missing pixels",                            name="in_interpolate",                            datatype="GPBoolean",                            parameterType="Required",                            direction="Input",                            multiValue=False                            )                # set slc-off value        par_interpolate.value = True        # set oa class values        par_fmask_flags = arcpy.Parameter(displayName="Include flags",                                          name="in_fmask_flags",                                          datatype="GPString",                                          parameterType="Required",                                          direction="Input",                                          category='Quality Options',                                          multiValue=True                                          )        # set options        fmask_flags = [            'NoData',             'Valid',             'Cloud',             'Shadow',             'Snow',             'Water'        ]                # set default bands        par_fmask_flags.filter.type = "ValueList"                par_fmask_flags.filter.list = fmask_flags        par_fmask_flags.values = ['Valid', 'Snow', 'Water']                # max cloud cover        par_max_cloud = arcpy.Parameter(                            displayName="Maximum cloud cover",                            name="in_max_cloud",                            datatype="GPDouble",                            parameterType="Optional",                            direction="Input",                            category='Quality Options',                            multiValue=False                            )                                    # set default platform        par_max_cloud.filter.type = 'Range'        par_max_cloud.filter.list = [0.0, 100.0]        par_max_cloud.value = 10.0        # combine parameters        parameters = [            par_nc_file,            par_veg_idx,            par_interpolate,            par_fmask_flags,            par_max_cloud        ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):                        # imports        import os, sys        import numpy as np        import xarray as xr        import arcpy        from datetime import datetime        from tempfile import NamedTemporaryFile                # import tools        sys.path.append(FOLDER_SHARED)        import arc, tools, satfetcher                # import gdvspectra module        sys.path.append(FOLDER_MODULES)        import cog                # notify         arcpy.AddMessage('Opening COG Explore.')                                                    # grab parameter values         in_nc = parameters[0].valueAsText      # raw netcdf        in_veg_idx = parameters[1].value       # vege index name        in_interpolate = parameters[2].value   # interpolate missing pixels        in_fmask_flags = parameters[3].valueAsText   # fmask flag values        in_max_cloud = parameters[4].value     # max cloud percentage        # set up progess bar        arcpy.SetProgressor(type='default', message='Loading and checking netcdf...')                # load netcdf file as xr        ds = satfetcher.load_local_nc(nc_path=in_nc,                                       use_dask=True,                                       conform_nodata_to=np.nan)                # tcheck if vars/bands exist        if len(ds.data_vars) == 0:            raise ValueError('No satellite bands detected in NetCDF.')                        # obtain band attributes for any band        band_attrs = ds[list(ds.data_vars)[0]].attrs                # get platform of current netcdf, expects dea aws labels        collections = ds.attrs.get('orig_collections')                # todo: put in func. grab collections whether string or tuple/list        if isinstance(collections, (list, tuple)) and len(collections) > 0:            in_platform = collections[0]        elif isinstance(collections, str):            in_platform = collections        else:            raise ValueError('Input NetCDF missing DEA AWS metadata.')                    # parse dea aws platform code from collections attirbute        if in_platform[:5] == 'ga_ls':            in_platform = 'Landsat'        elif in_platform[:2] == 's2':            in_platform = 'Sentinel'        else:            raise ValueError('Platform in NetCDF is not supported.')        # set name of mask band depending on platform        if in_platform == 'Landsat':            mask_band = 'oa_fmask'        elif in_platform == 'Sentinel':            mask_band = 'fmask'        else:            raise ValueError('No DEA AWS compatible mask band detected.')                # todo : func to do all this (convert value to num etc)        flags = [e for e in in_fmask_flags.split(';')]                # unpack flags as indexes. todo: move to arc function        in_fmask_flags = []        for flag in flags:            if flag == 'NoData':                in_fmask_flags.append(0)            elif flag == 'Valid':                in_fmask_flags.append(1)            elif flag == 'Cloud':                in_fmask_flags.append(2)            elif flag == 'Shadow':                in_fmask_flags.append(3)            elif flag == 'Snow':                in_fmask_flags.append(4)            elif flag == 'Water':                in_fmask_flags.append(5)                                        # remove clouded pixels and empty dates        arcpy.SetProgressorLabel('Removing invalid pixels and empty dates...')        ds = cog.remove_fmask_dates(ds=ds,                                     valid_class=in_fmask_flags,                                     max_invalid=in_max_cloud,                                     mask_band=mask_band,                                     nodata_value=np.nan,                                     drop_fmask=True)                  # generate mavi and drop bands         arcpy.SetProgressorLabel('Conforming band names...')        ds = satfetcher.conform_dea_ard_band_names(ds=ds,                                                    platform=in_platform.lower())                                                                                                                                                 # generate mavi and drop bands         arcpy.SetProgressorLabel('Calculating {}...'.format(in_veg_idx))        ds = tools.calculate_indices(ds=ds,                                      index=in_veg_idx.lower(),                                      custom_name=in_veg_idx.lower(),                                      rescale=False,                                      drop=True)                                          # add band attrs back on        ds[in_veg_idx.lower()].attrs = band_attrs                   # interpolate         if in_interpolate:            arcpy.SetProgressorLabel('Interpolating missing pixels...')            ds = ds.interpolate_na(dim='time', method='nearest')                # todo make this efficient        # create temp netcdf with clean data         with NamedTemporaryFile() as tmp:            fn = tmp.name + '.nc'            ds.to_netcdf(fn)                    # temp disable auto add of outputs to map        arcpy.env.addOutputsToMap = False                # prepare output filename and folder        in_folder = os.path.dirname(in_nc)        dt = datetime.now().strftime("%d%m%Y%H%M%S")        out_crf = os.path.join(in_folder, 'mdr' + '_' + dt + '.crf')                # export new multidim raster raster for visualise        mdr = arcpy.CopyRaster_management(in_raster=fn,                                           out_rasterdataset=out_crf)                                                  # todo make this safe        aprx = arcpy.mp.ArcGISProject('CURRENT')        m = aprx.activeMap        m.addDataFromPath(mdr)                    # re-enable auto add to map and apply cmap        arcpy.env.addOutputsToMap = True        lyr = arc.apply_cmap(aprx=aprx,                              lyr_name='mdr' + '_' + dt + '.crf',                              cmap_name='Precipitation',                              cutoff_pct=0.5)                                     # close and del dataset        ds.close()        del ds                returnclass GDVSpectra_Likelihood(object):    def __init__(self):        """        Initialise tool.        """            # set tool name, description, options        self.label = 'GDVSpectra Likelihood'        self.description = 'GDVSpectra Likelihood derives potential groundwater ' \                           'dependent vegetation (GDV) areas from three or more years of ' \                           'Landsat or Sentinel NetCDF data. This functions results in ' \                           'a single raster layer of GDV likelihood with values ranging ' \                           'from 0 to 1, with 1 being highest probability of GDV.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """                # input netcdf data file        par_nc_file = arcpy.Parameter(                        displayName='Input satellite NetCDF file',                        name='in_nc_file',                        datatype='DEFile',                        parameterType='Required',                        direction='Input')        par_nc_file.filter.list = ['nc']                # output netcdf location        par_out_nc_file = arcpy.Parameter(                            displayName='Output GDV Likelihood NetCDF file',                            name='out_likelihood_nc_file',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_file.filter.list = ['nc']                # input wet month(s)         par_wet_months = arcpy.Parameter(                           displayName='Wet month(s)',                           name='in_wet_months',                           datatype='GPLong',                           parameterType='Required',                           direction='Input',                           category='Wet Period',                           multiValue=True)        par_wet_months.filter.type = 'ValueList'        par_wet_months.filter.list = [m for m in range(1, 13)]        par_wet_months.value = [1, 2, 3]                        # input dry month(s)        par_dry_months = arcpy.Parameter(                           displayName='Dry month(s)',                           name='in_dry_months',                           datatype='GPLong',                           parameterType='Required',                           direction='Input',                           category='Dry Period',                           multiValue=True)        par_dry_months.filter.type = 'ValueList'        par_dry_months.filter.list = [m for m in range(1, 13)]        par_dry_months.value = [9, 10, 11]                # input vegetation index         par_veg_idx = arcpy.Parameter(                        displayName='Vegetation index',                        name='in_veg_idx',                        datatype='GPString',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_veg_idx.filter.type = 'ValueList'        par_veg_idx.filter.list = [            'NDVI',            'EVI',             'SAVI',            'MSAVI',            'SLAVI',            'MAVI',            'kNDVI',            'TCG'            ]        par_veg_idx.value = 'MAVI'                # input moisture index         par_mst_idx = arcpy.Parameter(                        displayName='Moisture index',                        name='in_mst_idx',                        datatype='GPString',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_mst_idx.filter.type = 'ValueList'        par_mst_idx.filter.list = ['NDMI', 'GVMI']        par_mst_idx.value = 'NDMI'                # set pvalue for zscore        par_zscore_pvalue = arcpy.Parameter(                              displayName='Z-Score p-value',                              name='in_zscore_pvalue',                              datatype='GPDouble',                              parameterType='Optional',                              direction='Input',                              category='Outlier Correction',                              multiValue=False)        par_zscore_pvalue.filter.type = 'ValueList'        par_zscore_pvalue.filter.list = [0.01, 0.05, 0.1]        par_zscore_pvalue.value = None                       # set q upper for standardisation        par_ivt_qupper = arcpy.Parameter(                           displayName='Upper percentile',                           name='in_stand_qupper',                           datatype='GPDouble',                           parameterType='Required',                           direction='Input',                           category='Invariant Standardisation',                           multiValue=False)        par_ivt_qupper.filter.type = 'Range'        par_ivt_qupper.filter.list = [0.0, 1.0]        par_ivt_qupper.value = 0.99                         # set q lower for standardisation        par_ivt_qlower = arcpy.Parameter(                           displayName='Lower percentile',                           name='in_stand_qlower',                           datatype='GPDouble',                           parameterType='Required',                           direction='Input',                           category='Invariant Standardisation',                           multiValue=False)        par_ivt_qlower.filter.type = 'Range'        par_ivt_qlower.filter.list = [0.0, 1.0]        par_ivt_qlower.value = 0.05                                              # set oa class values        par_fmask_flags = arcpy.Parameter(displayName='Include pixels flags',                            name='in_fmask_flags',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Satellite Quality Options',                            multiValue=True)        flags = [            'NoData',             'Valid',             'Cloud',             'Shadow',             'Snow',             'Water'            ]        par_fmask_flags.filter.type = 'ValueList'                par_fmask_flags.filter.list = flags        par_fmask_flags.values = ['Valid', 'Snow', 'Water']                # max cloud cover        par_max_cloud = arcpy.Parameter(                          displayName='Maximum cloud cover',                          name='in_max_cloud',                          datatype='GPDouble',                          parameterType='Required',                          direction='Input',                          category='Satellite Quality Options',                          multiValue=False)        par_max_cloud.filter.type = 'Range'        par_max_cloud.filter.list = [0.0, 100.0]        par_max_cloud.value = 10.0                # input interpolate        par_interpolate = arcpy.Parameter(                            displayName='Interpolate NoData pixels',                            name='in_interpolate',                            datatype='GPBoolean',                            parameterType='Required',                            direction='Input',                            category='Satellite Quality Options',                            multiValue=False)        par_interpolate.value = True                # input add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True        # combine parameters        parameters = [            par_nc_file,            par_out_nc_file,            par_wet_months,             par_dry_months,             par_veg_idx,             par_mst_idx,             par_zscore_pvalue,            par_ivt_qupper,            par_ivt_qlower,            par_fmask_flags,            par_max_cloud,            par_interpolate,            par_add_result_to_map            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""                        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the GDV Spectra Likelihood module.        """        # safe imports        import os, sys        # arcgis comes with these        import datetime       # arcgis comes with these        import numpy as np    # arcgis comes with these        import pandas as pd   # arcgis comes with these        # risky imports (not native to arcgis)        try:            import xarray as xr            import dask        except:            arcpy.AddError('Python libraries xarray and dask not installed.')            return        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                    # module folder            sys.path.append(FOLDER_MODULES)            import gdvspectra, cog         except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return                    # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        warnings.simplefilter(action='ignore', category=dask.array.core.PerformanceWarning)                 # grab parameter values         in_nc = parameters[0].valueAsText            # raw input satellite netcdf        out_nc = parameters[1].valueAsText           # output gdv likelihood netcdf        in_wet_months = parameters[2].valueAsText    # wet months         in_dry_months = parameters[3].valueAsText    # dry months         in_veg_idx = parameters[4].value             # vege index name        in_mst_idx = parameters[5].value             # moisture index name               in_zscore_pvalue = parameters[6].valueAsText # zscore pvalue        in_ivt_qupper = parameters[7].value          # upper quantile for standardisation        in_ivt_qlower = parameters[8].value          # lower quantile for standardisation        in_fmask_flags = parameters[9].valueAsText   # fmask flag values        in_max_cloud = parameters[10].value          # max cloud percentage        in_interpolate = parameters[11].value        # interpolate missing pixels        in_add_result_to_map = parameters[12].value  # add result to map        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning GDVSpectra Likelihood.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=16)        # prepare wet, dry season lists        if in_wet_months == '':            arcpy.AddError('Must include at least 1 month in wet months.')            return        elif in_dry_months == '':            arcpy.AddError('Must include at least 1 month in dry months.')            return                # unpack months        wet_month = [int(e) for e in in_wet_months.split(';')]        dry_month = [int(e) for e in in_dry_months.split(';')]                # check if same months in wet and dry        for v in wet_month:            if v in dry_month:                arcpy.AddError('Cannot use same value for wet and dry months.')                return                # prepare zscore selection        if in_zscore_pvalue not in [None, '']:            in_zscore_pvalue = float(in_zscore_pvalue)                # convert fmask as text to numeric code equivalents              in_fmask_flags = [e for e in in_fmask_flags.split(';')]                in_fmask_flags = arc.convert_fmask_codes(in_fmask_flags)        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading and checking netcdf...')        arcpy.SetProgressorPosition(1)                # do quick lazy load of netcdf for checking        ds = xr.open_dataset(in_nc)                            # check xr type, vars, coords, dims, attrs        if not isinstance(ds, xr.Dataset):            arcpy.AddError('Input NetCDF must be a xr dataset.')            return        elif len(ds.data_vars) == 0:            arcpy.AddError('Input NetCDF has no data/variables/bands.')            return        elif 'x' not in list(ds.coords) or 'y' not in list(ds.coords) or 'time' not in list(ds.coords):            arcpy.AddError('Input NetCDF must have x, y and time coords.')            return        elif 'spatial_ref' not in list(ds.coords):            arcpy.AddError('Input NetCDF must have a spatial_ref coord.')            return        elif 'x' not in list(ds.dims) or 'y' not in list(ds.dims) or 'time' not in list(ds.dims):            arcpy.AddError('Input NetCDF must have x, y and time dimensions.')            return        elif len(ds.groupby('time.year')) < 3:            arcpy.AddError('Input NetCDF needs three or more years of data.')            return        elif ds.attrs == {}:            arcpy.AddError('NetCDF attributes not found. NetCDF must have attributes.')            return        elif not hasattr(ds, 'crs'):            arcpy.AddError('NetCDF CRS attribute not found. CRS required.')            return        elif ds.crs != 'EPSG:3577':            arcpy.AddError('NetCDF CRS is not EPSG:3577. EPSG:3577 required.')                        return         elif not hasattr(ds, 'nodatavals'):            arcpy.AddError('NetCDF nodatavals attribute not found.')                        return                     # check if xr is all nan/0 via centroid pixel timeseries (saves full load)        pixel = ds.isel(x=int(ds['x'].size / 2), y=int(ds['y'].size / 2)).to_array()        if pixel.isnull().all():            arcpy.AddError('NetCDF is completely null. Please download again.')                        return         elif xr.where(pixel == 0, True, False).all():            arcpy.AddError('NetCDF is completely full of 0s. Please download again.')                        return                     # now, do proper open of netcdf properly (and set nodata to nan)        ds = satfetcher.load_local_nc(nc_path=in_nc,                                       use_dask=True,                                       conform_nodata_to=np.nan)        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Getting NetCDF attributes and mask information...')        arcpy.SetProgressorPosition(2)                    # get attributes from dataset        ds_attrs = ds.attrs        ds_band_attrs = ds[list(ds.data_vars)[0]].attrs        ds_spatial_ref_attrs = ds['spatial_ref'].attrs                # check if mask band in netcdf         if 'oa_fmask' not in list(ds.data_vars):            if 'fmask' not in list(ds.data_vars):                arcpy.AddError('Cloud mask band not found in NetCDF. Please download again.')                return                # get name of mask band        mask_band = arc.get_name_of_mask_band(list(ds.data_vars))                        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Removing invalid pixels and empty dates...')        arcpy.SetProgressorPosition(3)          # remove invalid pixels and empty scenes        ds = cog.remove_fmask_dates(ds=ds,                                     valid_class=in_fmask_flags,                                     max_invalid=in_max_cloud,                                     mask_band=mask_band,                                     nodata_value=np.nan,                                     drop_fmask=True)                                            # get platform name from attributes, error if no attributes        in_platform = arc.get_platform_from_dea_attrs(ds_attrs)                        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Conforming satellite band names...')        arcpy.SetProgressorPosition(4)                        # conform dea aws band names based on platform        ds = satfetcher.conform_dea_ard_band_names(ds=ds,                                                    platform=in_platform.lower())                                                              # check if all expected bands are in dataset         expected_bands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']        for band in expected_bands:            if band not in list(ds.data_vars):                arcpy.AddError('NetCDF is missing band: {}. Need all bands.'.format(band))                return                        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Reducing dataset to just wet and dry months...')        arcpy.SetProgressorPosition(5)           # reduce xr dataset into only wet, dry months        ds = gdvspectra.subset_months(ds=ds,                                       month=wet_month + dry_month,                                      inplace=True)        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Calculating vegetation and moisture indices...')        arcpy.SetProgressorPosition(6)                 # check if veg idx supported         veg_idxs = ['ndvi', 'evi', 'savi', 'msavi', 'slavi', 'mavi', 'kndvi', 'tcg']        mst_idxs = ['ndmi', 'gvmi']        if in_veg_idx.lower() not in veg_idxs:            arcpy.AddError('Vegetation Index not supported.')            return         elif in_mst_idx.lower() not in mst_idxs:            arcpy.AddError('Moisture Index not supported.')            return         # calculate vegetation and moisture index        ds = tools.calculate_indices(ds=ds,                                      index=[in_veg_idx.lower(), in_mst_idx.lower()],                                      custom_name=['veg_idx', 'mst_idx'],                                      rescale=True,                                      drop=True)                                       # add band attrs back on        ds['veg_idx'].attrs = ds_band_attrs           ds['mst_idx'].attrs = ds_band_attrs                        # # # # #        # interpolate NoData pixels        if in_interpolate:                    # notify and increment progress bar            arcpy.SetProgressorLabel('Interpolating NoData pixels...')            arcpy.SetProgressorPosition(7)                          # interpolate            ds = ds.interpolate_na(dim='time', method='nearest')        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Resampling dataset to annual wet/dry medians...')        arcpy.SetProgressorPosition(7)         # resample data        ds = gdvspectra.resample_to_wet_dry_medians(ds=ds,                                                     wet_month=wet_month,                                                     dry_month=dry_month,                                                    inplace=True)            # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Persisting data into memory...')        arcpy.SetProgressorPosition(8)                # persist (dont compute yet)        ds = ds.persist()        # # # # #        # remove outliers pixels        if in_zscore_pvalue is not None:                    # notify and increment progress bar            arcpy.SetProgressorLabel('Removing outliers via Z-Score...')            arcpy.SetProgressorPosition(9)                      # remove outliers            ds = gdvspectra.nullify_wet_dry_outliers(ds=ds,                                                      wet_month=wet_month,                                                      dry_month=dry_month,                                                      p_value=in_zscore_pvalue,                                                     inplace=True)             # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Computing data into memory...')        arcpy.SetProgressorPosition(9)                # compute now        ds = ds.compute()                        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Cleaning up years with insufficient season dates...')        arcpy.SetProgressorPosition(10)         # remove any years missing wet, dry season         ds = gdvspectra.drop_incomplete_wet_dry_years(ds=ds)                # do another check for number of years         if len(ds.groupby('time.year')) < 3:            arcpy.AddError('Input NetCDF needs more years. Download NetCDF with more years.')            return        # fill any empty first, last years using back/forward fill        ds = gdvspectra.fill_empty_wet_dry_edges(ds=ds,                                                 wet_month=wet_month,                                                  dry_month=dry_month,                                                 inplace=True)                                                         # interpolate missing values         ds = gdvspectra.interp_empty_wet_dry(ds=ds,                                             wet_month=wet_month,                                             dry_month=dry_month,                                             method='full',                                             inplace=True)        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Standardising data to dry season invariant targets...')        arcpy.SetProgressorPosition(11)                                                                                                   # standardise data to invariant targets derived from dry times        ds = gdvspectra.standardise_to_dry_targets(ds=ds,                                                    dry_month=dry_month,                                                    q_upper=in_ivt_qupper,                                                   q_lower=in_ivt_qlower,                                                   inplace=True)        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Calculating seasonal similarity...')        arcpy.SetProgressorPosition(12)          # calculate seasonal similarity        ds_similarity = gdvspectra.calc_seasonal_similarity(ds=ds,                                                            wet_month=wet_month,                                                            dry_month=dry_month,                                                            q_mask=0.9,                                                            inplace=True)        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Calculating GDV Likelihood...')        arcpy.SetProgressorPosition(13)          # calculate gdv likelihood        ds = gdvspectra.calc_likelihood(ds=ds,                                         ds_similarity=ds_similarity,                                        wet_month=wet_month,                                         dry_month=dry_month)                                                # set likelihood variable to float32 type        ds['like'] = ds['like'].astype('float32')        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(14)                # append attrbutes on to dataset and bands        ds.attrs = ds_attrs        ds['spatial_ref'].attrs = ds_spatial_ref_attrs        for var in list(ds.data_vars):            ds[var].attrs = ds_band_attrs                                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(15)           # export netcdf file        tools.export_xr_as_nc(ds=ds, filename=out_nc)                        # # # # #        # add to map if requested        if in_add_result_to_map:                    # notify and increment progress bar            arcpy.SetProgressorLabel('Adding GDV Likelihood to map...')            arcpy.SetProgressorPosition(16)                        try:                # for current project, open current map                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap                            # create output folder using datetime as name                dt = datetime.datetime.now().strftime('%d%m%Y%H%M%S')                out_folder = os.path.join(os.path.dirname(out_nc), 'like' + '_' + dt)                os.makedirs(out_folder)                            # disable visualise on map temporarily                arcpy.env.addOutputsToMap = False                                # create crf filename and copy it                out_file = os.path.join(out_folder, 'likelihood.crf')                crf = arcpy.CopyRaster_management(in_raster=out_nc,                                                   out_rasterdataset=out_file)                                                    # add to map                                  m.addDataFromPath(crf)                                   # re-enable add to map                arcpy.env.addOutputsToMap = True                # apply cmap to crf layer                lyr = arc.apply_cmap(aprx=aprx,                                      lyr_name='likelihood.crf',                                      cmap_name='Bathymetric Scale',                                      cutoff_pct=0.01)            except:                arcpy.AddWarning('Could not visualise output. Aborting visualisation.')        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(16)                # close and del datasets        ds.close()        ds_similarity.close()        del ds, ds_similarity        # notify user        arcpy.AddMessage('Generated GDV Likelihood successfully.')        returnclass GDVSpectra_Threshold(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = 'GDVSpectra Threshold'        self.description = 'Threshold an existing GDV Likelihood cube using a ' \                           'shapefile of points or a standard deviation value. ' \                           'Requires a GDVSpectra Likelihood netcdf file as input.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """            # input netcdf data file        par_nc_file = arcpy.Parameter(                        displayName='Input GDV Likelihood NetCDF file',                        name='in_nc_file',                        datatype='DEFile',                        parameterType='Required',                        direction='Input')        par_nc_file.filter.list = ['nc']                # output netcdf location        par_out_nc_file = arcpy.Parameter(                            displayName='Output GDV Threshold NetCDF file',                            name='out_nc_file',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_file.filter.list = ['nc']                # use all likelihood dates        par_use_all_dates = arcpy.Parameter(                              displayName='Use median of all input Likelihood images',                              name='in_use_all_dates',                              datatype='GPBoolean',                              parameterType='Required',                              direction='Input',                              multiValue=False)        par_use_all_dates.value = True                # set specific year        par_specific_year = arcpy.Parameter(                              displayName='Set specific year to threshold',                              name='in_specific_year',                              datatype='GPLong',                              parameterType='Optional',                              direction='Input',                              multiValue=False)        par_specific_year.filter.type = 'ValueList'        par_specific_year.filter.list = []        #par_specific_year.value = None                # occurrence points shapefile        par_occurrence_feat = arcpy.Parameter(                                displayName='Occurrence points',                                name='in_occurrence_feat',                                datatype='GPFeatureLayer',                                parameterType='Optional',                                direction='Input')        par_occurrence_feat.filter.list = ['Point']                # field of presence/absence values        par_pa_column = arcpy.Parameter(                          displayName='Set field of presence and absence values',                          name='in_pa_column',                          datatype='GPString',                          parameterType='Optional',                          direction='Input',                          multiValue=False)        par_pa_column.filter.type = 'ValueList'        par_pa_column.filter.list = []                # standard dev        par_std_dev = arcpy.Parameter(                        displayName='Set standard deviation for threshold',                        name='in_std_dev',                        datatype='GPDouble',                        parameterType='Optional',                        direction='Input',                        multiValue=False)        par_std_dev.filter.type = 'Range'        par_std_dev.filter.list = [0.0, 10.0]        par_std_dev.value = 2.0                # nan handler        par_if_nodata = arcpy.Parameter(                          displayName='Ignore analysis when NoData',                          name='in_if_nodata',                          datatype='GPString',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_if_nodata.filter.type = 'ValueList'        par_if_nodata.filter.list = ['Any', 'All']        par_if_nodata.value = 'Any'                # remove stray pixels        par_remove_stray = arcpy.Parameter(                             displayName='Remove stray pixels',                             name='in_remove_stray',                             datatype='GPBoolean',                             parameterType='Required',                             direction='Input',                             multiValue=False)        par_remove_stray.value = True                # set use all dates checkbox        par_convert_binary = arcpy.Parameter(                               displayName='Convert output to binary values',                               name='in_convert_binary',                               datatype='GPBoolean',                               parameterType='Required',                               direction='Input',                               multiValue=False)        par_convert_binary.value = True                # add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True                # combine parameters        parameters = [            par_nc_file,            par_out_nc_file,            par_use_all_dates,            par_specific_year,            par_occurrence_feat,            par_pa_column,            par_std_dev,            par_if_nodata,            par_remove_stray,            par_convert_binary,            par_add_result_to_map            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""                return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # imports        try:            import numpy as np            import xarray as xr        except:            arcpy.AddError('Python library xarray not installed.')            raise                        # modify use all dates boolean control        if not parameters[2].value:                    # disable specific year control            parameters[3].enabled = True                            # try and get nc if exists            years = []            try:                # get path, open dataset, extract years                nc_path = parameters[0].valueastext                ds = xr.open_dataset(nc_path)                                # check if time dim/coord in ds, if not, return                 if 'time' in ds:                    years = np.unique(ds['time.year']).tolist()                else:                    years = []            except:                arcpy.AddError('Could not read times from input NetCDF.')                raise                    # update list of years            parameters[3].filter.list = years                        # set to first time if exists but only if blank            if len(years) > 0:                if parameters[3].value is None:                    parameters[3].value = years[0]                    else:            # disable specific year control            parameters[3].enabled = False                        # update list of years to empty            parameters[3].filter.list = []            parameters[3].value = None                        # modify occurrence shapefile control        if parameters[4].value:                    # enable occurrence column, disable std dev control            parameters[5].enabled = True            parameters[6].enabled = False                        # update shapefile column name list            try:                # get shapefile filepath and get column names                shp_path = parameters[4].valueAsText                col_names = [f.name for f in arcpy.ListFields(shp_path)]            except:                arcpy.AddError('Could not read fields from occurrence shapefile.')                raise                        # update values in control            parameters[5].filter.list = col_names                    else:            # disable occurrence column, enable std dev control            parameters[5].enabled = False            parameters[6].enabled = True                        # update values in control            parameters[5].filter.list = []            return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""                return    def execute(self, parameters, messages):               """        Executes the GDV Spectra Threshold module.        """                # safe imports        import os, sys                           # arcgis comes with these        import datetime                          # arcgis comes with this        import numpy as np                       # arcgis comes with this        import pandas as pd                      # arcgis comes with this        import arcpy                             # arcgis comes with this                # risky imports (not native to arcgis)        try:            import xarray as xr            import dask        except:            arcpy.AddError('Python libraries xarray and dask not installed.')            return                        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                    # module folder            sys.path.append(FOLDER_MODULES)            import gdvspectra         except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return                    # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        warnings.simplefilter(action='ignore', category=dask.array.core.PerformanceWarning)                                                    # grab parameter values         in_nc = parameters[0].valueAsText                # likelihood netcdf        out_nc = parameters[1].valueAsText               # output netcdf        in_use_all_dates = parameters[2].value           # use all dates in nc         in_specific_year = parameters[3].value           # set specific year         in_occurrence_feat = parameters[4]               # occurrence shp path         in_pa_column = parameters[5].value               # occurrence shp pres/abse col         in_std_dev = parameters[6].value                 # std dev threshold value         in_if_nodata = parameters[7].value               # ignore analysis if no data any or all         in_remove_stray = parameters[8].value            # apply salt n pepper -- requires sa        in_convert_binary = parameters[9].value          # convert thresh to binary 1, nan        in_add_result_to_map = parameters[10].value      # add result to map                # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning GDVSpectra Threshold.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=10)                                                                # check median all inputs and specific year         if in_use_all_dates is None:            arcpy.AddError('Must provide True or False for use all dates input.')            return        elif in_use_all_dates == False and in_specific_year is None:            arcpy.AddError('Must provide specifc year if median of all images not used.')            return                                    # prepare occurrence shapefile        df_records = None        if in_occurrence_feat.value is not None and in_pa_column is not None:            try:                # get shapefile metadata and do checks                shp_desc = arcpy.Describe(in_occurrence_feat)                in_occurrence_feat = os.path.join(shp_desc.path, shp_desc.name)                                # check if crs is albers (3577)                if shp_desc.spatialReference.factoryCode != 3577:                    arcpy.AddError('Occurrence points are not in GDA94 Albers. Please project.')                    return                                # check if presence/absence column is integer                for field in shp_desc.fields:                    if field.name == in_pa_column and field.type != 'Integer':                        arcpy.AddError('Presence/Absence column must be Integer type of 1s and 0s.')                        return                                        # check if any non 1s and 0s in pres/abse column                with arcpy.da.SearchCursor(in_occurrence_feat, [in_pa_column]) as cursor:                    vals = np.unique([row[0] for row in cursor])                    if len(vals) != 2 or (0 not in vals or 1 not in vals):                        arcpy.AddError('Presence/absence column does not contain just 1s and 0s.')                        return                                        # check if empty shapefile                 if int(arcpy.GetCount_management(in_occurrence_feat)[0]) == 0:                    arcpy.AddError('Occurrence points shapefile is empty.')                    return                # read shapefile as pandas dataframe                 df_records = tools.read_shapefile(shp_path=in_occurrence_feat)                                # subset to just x, y, pres/abse column                df_records = tools.subset_records(df_records=df_records,                                                   p_a_column=in_pa_column)            except:                arcpy.AddWarning('Could not read occurrence shapefile. Using standard deviation: 2.')                df_records, in_std_dev = None, 2                        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading and checking netcdf...')        arcpy.SetProgressorPosition(1)                # do quick lazy load of netcdf for checking        ds = xr.open_dataset(in_nc)                # check xr type, vars, coords, dims, attrs        if not isinstance(ds, xr.Dataset):            arcpy.AddError('Input NetCDF must be a xr dataset.')            raise        elif len(ds.data_vars) == 0:            arcpy.AddError('Input NetCDF has no data/variables/bands.')            raise        elif 'like' not in list(ds.data_vars):            arcpy.AddError('Input NetCDF missing likelihood variable. Run likelihood tool first.')            raise        elif 'x' not in list(ds.coords) or 'y' not in list(ds.coords) or 'time' not in list(ds.coords):            arcpy.AddError('Input NetCDF must have x, y and time coords.')            raise        elif 'spatial_ref' not in list(ds.coords):            arcpy.AddError('Input NetCDF must have a spatial_ref coord.')            raise        elif 'x' not in list(ds.dims) or 'y' not in list(ds.dims) or 'time' not in list(ds.dims):            arcpy.AddError('Input NetCDF must have x, y and time dimensions.')            raise        elif ds.attrs == {}:            arcpy.AddError('NetCDF attributes not found. NetCDF must have attributes.')            raise        elif not hasattr(ds, 'crs'):            arcpy.AddError('NetCDF CRS attribute not found. CRS required.')            raise        elif ds.crs != 'EPSG:3577':            arcpy.AddError('NetCDF CRS is not EPSG:3577. EPSG:3577 required.')                        raise         elif not hasattr(ds, 'nodatavals'):            arcpy.AddError('NetCDF nodatavals attribute not found.')                        raise         # check if xr is all nan/0 via centroid pixel timeseries (saves full load)        pixel = ds.isel(x=int(ds['x'].size / 2), y=int(ds['y'].size / 2)).to_array()        if pixel.isnull().all():            arcpy.AddError('NetCDF is completely null. Please download again.')                        raise         elif xr.where(pixel == 0, True, False).all():            arcpy.AddWarning('NetCDF is completely full of 0s. Output could be problematic.')                                # check if selected year in dataset, if selected         if in_use_all_dates == False:            if in_specific_year not in list(ds['time.year']):                arcpy.AddError('Specified year not found in NetCDF.')                raise                    # load raw netcdf (set nodata to nan)        ds = satfetcher.load_local_nc(nc_path=in_nc,                                       use_dask=True,                                       conform_nodata_to=np.nan)        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Getting NetCDF attributes and mask information...')        arcpy.SetProgressorPosition(2)                    # get attributes from dataset        ds_attrs = ds.attrs        ds_band_attrs = ds[list(ds.data_vars)[0]].attrs        ds_spatial_ref_attrs = ds['spatial_ref'].attrs        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Subsetting dataset based on time...')        arcpy.SetProgressorPosition(3)        # aggregate to median all time or get specific year        if in_use_all_dates:            arcpy.AddMessage('Thresholding median of all time.')            ds = ds.median('time', keep_attrs=True)        elif in_specific_year is not None:            arcpy.AddMessage('Thresholding specific year: {}.'.format(in_specific_year))            ds = ds.where(ds['time.year'] == in_specific_year, drop=True)        else:            arcpy.AddWarning('No aggregate all time or year provided - using all time.')            ds = ds.median('time', keep_attrs=True)                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Thresholding GDV Likelihood...')        arcpy.SetProgressorPosition(4)                # perform thresholding using either shapefile points or std dev        if df_records is not None:            ds_thresh = gdvspectra.threshold_likelihood(ds=ds,                                                        df=df_records,                                                         res_factor=3,                                                         if_nodata=in_if_nodata.lower())        else:            ds_thresh = gdvspectra.threshold_likelihood(ds=ds,                                                        num_stdevs=in_std_dev,                                                         res_factor=3,                                                         if_nodata=in_if_nodata.lower())        # # # # #        # remove stray pixels if requested        if in_remove_stray:                    # notify and increment progress bar            arcpy.SetProgressorLabel('Removing stray salt and pepper pixels...')            arcpy.SetProgressorPosition(5)                        # convert mask to 1, 0            ds_mask = xr.where(~ds_thresh.isnull(), 1, 0).astype('int8')                        # create moving 3 x 3 window, apply mask, clean smoothing            ds_mask_despeck = ds_mask.rolling(x=3, y=3, center=True).sum()            ds_mask_despeck = xr.where(ds_mask_despeck >= 4, 1, 0)            ds_mask = ds_mask_despeck.where(ds_mask == 1, 0)                        # apply mask to threshold dataset            ds_thresh = ds_thresh.where(ds_mask == 1, np.nan)                    # # # # #        # binarise threshold values        if in_convert_binary:                        # notify and increment progress bar            arcpy.SetProgressorLabel('Removing stray salt and pepper pixels...')            arcpy.SetProgressorPosition(6)                        # set all threshold non-nan values to 1            ds_thresh = ds_thresh.where(ds_thresh.isnull(), 1)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(7)                # append attrbutes on to dataset and bands        ds_thresh.attrs = ds_attrs        ds_thresh['spatial_ref'].attrs = ds_spatial_ref_attrs        for var in list(ds_thresh.data_vars):            ds_thresh[var].attrs = ds_band_attrs        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(8)           # export netcdf file        tools.export_xr_as_nc(ds=ds_thresh, filename=out_nc)        # # # # #        # add to map if requested        if in_add_result_to_map:                    # notify and increment progess bar            arcpy.SetProgressorLabel('Adding GDV Threshold to map...')            arcpy.SetProgressorPosition(9)                        try:                # for current project, open current map                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap                                # create output folder using datetime as name                dt = datetime.datetime.now().strftime('%d%m%Y%H%M%S')                out_folder = os.path.join(os.path.dirname(out_nc), 'thresh' + '_' + dt)                os.makedirs(out_folder)                            # disable visualise on map temporarily                arcpy.env.addOutputsToMap = False                                # create crf filename and copy it                out_file = os.path.join(out_folder, 'thresh.crf')                crf = arcpy.CopyRaster_management(in_raster=out_nc,                                                   out_rasterdataset=out_file)                                                    # add to map                                  m.addDataFromPath(crf)                                  # re-enable add to map                arcpy.env.addOutputsToMap = True                                # determine cmap and transparency depending on binary or not                cmaps = ['Red-Blue (Continuous)', 'Spectrum By Wavelength-Full Bright']                cmap = cmaps[0] if in_convert_binary else cmaps[1]                transparency = 25 if in_convert_binary else 0                # apply cmap                lyr = arc.apply_cmap(aprx=aprx,                                      lyr_name='thresh.crf',                                      cmap_name=cmap,                                      cutoff_pct=0.01)                                                      # apply transperancy                lyr.transparency = transparency            except:                arcpy.AddWarning('Could not visualise output. Aborting visualisation.')        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(10)                # close and del datasets        ds.close()        ds_thresh.close()        del ds, ds_thresh                # close mask if used        if in_remove_stray:            ds_mask.close()            del ds_mask        # notify user        arcpy.AddMessage('Generated GDV Likelihood successfully.')                          returnclass GDVSpectra_Trend(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = 'GDVSpectra Trend'        self.description = 'Perform a time-series trend analysis on an existing ' \                           'GDV Likelihood data cube. Produces a map of areas where ' \                           'vegetation has continuously increased, decreased or ' \                           'has not changed.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """                # input like netcdf file        par_like_nc_file = arcpy.Parameter(                             displayName='Input GDV Likelihood NetCDF file',                             name='in_like_nc_file',                             datatype='DEFile',                             parameterType='Required',                             direction='Input')        par_like_nc_file.filter.list = ['nc']                # input mask netcdf file        par_mask_nc_file = arcpy.Parameter(                             displayName='Input GDV Threshold mask NetCDF file',                             name='in_mask_nc_file',                             datatype='DEFile',                             parameterType='Optional',                             direction='Input')        par_mask_nc_file.filter.list = ['nc']                # output netcdf location        par_out_nc_file = arcpy.Parameter(                                  displayName='Output NetCDF file',                                  name='out_nc_file',                                  datatype='DEFile',                                  parameterType='Required',                                  direction='Output')        par_out_nc_file.filter.list = ['nc']                # date of analysis from        par_date_from = arcpy.Parameter(                          displayName='Date from',                          name='in_from_date',                          datatype='GPDate',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_date_from.values = '2015/01/01'                # date of analysis to        par_date_to = arcpy.Parameter(                        displayName='Date to',                        name='in_to_date',                        datatype='GPDate',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_date_to.values = '2020/12/31'                # set analysis type        par_analysis_type = arcpy.Parameter(                              displayName='Trend analysis method',                              name='in_analysis_type',                              datatype='GPString',                              parameterType='Required',                              direction='Input',                              multiValue=False)        par_analysis_type.filter.type = 'ValueList'        par_analysis_type.filter.list = ['Mann-Kendall', 'Theilsen Slope']        par_analysis_type.value = 'Mann-Kendall'                # mk p-value        par_mk_pvalue = arcpy.Parameter(                          displayName='Mann-Kendall p-value',                          name='in_mk_pvalue',                          datatype='GPDouble',                          parameterType='Optional',                          direction='Input',                          multiValue=False)        par_mk_pvalue.filter.type = 'Range'        par_mk_pvalue.filter.list = [0.001, 0.5]        par_mk_pvalue.value = 0.05        # mk direction        par_mk_direction = arcpy.Parameter(                            displayName='Mann-Kendall trend direction',                            name='in_mk_direction',                            datatype='GPString',                            parameterType='Optional',                            direction='Input',                            multiValue=False)        par_mk_direction.filter.type = 'ValueList'        par_mk_direction.filter.list = ['Both', 'Increasing', 'Decreasing']        par_mk_direction.value = 'Both'                # ts alpha        par_ts_alpha = arcpy.Parameter(                         displayName='Theil-Sen alpha',                         name='in_ts_alpha',                         datatype='GPDouble',                         parameterType='Optional',                         direction='Input',                         multiValue=False)        par_ts_alpha.filter.type = 'Range'        par_ts_alpha.filter.list = [0, 1]        par_ts_alpha.value = 0.95                # add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True                # combine parameters        parameters = [            par_like_nc_file,            par_mask_nc_file,            par_out_nc_file,            par_date_from,            par_date_to,            par_analysis_type,            par_mk_pvalue,            par_mk_direction,            par_ts_alpha,            par_add_result_to_map        ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # imports        try:            import datetime            import numpy as np            import xarray as xr        except:            arcpy.AddError('Python library xarray not installed.')            raise                    # modify date to and from controls when netcdf set        if parameters[0].altered and not parameters[0].hasBeenValidated:                    # try and get nc if exists            try:                # get path, open dataset, get array of datetimes                nc_path = parameters[0].valueAsText                ds = xr.open_dataset(nc_path)                dts = ds['time']                                # extract first and last date in array                s_date = dts.isel(time=0).dt.strftime('%Y-%m-%d').values                e_date = dts.isel(time=-1).dt.strftime('%Y-%m-%d').values                            except:                arcpy.AddError('Could not extract date from input NetCDF.')                raise                        # update start and end dates             parameters[3].value = str(s_date)            parameters[4].value = str(e_date)                        # modify mk/ts on change        if parameters[5].valueAsText == 'Mann-Kendall':                    # enable mk pvalue and direction, disable ts alpha            parameters[6].enabled = True            parameters[7].enabled = True            parameters[8].enabled = False        else:            # disable mk pvalue and direction, enable ts alpha            parameters[6].enabled = False            parameters[7].enabled = False            parameters[8].enabled = True        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the GDV Spectra Trend module.        """                # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)                # safe imports        import os, sys                           # arcgis comes with these        import datetime                          # arcgis comes with this        import numpy as np                       # arcgis comes with this        import scipy                             # arcgis comes with this        import arcpy                             # arcgis comes with this        from tempfile import NamedTemporaryFile  # arcgis comes with this                # risky imports (not native to arcgis)        try:            import xarray as xr            import dask        except:            arcpy.AddError('Python libraries xarray and dask not installed.')            raise                        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                    # module folder            sys.path.append(FOLDER_MODULES)            import gdvspectra         except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            raise                    # grab parameter values         in_like_nc = parameters[0].valueAsText         # likelihood netcdf        in_mask_nc = parameters[1].valueAsText         # thresh mask netcdf        out_nc = parameters[2].valueAsText             # output netcdf        in_from_date = parameters[3].value             # from date        in_to_date = parameters[4].value               # to date        in_trend_method = parameters[5].value          # trend method        in_mk_pvalue = parameters[6].value             # mk pvalue        in_mk_direction = parameters[7].value          # mk direction        in_ts_alpha = parameters[8].value              # ts alpha        in_add_result_to_map = parameters[9].value     # add result to map                        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning GDVSpectra Trend.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=6)        # prepare start and end date times as strings        in_from_date = arc.datetime_to_string(in_from_date)        in_to_date = arc.datetime_to_string(in_to_date)                # prepare mk direction label        if in_trend_method == 'Mann-Kendall':            if in_mk_direction == 'Both':                mk_dir = 'both'            elif in_mk_direction == 'Increasing':                mk_dir = 'inc'            else:                mk_dir = 'dec'                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading and checking GDV Likelihood NetCDF...')        arcpy.SetProgressorPosition(1)                # load raw netcdf (set nodata to nan)        ds_like = satfetcher.load_local_nc(nc_path=in_like_nc,                                            use_dask=True,                                            conform_nodata_to=np.nan)        # check if xr is datatse, has data get attributes for ds and band        if not isinstance(ds_like, xr.Dataset):            arcpy.AddError('Input NetCDF must be a xr dataset.')            raise        elif 'time' not in list(ds_like.dims):            arcpy.AddError('Input NetCDF must have a time dimension.')            raise        elif len(ds_like.data_vars) == 0:            arcpy.AddError('Input NetCDF has no data/variables/bands.')            raise        elif 'like' not in list(ds_like.data_vars):            arcpy.AddError('Input NetCDF missing likelihood variable.')            raise        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Getting NetCDF attributes information...')        arcpy.SetProgressorPosition(2)                    # get attributes from dataset        ds_attrs = ds_like.attrs        ds_band_attrs = ds_like[list(ds_like.data_vars)[0]].attrs        ds_spatial_ref_attrs = ds_like['spatial_ref'].attrs                             # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Subsetting dataset based on time...')        arcpy.SetProgressorPosition(3)                # convert datetimes to numpy        np_s_dt = np.datetime64(in_from_date)        np_e_dt = np.datetime64(in_to_date)                # subset based on numpy datetimes        ds_like = ds_like.where((ds_like['time'] >= np_s_dt) &                                 (ds_like['time'] <= np_e_dt),                                 drop=True)                                        # check number of time indexes >= 3, else error        if len(ds_like['time']) < 3:            arcpy.AddError('Trend analysis requires three or more images/times.')            raise                # # # # #        # apply threshold mask if requested        if in_mask_nc is not None:            # notify and increment progress bar            arcpy.SetProgressorLabel('Loading and checking GDV Threshold mask NetCDF...')            arcpy.SetProgressorPosition(4)                    try:                # load mask netcdf (set nodata to nan)                ds_mask = satfetcher.load_local_nc(nc_path=in_mask_nc,                                                    use_dask=True,                                                    conform_nodata_to=np.nan)                # check if mask dataset has one variable / mask                if len(ds_mask.data_vars) != 1:                    arcpy.AddError('Input mask NetCDF can have only one variable/band.')                    raise                elif 'like' not in list(ds_mask.data_vars):                    arcpy.AddError('Input mask NetCDF missing likelihood variable.')                    raise                elif isinstance(ds_mask, xr.Dataset):                    ds_mask = ds_mask.to_array()                                    # mask out likelihood dataset                ds_like = ds_like.where(~ds_mask.isnull())                                # squeeze                ds_like = ds_like.squeeze(drop=True)                            except:                arcpy.AddWarning('Could not mask likelihood NetCDF. Aborting masking procedure.')                        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Performing trend analysis, this can take awhile...')        arcpy.SetProgressorPosition(4)        # perform trend analysis based on requested type        if in_trend_method == 'Mann-Kendall':            ds_trend = gdvspectra.perform_mk_original(ds=ds_like.compute(),                                                       pvalue=in_mk_pvalue,                                                       direction=mk_dir)        elif in_trend_method == 'Theilsen Slope':            ds_trend = gdvspectra.perform_theilsen_slope(ds=ds_like.compute(),                                                          alpha=in_ts_alpha)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(5)                # append attrbutes on to dataset and bands        ds_trend.attrs = ds_attrs        ds_trend['spatial_ref'].attrs = ds_spatial_ref_attrs        for var in list(ds_trend.data_vars):            ds_trend[var].attrs = ds_band_attrs                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(6)           # export netcdf file        tools.export_xr_as_nc(ds=ds_trend, filename=out_nc)                       # # # # #        # add to map if requested        if in_add_result_to_map:                        # notify and increment progess bar            arcpy.SetProgressorLabel('Adding GDV Trend to map...')            arcpy.SetProgressorPosition(7)                        try:                # for current project, open current map                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap                # create output folder using datetime as name                dt = datetime.datetime.now().strftime('%d%m%Y%H%M%S')                out_folder = os.path.join(os.path.dirname(out_nc), 'trend' + '_' + dt)                os.makedirs(out_folder)                                # disable visualise on map temporarily                arcpy.env.addOutputsToMap = False                                # create crf filename and copy it                out_file = os.path.join(out_folder, 'trend.crf')                crf = arcpy.CopyRaster_management(in_raster=out_nc,                                                   out_rasterdataset=out_file)                                                                  # add to map                                  m.addDataFromPath(crf)                                 # re-enable add to map                arcpy.env.addOutputsToMap = True                                # tailor cmap depending on method and parameters                if in_trend_method == 'Mann-Kendall':                    if mk_dir == 'both':                        cmap = 'Red-Blue (Continuous)'                    elif mk_dir == 'inc':                        cmap = 'Yellow-Green-Blue (Continuous)'                    else:                        cmap = 'Yellow-Orange-Red (Continuous)'                   else:                    cmap = 'Red-Blue (Continuous)'                                # apply cmap                lyr = arc.apply_cmap(aprx=aprx,                                      lyr_name='trend.crf',                                      cmap_name=cmap,                                      cutoff_pct=0.0001)                                                      # invert if mk dec                if in_trend_method == 'Mann-Kendall' and mk_dir == 'dec':                    lyr.invertColorRamp = True                                except:                arcpy.AddWarning('Could not visualise output. Aborting visualisation.')                                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(10)                # close likelihood ds        ds_like.close()        del ds_like                # close mask if exists         if in_mask_nc is not None:            ds_mask.close()            del ds_mask        # notify user        arcpy.AddMessage('Generated GDV Trend successfully.')                          returnclass GDVSpectra_CVA(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = 'GDVSpectra CVA'        self.description = 'Perform a Change Vector Analysis (CVA) on a data cube.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """                # input netcdf file        par_raw_nc_path = arcpy.Parameter(                            displayName='Input satellite NetCDF file',                            name='in_raw_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Input')        par_raw_nc_path.filter.list = ['nc']                # input netcdf mask (thresh) file        par_mask_nc_path = arcpy.Parameter(                             displayName='Input GDV Threshold mask NetCDF file',                             name='in_mask_nc_path',                             datatype='DEFile',                             parameterType='Optional',                             direction='Input')        par_mask_nc_path.filter.list = ['nc']                # output folder location        par_out_nc_path = arcpy.Parameter(                            displayName='Output CVA NetCDF file',                            name='out_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_path.filter.list = ['nc']                # base start year        par_base_start_year = arcpy.Parameter(                                displayName='Base start year',                                name='in_base_start_year',                                datatype='GPLong',                                parameterType='Required',                                direction='Input',                                multiValue=False)        par_base_start_year.filter.type = 'ValueList'        par_base_start_year.filter.list = []                # base end year        par_base_end_year = arcpy.Parameter(                              displayName='Base end year',                              name='in_base_end_year',                              datatype='GPLong',                              parameterType='Required',                              direction='Input',                              multiValue=False)        par_base_end_year.filter.type = 'ValueList'        par_base_end_year.filter.list = []        # comparison start year        par_comp_start_year = arcpy.Parameter(                                displayName='Comparison start year',                                name='in_comp_start_year',                                datatype='GPLong',                                parameterType='Required',                                direction='Input',                                multiValue=False)        par_comp_start_year.filter.type = 'ValueList'        par_comp_start_year.filter.list = []                # comparison end year        par_comp_end_year = arcpy.Parameter(                              displayName='Comparison end year',                              name='in_comp_end_year',                              datatype='GPLong',                              parameterType='Required',                              direction='Input',                              multiValue=False)        par_comp_end_year.filter.type = 'ValueList'        par_comp_end_year.filter.list = []        # analysis months        par_analysis_months = arcpy.Parameter(                                displayName='Set analysis month(s)',                                name='in_analysis_months',                                datatype='GPLong',                                parameterType='Required',                                direction='Input',                                multiValue=True)        par_analysis_months.filter.type = 'ValueList'        par_analysis_months.filter.list = [m for m in range(1, 13)]        par_analysis_months.value = [9, 10, 11]        # cva magnitude threshold         par_tmf = arcpy.Parameter(                    displayName='Magnitude threshold',                    name='in_tmf',                    datatype='GPDouble',                    parameterType='Required',                    direction='Input',                    category='CVA Options',                    multiValue=False)        par_tmf.filter.type = 'Range'        par_tmf.filter.list = [0.0, 100.0]          par_tmf.value = 2.0          # set q upper for standardisation        par_ivt_qupper = arcpy.Parameter(                           displayName='Upper percentile',                           name='in_stand_qupper',                           datatype='GPDouble',                           parameterType='Required',                           direction='Input',                           category='Invariant Standardisation',                           multiValue=False)        par_ivt_qupper.filter.type = 'Range'        par_ivt_qupper.filter.list = [0.0, 1.0]        par_ivt_qupper.value = 0.99                         # set q lower for standardisation        par_ivt_qlower = arcpy.Parameter(                           displayName='Lower percentile',                           name='in_stand_qlower',                           datatype='GPDouble',                           parameterType='Required',                           direction='Input',                           category='Invariant Standardisation',                           multiValue=False)        par_ivt_qlower.filter.type = 'Range'        par_ivt_qlower.filter.list = [0.0, 1.0]        par_ivt_qlower.value = 0.05                     # set oa class values        par_fmask_flags = arcpy.Parameter(displayName='Include pixel flags',                            name='in_fmask_flags',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Satellite Quality Options',                            multiValue=True)        flags = [            'NoData',             'Valid',             'Cloud',             'Shadow',             'Snow',             'Water'            ]        par_fmask_flags.filter.type = 'ValueList'                par_fmask_flags.filter.list = flags        par_fmask_flags.values = ['Valid', 'Snow', 'Water']                # max cloud cover        par_max_cloud = arcpy.Parameter(                          displayName='Maximum cloud cover',                          name='in_max_cloud',                          datatype='GPDouble',                          parameterType='Required',                          direction='Input',                          category='Satellite Quality Options',                          multiValue=False)        par_max_cloud.filter.type = 'Range'        par_max_cloud.filter.list = [0.0, 100.0]        par_max_cloud.value = 10.0                # interpolate        par_interpolate = arcpy.Parameter(                            displayName='Interpolate NoData pixels',                            name='in_interpolate',                            datatype='GPBoolean',                            parameterType='Required',                            direction='Input',                            category='Satellite Quality Options',                            multiValue=False)        par_interpolate.value = True                # add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True                # combine parameters        parameters = [            par_raw_nc_path,            par_mask_nc_path,            par_out_nc_path,            par_base_start_year,            par_base_end_year,            par_comp_start_year,            par_comp_end_year,            par_analysis_months,            par_tmf,            par_ivt_qupper,            par_ivt_qlower,            par_fmask_flags,            par_max_cloud,            par_interpolate,            par_add_result_to_map            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # imports        try:            import numpy as np            import xarray as xr        except:            arcpy.AddError('Python library xarray not installed.')            raise                        # modify use all dates boolean control        if parameters[0].altered and not parameters[0].hasBeenValidated:                                # try and get nc if exists            years = []            try:                # get path, open dataset, extract years                nc_path = parameters[0].valueAsText                ds = xr.open_dataset(nc_path)                years = np.unique(ds['time.year']).tolist()            except:                arcpy.AddError('Could not read times from input NetCDF.')                raise                        # populate base and comp start and end years            parameters[3].filter.list = years             parameters[4].filter.list = years            parameters[5].filter.list = years            parameters[6].filter.list = years                        # set initial value for base start and end            parameters[3].value = years[0]            parameters[4].value = years[0]                        # set initial value for comparison start and end            parameters[5].value = years[-1]            parameters[6].value = years[-1]                    return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the GDV Spectra CVA module.        """                # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)                # safe imports        import os, sys        # arcgis comes with these        import datetime       # arcgis comes with these        import numpy as np    # arcgis comes with these        import pandas as pd   # arcgis comes with these                # risky imports (not native to arcgis)        try:            import xarray as xr            import dask        except:            arcpy.AddError('Python libraries xarray and dask not installed.')            raise                        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                    # module folder            sys.path.append(FOLDER_MODULES)            import gdvspectra, cog         except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            raise                                                    # grab parameter values         in_raw_nc = parameters[0].valueAsText               # raw input satellite netcdf        in_mask_nc = parameters[1].valueAsText              # mask input satellite netcdf        out_nc = parameters[2].valueAsText                  # output gdv likelihood netcdf        in_base_start_year =  parameters[3].value           # base start year        in_base_end_year =  parameters[4].value             # base end year        in_comp_start_year =  parameters[5].value           # comp start year        in_comp_end_year =  parameters[6].value             # comp end year        in_analysis_months = parameters[7].valueAsText      # analysis months        in_tmf = parameters[8].value                        # magnitude threshold        in_ivt_qupper = parameters[9].value                 # upper quantile for standardisation        in_ivt_qlower = parameters[10].value                # lower quantile for standardisation        in_fmask_flags = parameters[11].valueAsText         # fmask flag values        in_max_cloud = parameters[12].value                 # max cloud percentage        in_interpolate = parameters[13].value               # interpolate missing pixels        in_add_result_to_map = parameters[14].value         # add result to map        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning GDVSpectra CVA.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=14)                                    # check if base end year >= start year        if in_base_end_year < in_base_start_year:            arcpy.AddWarning('Base end year is earlier than end, setting end to start.')            in_base_end_year = in_base_start_year                # ... do same for comparison        if in_comp_end_year < in_comp_start_year:            arcpy.AddWarning('Comparison end year is earlier than end, setting end to start.')            in_comp_end_year = in_comp_start_year                    # ... now, compare base to comparison        if in_comp_start_year < in_base_start_year:            arcpy.AddError('Comparison start year is earlier than base start year.')            raise                # prepare analysis month lists        in_analysis_months = [int(e) for e in in_analysis_months.split(';')]        # convert fmask as text to numeric code equivalents              in_fmask_flags = [e for e in in_fmask_flags.split(';')]                in_fmask_flags = arc.convert_fmask_codes(in_fmask_flags)        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading and checking netcdf...')        arcpy.SetProgressorPosition(1)                # load raw netcdf (set nodata to nan)        ds = satfetcher.load_local_nc(nc_path=in_raw_nc,                                       use_dask=True,                                       conform_nodata_to=np.nan)        # check if xr is datatset, has data get attributes for ds and band        if not isinstance(ds, xr.Dataset):            arcpy.AddError('Input NetCDF must be a xr dataset.')            raise        elif len(ds.data_vars) == 0:            arcpy.AddError('Input NetCDF has no data/variables/bands.')            raise                                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Getting NetCDF attributes and mask information...')        arcpy.SetProgressorPosition(2)        # get attributes from dataset        ds_attrs = ds.attrs        ds_band_attrs = ds[list(ds.data_vars)[0]].attrs        ds_spatial_ref_attrs = ds['spatial_ref'].attrs                # check if mask band exists        mask_band = arc.get_name_of_mask_band(list(ds.data_vars))        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Removing invalid pixels and empty dates...')        arcpy.SetProgressorPosition(3)          # remove invalid pixels and empty scenes        ds = cog.remove_fmask_dates(ds=ds,                                     valid_class=in_fmask_flags,                                     max_invalid=in_max_cloud,                                     mask_band=mask_band,                                     nodata_value=np.nan,                                     drop_fmask=True)                                            # get platform name from attributes, error if no attributes        in_platform = arc.get_platform_from_dea_attrs(ds_attrs)        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Conforming satellite band names...')        arcpy.SetProgressorPosition(4)                        # conform dea aws band names based on platform        ds = satfetcher.conform_dea_ard_band_names(ds=ds,                                                    platform=in_platform.lower())                   # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Reducing dataset down to analysis months...')        arcpy.SetProgressorPosition(5)        ds = gdvspectra.subset_months(ds=ds,                                       month=in_analysis_months,                                      inplace=True)                                                                                    # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Calculating tasselled cap index...')        arcpy.SetProgressorPosition(6)         # calculate vegetation and moisture index        ds = tools.calculate_indices(ds=ds,                                      index=['tcg', 'tcb'],                                      rescale=False,                                      drop=True)                                             # add band attributes back on        for var in list(ds.data_vars):            ds[var].attrs = ds_band_attrs                    # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Reducing selected months into annual medians...')        arcpy.SetProgressorPosition(7)                # reduce all selected months into annual medians (year starts, YS)        ds = gdvspectra.resample_to_freq_medians(ds=ds,                                                 freq='YS',                                                 inplace=True)                                                         # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Computing data into memory...')        arcpy.SetProgressorPosition(7)                # persist (dont compute yet)        ds = ds.compute()        # # # # #        # interpolate if requested        if in_interpolate:                    # notify and increment progress bar            arcpy.SetProgressorLabel('Interpolating NoData pixels...')            arcpy.SetProgressorPosition(8)                      # interpolate all missing pixels using full linear interpolation            ds = gdvspectra.interp_empty(ds=ds,                                         method='full',                                         inplace=True)        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Standardising to invariant targets...')        arcpy.SetProgressorPosition(8)        # standardise to targets        ds = gdvspectra.standardise_to_targets(ds,                                                q_upper=in_ivt_qupper,                                                q_lower=in_ivt_qlower)        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Performing CVA...')        arcpy.SetProgressorPosition(9)        # generate cva        ds_cva = gdvspectra.perform_cva(ds=ds,                                        base_times=(in_base_start_year, in_base_end_year),                                        comp_times=(in_comp_start_year, in_comp_end_year),                                        reduce_comp=False,                                        vege_var = 'tcg',                                        soil_var = 'tcb',                                        tmf=in_tmf)                                                                                        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Isolating CVA angles and magnitude...')        arcpy.SetProgressorPosition(10)                                                        # isolate moisture decline        ds_temp = gdvspectra.isolate_cva_change(ds_cva, angle_min=0, angle_max=90)        ds_cva['mst_dec_magnitude'] = ds_temp['magnitude']        # isolate vegetation decline        ds_temp = gdvspectra.isolate_cva_change(ds_cva, angle_min=90, angle_max=180)        ds_cva['veg_dec_magnitude'] = ds_temp['magnitude']                                                           # isolate moisture incline        ds_temp = gdvspectra.isolate_cva_change(ds_cva, angle_min=180, angle_max=270)        ds_cva['mst_inc_magnitude'] = ds_temp['magnitude']                # isolate vegetation incline        ds_temp = gdvspectra.isolate_cva_change(ds_cva, angle_min=270, angle_max=360)        ds_cva['veg_inc_magnitude'] = ds_temp['magnitude']                # remove angles and magnitude for all        ds_cva = ds_cva.drop(['angle', 'magnitude'])                                 # # # # #        # apply threshold mask if requested        if in_mask_nc is not None:            # notify and increment progress bar            arcpy.SetProgressorLabel('Loading and checking GDV Threshold mask NetCDF...')            arcpy.SetProgressorPosition(11)                    try:                # load mask netcdf (set nodata to nan)                ds_mask = satfetcher.load_local_nc(nc_path=in_mask_nc,                                                    use_dask=True,                                                    conform_nodata_to=np.nan)                # check if mask dataset has one variable / mask                if len(ds_mask.data_vars) != 1:                    arcpy.AddError('Input mask NetCDF can have only one variable/band.')                    raise                elif 'like' not in list(ds_mask.data_vars):                    arcpy.AddError('Input mask NetCDF missing likelihood variable.')                    raise                elif isinstance(ds_mask, xr.Dataset):                    ds_mask = ds_mask.to_array()                                    # mask out likelihood dataset                ds_cva = ds_cva.where(~ds_mask.isnull())                                # squeeze extra dims                ds_cva = ds_cva.squeeze(drop=True)                            except:                arcpy.AddWarning('Could not mask likelihood NetCDF. Aborting masking procedure.')                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(11)                # append attrbutes on to dataset and bands        ds_cva.attrs = ds_attrs        ds_cva['spatial_ref'].attrs = ds_spatial_ref_attrs        for var in list(ds_cva.data_vars):            ds_cva[var].attrs = ds_band_attrs                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(12)           # export netcdf file        tools.export_xr_as_nc(ds=ds_cva, filename=out_nc)                          # # # # #        # add to map if requested        if in_add_result_to_map:                        # notify and increment progess bar            arcpy.SetProgressorLabel('Adding GDV CVA to map...')            arcpy.SetProgressorPosition(13)                        try:                # for current project, open current map                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap                # create output folder using datetime as name                dt = datetime.datetime.now().strftime('%d%m%Y%H%M%S')                out_folder = os.path.join(os.path.dirname(out_nc), 'cva' + '_' + dt)                os.makedirs(out_folder)                                # disable visualise on map temporarily                arcpy.env.addOutputsToMap = False                                # create crf filename and copy it                out_file = os.path.join(out_folder, 'cva.crf')                crf = arcpy.CopyRaster_management(in_raster=out_nc,                                                   out_rasterdataset=out_file)                                                                  # add to map                                  m.addDataFromPath(crf)                                 # re-enable add to map                arcpy.env.addOutputsToMap = True                                # apply cmap                lyr = arc.apply_cmap(aprx=aprx,                                      lyr_name='cva.crf',                                      cmap_name='Spectrum By Wavelength-Full Bright',                                      cutoff_pct=0.001)             except:                arcpy.AddWarning('Could not visualise output. Aborting visualisation.')                                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(14)                # close likelihood ds        ds.close()        ds_cva.close()        del ds, ds_cva                # close mask if exists         if in_mask_nc is not None:            ds_mask.close()            del ds_mask        # notify user        arcpy.AddMessage('Generated CVA successfully.')                          returnclass Phenolopy_Metrics(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = 'Phenolopy Metrics'        self.description = 'Calculate various metrics that describe various. ' \                           'aspects of vegetation phenology from a data cube. ' \                           'Key metrics include Peak of Season (POS), Start and ' \                           'End of Season (SOS, EOS), and various productivity metrics.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """                # input netcdf file        par_raw_nc_path = arcpy.Parameter(                            displayName='Input Satellite NetCDF file',                            name='in_raw_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Input')        par_raw_nc_path.filter.list = ['nc']                # output netcdf file        par_out_nc_path = arcpy.Parameter(                            displayName='Output Phenometrics Netcdf file',                            name='out_metrics_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_path.filter.list = ['nc']                # use all dates        par_use_all_dates = arcpy.Parameter(                              displayName='Use median of all input images',                              name='in_use_all_dates',                              datatype='GPBoolean',                              parameterType='Required',                              direction='Input',                              multiValue=False)        par_use_all_dates.value = True                # set specific year        par_specific_years = arcpy.Parameter(                              displayName='Set specific year(s) aggregate',                              name='in_specific_years',                              datatype='GPLong',                              parameterType='Optional',                              direction='Input',                              multiValue=True)        par_specific_years.filter.type = 'ValueList'        par_specific_years.filter.list = []        par_specific_years.value = None                # input vegetation index         par_veg_idx = arcpy.Parameter(                        displayName='Vegetation index',                        name='in_veg_idx',                        datatype='GPString',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_veg_idx.filter.type = 'ValueList'        par_veg_idx.filter.list = [            'NDVI',            'EVI',             'SAVI',            'MSAVI',            'SLAVI',            'MAVI',            'kNDVI',            'TCG'            ]        par_veg_idx.value = 'MAVI'                # input time-series resampling        par_resampling_interval = arcpy.Parameter(                                    displayName='Resampling interval',                                    name='in_resampling_interval',                                    datatype='GPString',                                    parameterType='Required',                                    direction='Input',                                    multiValue=False)        par_resampling_interval.filter.type = 'ValueList'        par_resampling_interval.filter.list = [            'Weekly',            'Bi-monthly',             'Monthly'            ]        par_resampling_interval.value = 'Bi-monthly'                      # input metrics        par_metrics = arcpy.Parameter(                        displayName='Phenological metrics',                        name='in_metrics',                        datatype='GPString',                        parameterType='Required',                        direction='Input',                        multiValue=True)        metrics = [            'POS: Peak of season',            'MOS: Middle of season',            'VOS: Valley of season',            'BSE: Base of season',            'AOS: Amplitude of season',            'SOS: Start of season',            'EOS: End of season',            'LOS: Length of season',            'ROI: Rate of increase',            'ROD: Rate of decrease',            'LIOS: Long integral of season',            'SIOS: Short integral of season',            'LIOT: Long integral of total',            'SIOT: Short integral of total'            ]        par_metrics.filter.type = 'ValueList'                par_metrics.filter.list = metrics        remove = [            'MOS: Middle of season',             'BSE: Base of season',             'AOS: Amplitude of season',            'LOS: Length of season'            ]        par_metrics.values = [m for m in metrics if m not in remove]                       # input calculate nos        par_calc_nos = arcpy.Parameter(                         displayName='Calculate NOS (Num of seasons)',                         name='in_calc_nos',                         datatype='GPBoolean',                         parameterType='Required',                         direction='Input',                         multiValue=False)        par_calc_nos.value = False        # input method type        par_method_type = arcpy.Parameter(                            displayName='Season detection method',                            name='in_method_type',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Phenolopy Options',                            multiValue=False)        par_method_type.filter.list = [            'First of slope',            'Median of slope',            'Absolute value',            'Seasonal amplitude',            'Relative amplitude'            ]        par_method_type.values = 'Median of slope'                # input peak detection type        par_peak_metric = arcpy.Parameter(                            displayName='Peak detection metric',                            name='in_peak_metric',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Phenolopy Options',                            multiValue=False)        par_peak_metric.filter.list = ['POS: Peak of season', 'MOS: Middle of season']        par_peak_metric.values = 'POS: Peak of season'            # input base detection type        par_base_metric = arcpy.Parameter(                            displayName='Base detection metric',                            name='in_base_metric',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Phenolopy Options',                            multiValue=False)        par_base_metric.filter.list = ['BSE: Base of season', 'VOS: Valley of season']        par_base_metric.values = 'VOS: Valley of season'                # input threshold side        par_threshold_side = arcpy.Parameter(                               displayName='Threshold side',                               name='in_threshold_side',                               datatype='GPString',                               parameterType='Required',                               direction='Input',                               category='Phenolopy Options',                               multiValue=False)        par_threshold_side.filter.list = ['One sided', 'Two sided']        par_threshold_side.values = 'Two sided'                # input seasonal amplitude factor. used only during  season amplitude method        par_seasonal_amp_factor = arcpy.Parameter(                                    displayName='Seasonal amplitude factor',                                    name='in_seasonal_amp_factor',                                    datatype='GPDouble',                                    parameterType='Optional',                                    direction='Input',                                    category='Phenolopy Options',                                    multiValue=False)        par_seasonal_amp_factor.filter.type = 'Range'        par_seasonal_amp_factor.filter.list = [0.0, 1.0]        par_seasonal_amp_factor.value = 0.5                # input absolute value. used only during absolute value method        par_absolute_value = arcpy.Parameter(                               displayName='Absolute value',                               name='in_absolute_value',                               datatype='GPDouble',                               parameterType='Optional',                               direction='Input',                               category='Phenolopy Options',                               multiValue=False)        par_absolute_value.value = 0.0        # input smooth method        par_smooth_method = arcpy.Parameter(                              displayName='Smoothing method',                              name='in_smooth_method',                              datatype='GPString',                              parameterType='Required',                              direction='Input',                              category='Smoothing Options',                              multiValue=False)        par_smooth_method.filter.list = ['Savitsky-Golay', 'Symmetrical Gaussian']        par_smooth_method.values = 'Savitsky-Golay'                 # input savitsky window length. only needed for savitsky        par_savitsky_window_length = arcpy.Parameter(                                       displayName='Window length',                                       name='in_savitsky_window_length',                                       datatype='GPLong',                                       parameterType='Required',                                       direction='Input',                                       category='Smoothing Options',                                       multiValue=False)        par_savitsky_window_length.filter.type = 'Range'        par_savitsky_window_length.filter.list = [3, 99]        par_savitsky_window_length.value = 3                 # input savitsky polyorder. only needed for savitsky        par_savitsky_polyorder = arcpy.Parameter(                                   displayName='Polyorder',                                   name='in_savitsky_polyorder',                                   datatype='GPLong',                                   parameterType='Required',                                   direction='Input',                                   category='Smoothing Options',                                   multiValue=False)        par_savitsky_polyorder.filter.type = 'Range'        par_savitsky_polyorder.filter.list = [1, 100]        par_savitsky_polyorder.value = 1                 # input symmetrical gaussian sigma. only needed for symm gauss        par_gaussian_sigma = arcpy.Parameter(                               displayName='Sigma',                               name='in_gaussian_sigma',                               datatype='GPLong',                               parameterType='Required',                               direction='Input',                               category='Smoothing Options',                               multiValue=False)        par_gaussian_sigma.filter.type = 'Range'        par_gaussian_sigma.filter.list = [1, 100]        par_gaussian_sigma.value = 1                        # input outlier removal method        par_outlier_method = arcpy.Parameter(                               displayName='Outlier removal method',                               name='in_outlier_method',                               datatype='GPString',                               parameterType='Required',                               direction='Input',                               category='Outlier Removal',                               multiValue=False)        par_outlier_method.filter.list = ['Local Median Threshold', 'Z-Score']        par_outlier_method.values = 'Local Median Threshold'                        # input user factor        par_user_factor = arcpy.Parameter(                            displayName='User factor',                            name='in_user_factor',                            datatype='GPLong',                            parameterType='Required',                            direction='Input',                            category='Outlier Removal',                            multiValue=False)        par_user_factor.filter.type = 'Range'        par_user_factor.filter.list = [1, 100]        par_user_factor.value = 2                                  # input zscore pvalue        par_zscore_pvalue = arcpy.Parameter(                              displayName='Zscore p-value',                              name='in_zscore_pvalue',                              datatype='GPDouble',                              parameterType='Required',                              direction='Input',                              category='Outlier Removal',                              multiValue=False)        par_zscore_pvalue.filter.type = 'Range'        par_zscore_pvalue.filter.list = [0.0001, 0.2]        par_zscore_pvalue.value = 0.05                      # input oa fmask         par_fmask_flags = arcpy.Parameter(                            displayName='Include flags',                            name='in_fmask_flags',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Satellite Quality Options',                            multiValue=True)        flags = ['NoData', 'Valid', 'Cloud', 'Shadow', 'Snow', 'Water']        par_fmask_flags.filter.type = 'ValueList'              par_fmask_flags.filter.list = flags        par_fmask_flags.values = ['Valid', 'Snow', 'Water']                # input max cloud cover        par_max_cloud = arcpy.Parameter(                          displayName='Maximum cloud cover',                          name='in_max_cloud',                          datatype='GPDouble',                          parameterType='Optional',                          direction='Input',                          category='Satellite Quality Options',                          multiValue=False)        par_max_cloud.filter.type = 'Range'        par_max_cloud.filter.list = [0.0, 100.0]        par_max_cloud.value = 10.0                # input add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True        # combine parameters        parameters = [            par_raw_nc_path,            par_out_nc_path,            par_use_all_dates,            par_specific_years,            par_veg_idx,            par_resampling_interval,            par_metrics,            par_calc_nos,            par_method_type,            par_peak_metric,            par_base_metric,            par_threshold_side,            par_seasonal_amp_factor,            par_absolute_value,            par_smooth_method,            par_savitsky_window_length,            par_savitsky_polyorder,            par_gaussian_sigma,            par_outlier_method,            par_user_factor,            par_zscore_pvalue,            par_fmask_flags,             par_max_cloud,            par_add_result_to_map            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # imports        try:            import numpy as np            import xarray as xr        except:            arcpy.AddError('Python library xarray not installed.')            raise                # enable seasonal amp factor if seasonal amp method selected        if parameters[8].value == 'Seasonal amplitude':            parameters[12].enabled = True  # enable seasonal amp factor        else:            parameters[12].enabled = False  # disable seasonal amp factor                    # enable absolute value if absolute value method selected        if parameters[8].value == 'Absolute value':            parameters[13].enabled = True  # enable absolute value        else:            parameters[13].enabled = False  # disable absolute value                    # enable window length and polyorder when savitsky method selected        if parameters[14].value == 'Savitsky-Golay':            parameters[15].enabled = True  # enable window lngth            parameters[16].enabled = True  # enable polyorder        else:            parameters[15].enabled = False  # disable window lngth            parameters[16].enabled = False  # disable polyorder                   # enable sigma when symmetrical gaussian method selected        if parameters[14].value == 'Symmetrical Gaussian':            parameters[17].enabled = True  # enable sigma        else:            parameters[17].enabled = False  # disable sigma                    # enable zscore pvalue when zscore selected        if parameters[18].value == 'Z-Score':            parameters[20].enabled = True  # enable zscore pvalue        else:            parameters[20].enabled = False  # disable zscore pvalue        # modify use all dates boolean control        if not parameters[2].value:                    # disable specific year control            parameters[3].enabled = True                        # try and get nc if exists            years = []            try:                # get path, open dataset, extract years                nc_path = parameters[0].valueastext                ds = xr.open_dataset(nc_path)                years = np.unique(ds['time.year']).tolist()            except:                arcpy.AddError('Could not read times from input NetCDF.')                raise                        # update list of years            parameters[3].filter.list = years        else:            # disable specific year control            parameters[3].enabled = False                        # update list of years to empty            parameters[3].filter.list = []                return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the Phenolopy Metrics module.        """                # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)                # safe imports        import os, sys       # arcgis comes with these        import datetime      # arcgis comes with this        import numpy as np   # arcgis comes with this        import pandas as pd  # arcgis comes with this        import tempfile      # arcgis comes with this        # risk imports (non-native to arcgis)        try:            import xarray as xr  # not in arcgis        except:            arcpy.AddError('Python library Xarray is not installed.')            raise                # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                          # module folder            sys.path.append(FOLDER_MODULES)            import phenolopy, cog           except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            raise                                        # grab parameter values         in_nc = parameters[0].valueAsText                 # raw input satellite netcdf        out_nc = parameters[1].valueAsText                # output phenometrics netcdf        in_use_all_dates = parameters[2].value            # use all dates in nc         in_specific_years = parameters[3].valueAsText     # set specific year         in_veg_idx = parameters[4].value                  # vege index name        in_resample_interval = parameters[5].value        # resample interval        in_metrics = parameters[6].valueAsText            # phenometrics        in_calc_nos = parameters[7].value                 # calculate nos        in_method_type = parameters[8].value              # phenolopy method type        in_peak_metric = parameters[9].value              # peak metric        in_base_metric = parameters[10].value             # base metric        in_threshold_side = parameters[11].value          # threshold side        in_seasonal_amp_factor = parameters[12].value     # seasonal amplitude factor        in_absolute_value = parameters[13].value          # absolute value        in_smooth_method = parameters[14].value           # smoothing method         in_sav_window_length = parameters[15].value       # savitsky window length         in_sav_polyorder = parameters[16].value           # savitsky polyorder         in_gaussian_sigma = parameters[17].value          # gaussian sigma        in_outlier_method = parameters[18].value          # outlier method        in_user_factor = parameters[19].value             # outlier cutoff user factor        in_zscore_pvalue = parameters[20].value           # zscore pvalue        in_fmask_flags = parameters[21].valueAsText       # fmask flag values        in_max_cloud = parameters[22].value               # max cloud percentage        in_add_result_to_map = parameters[23].value       # add result to map        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning Phenolopy Metrics.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                             min_range=0, max_range=17)                                    # convetr specific years if exists        if in_specific_years is not None:            in_specific_years = [int(e) for e in in_specific_years.split(';')]                                            # convert resample interval to correct input (e.g. Bi-monthly to 1MS)        in_resample_interval = arc.convert_resample_interval_code(in_resample_interval)                # convert arcgis multi-value format to list of values        in_metrics = in_metrics.lower().replace("'", '').split(';')        in_metrics = [e.split(':')[0] for e in in_metrics]        # convert phenolopy method from arcgs to module friendly name        in_method_type = in_method_type.lower().replace(' ', '_')        # convert arcgis peak and base metrics format to module friendly names        in_peak_metric = in_peak_metric.lower().split(':')[0]        in_base_metric = in_base_metric.lower().split(':')[0]                # convert arcgis threshold side format to module friendly name        in_threshold_side = in_threshold_side.lower().replace(' ', '_')                # convert arcgis smoother name to module friendly name        in_smooth_method = arc.convert_smoother_code(in_smooth_method)                # check if savitsky parameters are correct, if selected        in_sav_window_length, in_sav_polyorder = arc.check_savitsky_inputs(in_sav_window_length,                                                                            in_sav_polyorder)                # convert arcgis outlier method to module friendly name        if in_outlier_method == 'Local Median Threshold':            in_outlier_method = 'median'        else:            in_outlier_method = 'zscore'                    # convert fmask flags as text to numeric code equivalents        in_fmask_flags = [e for e in in_fmask_flags.split(';')]        in_fmask_flags = arc.convert_fmask_codes(in_fmask_flags)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Loading and checking netcdf...')        arcpy.SetProgressorPosition(1)                # load raw netcdf        ds = satfetcher.load_local_nc(nc_path=in_nc,                                       use_dask=True,                                       conform_nodata_to=np.nan)                                              # check netcdf if it has bands, get attributes for ds and a band        if len(ds.data_vars) == 0:            arcpy.AddError('Input NetDF must be a xr dataset.')            raise        elif len(ds.data_vars) == 0:            arcpy.AddError('Input NetDF has no data/variables/bands.')            raise                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Getting NetCDF attributes and mask information...')        arcpy.SetProgressorPosition(2)                        # get attributes from dataset        ds_attrs = ds.attrs        ds_band_attrs = ds[list(ds.data_vars)[0]].attrs        ds_spatial_ref_attrs = ds['spatial_ref'].attrs                                # check if expected band name exists        mask_band = arc.get_name_of_mask_band(list(ds.data_vars))        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Removing invalid pixels and empty dates...')        arcpy.SetProgressorPosition(3)                # remove invalid pixels and empty scenes        ds = cog.remove_fmask_dates(ds=ds,                                     valid_class=in_fmask_flags,                                     max_invalid=in_max_cloud,                                     mask_band=mask_band,                                     nodata_value=np.nan,                                     drop_fmask=True)                                             # get platform name from attributes, error if no attributes        in_platform = arc.get_platform_from_dea_attrs(ds_attrs)                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Calculating vegetation index...')        arcpy.SetProgressorPosition(4)                # conform dea aws band names based on platform        ds = satfetcher.conform_dea_ard_band_names(ds=ds,                                                    platform=in_platform.lower())         # calculate vegetation index         ds = tools.calculate_indices(ds=ds,                                      index=in_veg_idx.lower(),                                      custom_name='veg_idx',                                      rescale=False,                                      drop=True)                # append original attributes on to new band        ds['veg_idx'].attrs = ds_band_attrs                 # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Correcting edge dates...')        arcpy.SetProgressorPosition(5)                # ensure first/last date are start/end of year        ds = phenolopy.conform_edge_dates(ds=ds)                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Performing a resample to equalise dates...')        arcpy.SetProgressorPosition(6)                # resample to weekly medians, prior to group-resample        ds = phenolopy.resample(ds=ds,                                 interval='1W',                                inplace=True)                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Interpolating initial gaps...')        arcpy.SetProgressorPosition(7)                # interpolate missing values        ds = phenolopy.interpolate(ds=ds,                                    method='full',                                    inplace=True)        # # # # #        # subset data to specific years if requested        if not in_use_all_dates:                    # notify and increment progess bar            arcpy.SetProgressorLabel('Subsetting data to requeste year(s)...')            arcpy.SetProgressorPosition(8)                    # if actual years provided, execute            if len(in_specific_years) > 0:                ds = ds.where(ds['time.year'].isin(in_specific_years), drop=True)            else:                arcpy.AddError('No analysis year(s) provided.')                raise        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Grouping times...')        arcpy.SetProgressorPosition(8)                # group and reduce dataset into median weeks (52 for one year)        ds = phenolopy.group(ds=ds,                              interval='week',                             inplace=True)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Removing outliers...')        arcpy.SetProgressorPosition(9)                #  remove outliers from data using requeted method        ds = phenolopy.remove_outliers(ds=ds,                                        method=in_outlier_method,                                        user_factor=in_user_factor,                                        z_pval=in_zscore_pvalue)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Performing resample...')        arcpy.SetProgressorPosition(10)                # resample data using user interval        ds = phenolopy.resample(ds=ds,                                 interval=in_resample_interval,                                inplace=True)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Removing overshot times...')        arcpy.SetProgressorPosition(11)                # resampling can overshoot to previous/next year... remove those         ds = phenolopy.remove_overshoot_times(ds=ds, max_times=3)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Smoothing time-series...')        arcpy.SetProgressorPosition(12)               # use smoothing filter to smooth across time dimension        ds = phenolopy.smooth(ds=ds,                               method=in_smooth_method,                               window_length=in_sav_window_length,                               polyorder=in_sav_polyorder,                               sigma=in_gaussian_sigma)                                              # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Computing data into memory...')        arcpy.SetProgressorPosition(13)                # compute prior to phenometrics        ds = ds.compute()                        # # # # #        # calculate number of seasons, if requested        if in_calc_nos:                    # notify and increment progess bar            arcpy.SetProgressorLabel('Calculating number of seasons...')            arcpy.SetProgressorPosition(14)            # calculate number of seasons (num of major peaks) per-pixel            ds_nos = phenolopy.calc_num_seasons(ds=ds)                    # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Calculating phenometrics...')        arcpy.SetProgressorPosition(14)                # calc phenometrics via phenolopy!        ds = phenolopy.calc_phenometrics(ds=ds,                                          metric=in_metrics,                                         peak_metric=in_peak_metric,                                          base_metric=in_base_metric,                                          method=in_method_type,                                          factor=in_seasonal_amp_factor,                                          thresh_sides=in_threshold_side,                                          abs_value=in_absolute_value)                                                 # add number of seasons to dataset if calculated        if in_calc_nos:            ds['nos_values'] = ds_nos['nos_values']                                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(15)                # append attrbutes on to dataset and bands        ds.attrs = ds_attrs        ds['spatial_ref'].attrs = ds_spatial_ref_attrs        for var in list(ds.data_vars):            ds[var].attrs = ds_band_attrs                 # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(16)          # set any nan to 0        ds = ds.where(~ds.isnull(), 0)        # remove any rogue dimensions        ds = ds.squeeze(drop=True)        # export netcdf file        tools.export_xr_as_nc(ds=ds, filename=out_nc)                        # # # # #        # add multi-dim raster to current map        if in_add_result_to_map:                        # notify and increment progess bar            arcpy.SetProgressorLabel('Adding metrics to current ArcGIS map...')            arcpy.SetProgressorPosition(16)                                    # create output folder with dt            dt = datetime.datetime.now().strftime("%d%m%Y%H%M%S")            out_folder = os.path.join(os.path.dirname(out_nc), 'metrics' + '_' + dt)            os.makedirs(out_folder)            try:                # try to get current map, fail if doesnt exist                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap                                # enable auto-add to map                #arcpy.env.addOutputsToMap = False                                # setup a group layer via template                grp_lyr = arcpy.mp.LayerFile(GRP_LYR_FILE)                grp = m.addLayer(grp_lyr)[0]                grp.name = 'metrics'                # loop each metric and export a seperate crf                for var in list(ds.data_vars):                                # create temporary netcdfil for one var (prevents 2.9 bug)                    with tempfile.NamedTemporaryFile() as tmp:                        tmp_nc = '{}_{}.nc'.format(tmp.name, var)                        ds[var].to_dataset().to_netcdf(tmp_nc)                                            # build in-memory crf for temp netcdf                    out_crf = os.path.join(out_folder, 'temp_{}.crf'.format(var))                    lyr = arcpy.md.MakeMultidimensionalRasterLayer(in_multidimensional_raster=tmp_nc,                                                                    out_multidimensional_raster_layer=out_crf)                                        # export final tif                    out_tif = os.path.join(out_folder, '{}.tif'.format(var))                    arcpy.management.CopyRaster(in_raster=lyr, out_rasterdataset=out_tif)                                                                          # add to current map                    m.addDataFromPath(out_tif)                                        # determine optimal cmap based on data type (or los)                    if 'values' in var:                        if 'los' in var:                            cmap, cutoff = 'Spectrum By Wavelength-Full Bright', 0.0                        elif 'rod' in var or 'roi' in var:                            cmap, cutoff = 'Orange-Red (Continuous)', 1.0                        else:                            cmap, cutoff = 'Precipitation', 0.5                    else:                        cmap, cutoff = 'Temperature', 0.0                    # apply symbology to layer                    sym = arc.apply_cmap(aprx=aprx,                                          lyr_name='{}.tif'.format(var),                                         cmap_name=cmap,                                         cutoff_pct=cutoff)                                                             # rename lyr, add to group, remove second instance                        m.addLayerToGroup(grp, sym)                    m.removeLayer(sym)                                except:                arcpy.AddWarning('Could not visualise output.')        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(17)                # close and del dataset        ds.close()        del ds        # notify user        arcpy.AddMessage('Generated Phenometrics successfully.')        returnclass Nicher_SDM(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = 'Nicher SDM'        self.description = 'Generate a species distribution model.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set various ArcGIS Pro UI controls. Data validation        is enforced via ArcGIS Pro API.        """                # input topographic variable tif(s) and type(s)        par_topo_tifs = arcpy.Parameter(                          displayName='Input topographic variables and variable type',                          name='in_topo_tifs',                          datatype='GPValueTable',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_topo_tifs.columns = [['DEFile', 'Raster'], ['GPString', 'Type']]        par_topo_tifs.filters[0].list = ['tif']        par_topo_tifs.filters[1].type = 'ValueList'        par_topo_tifs.filters[1].list = ['Continuous', 'Categorical']                         # output netcdf file        par_out_nc_path = arcpy.Parameter(                            displayName='Output SDM Netcdf file',                            name='out_sdm_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_path.filter.list = ['nc']        # input occurrence points shapefile        par_occurrence_feat = arcpy.Parameter(                               displayName='Input occurrence point feature',                                name='in_occurrence_feat',                                datatype='GPFeatureLayer',                                parameterType='Required',                                direction='Input',                                multiValue=False)        par_occurrence_feat.filter.list = ['Point']                # input number of pseudoabsence        par_num_pseudos = arcpy.Parameter(                            displayName='Number of psuedoabsence points',                            name='in_num_pseudos',                            datatype='GPLong',                            parameterType='Required',                            direction='Input',                            category='Occurrence Data Options',                            multiValue=False)        par_num_pseudos.filter.type = 'Range'        par_num_pseudos.filter.list = [1, 10000]        par_num_pseudos.value = 500                # input exclusion buffer        par_exclusion_buffer = arcpy.Parameter(                                 displayName='Exclusion buffer area',                                 name='in_exclusion_buffer',                                 datatype='GPLong',                                 parameterType='Required',                                 direction='Input',                                 category='Occurrence Data Options',                                 multiValue=False)        par_exclusion_buffer.filter.type = 'Range'        par_exclusion_buffer.filter.list = [1, 10000]        par_exclusion_buffer.value = 250                # input whether to equalise absence to presence        par_equalise_absence = arcpy.Parameter(                                 displayName="Equalise number of absence points",                                 name="in_equalise_absence",                                 datatype="GPBoolean",                                 parameterType="Required",                                 direction="Input",                                 category='Occurrence Data Options',                                 multiValue=False)        par_equalise_absence.value = False                # input model estimator        par_estimator = arcpy.Parameter(                          displayName='Model estimator',                          name='in_etimator',                          datatype='GPString',                          parameterType='Required',                          direction='Input',                          category='Nicher Options',                          multiValue=False)        estimators = ['Random Forest', 'Extra Trees']        par_estimator.filter.type = 'ValueList'              par_estimator.filter.list = estimators        par_estimator.value = 'Random Forest'                 # input number of model estimators        par_num_estimators = arcpy.Parameter(                               displayName='Number of model estimators',                               name='in_num_estimators',                               datatype='GPLong',                               parameterType='Required',                               direction='Input',                               category='Nicher Options',                               multiValue=False)        par_num_estimators.filter.type = 'Range'        par_num_estimators.filter.list = [5, 1000]        par_num_estimators.value = 100                # input number of model replicates        par_num_replicates = arcpy.Parameter(                               displayName='Number of model replications',                               name='in_num_replicates',                               datatype='GPLong',                               parameterType='Required',                               direction='Input',                               category='Nicher Options',                               multiValue=False)        par_num_replicates.filter.type = 'Range'        par_num_replicates.filter.list = [1, 100]        par_num_replicates.value = 5                # input test ratio        par_test_ratio = arcpy.Parameter(                           displayName='Test set ratio',                           name='in_test_ratio',                           datatype='GPDouble',                           parameterType='Required',                           direction='Input',                           category='Nicher Options',                           multiValue=False)        par_test_ratio.filter.type = 'Range'        par_test_ratio.filter.list = [0, 1]        par_test_ratio.value = 0.1                    # input add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True                # combine parameters        parameters = [            par_topo_tifs,            par_out_nc_path,            par_occurrence_feat,            par_num_pseudos,            par_exclusion_buffer,            par_equalise_absence,            par_estimator,            par_num_estimators,            par_num_replicates,            par_test_ratio,            par_add_result_to_map            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                if parameters[0].altered:                    # create a value table object, fill with current values from parameter on ui            vals_table = arcpy.ValueTable(len(parameters[0].columns))            vals_table.loadFromString(parameters[0].valueAsText)                        # create a list of the values in the parameter, this will be a list of lists            vals_list = parameters[0].values                    # iterate each row, update            for i, item in enumerate(vals_list):                lyr, lyr_type = item                                try:                    # if in-app layer, get source                    path = lyr.dataSource                except:                    # else its a external file, so just get value                     path = lyr.value                # only change type if empty                if lyr_type == '':                    newvalue = "'{}' '{}'".format(path, 'Continuous')                else:                    newvalue = "'{}' '{}'".format(path, lyr_type)                                # update row                vals_table.setRow(i, newvalue)                            # reconstruct the valuetable on the ui            parameters[0].value = vals_table.exportToString()                   return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the Nicher SDM module.        """                # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)                # safe imports        import os, sys                         # arcgis comes with these        import datetime                        # arcgis comes with this        import numpy as np                     # arcgis comes with this        import pandas as pd                    # arcgis comes with this        import tempfile                        # arcgis comes with this        from io import StringIO                # arcgis comes with this        from contextlib import redirect_stdout # arcgis comes with this        # risk imports (non-native to arcgis)        try:            import xarray as xr  # not in arcgis        except:            arcpy.AddError('Python library Xarray is not installed.')            raise                # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                          # module folder            sys.path.append(FOLDER_MODULES)            import nicher, cog                      except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            raise        # grab parameter values         in_topo_tifs_and_types = parameters[0].value      # topo vars and types        out_nc = parameters[1].valueAsText                # output nicher sdm netcdf        in_occurrence_feat = parameters[2]                # occurrence point shapefile           in_num_pseudos = parameters[3].value              # num pseudoabsences         in_exclusion_buffer = parameters[4].value         # exclusion buffer        in_equalise_absence = parameters[5].value         # equalise absence points        in_estimator = parameters[6].value                # estimator         in_num_estimator = parameters[7].value            # number of estimators        in_num_replicates = parameters[8].value           # number of replicates        in_test_ratio = parameters[9].value               # test train ratio        in_add_result_to_map = parameters[10].value       # add result to map        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning Nicher Species Distribution Modelling.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                             min_range=0, max_range=16)        # unpack topographic variables into relevant lists        rast_cont_list, rast_cate_list = [], []        for item in in_topo_tifs_and_types:            if item[1] == 'Continuous':                rast_cont_list.append(item[0].value)            elif item[1] == 'Categorical':                rast_cate_list.append(item[0].value)        # prepare occurrence shapefile        shp_desc = arcpy.Describe(in_occurrence_feat)        in_occurrence_feat = os.path.join(shp_desc.path, shp_desc.name)                # prepare estimator model type        in_estimator = 'rf' if in_estimator == 'Random Forest' else 'et'        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Loading and checking topographic GeoTIFs...')        arcpy.SetProgressorPosition(1)        # load rasters (continuous and categorical)        combined_rast_lists = rast_cont_list + rast_cate_list        ds = satfetcher.load_local_rasters(rast_path_list=combined_rast_lists,                                            use_dask=True,                                            conform_nodata_to=-999)                                                   # check netcdf if it has bands, get attributes for ds and a band        if len(ds.data_vars) == 0:            arcpy.AddError('No bands/variables detected in input NetCDF.')            raise        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Getting Dataset attributes...')        arcpy.SetProgressorPosition(2)                        # get attributes from dataset        ds_attrs = ds.attrs        ds_band_attrs = ds[list(ds.data_vars)[0]].attrs                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Computing topographic variable GeoTIFs into memory...')        arcpy.SetProgressorPosition(3)                # compute into memory        ds = ds.compute()                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Reading occurrence points...')        arcpy.SetProgressorPosition(4)                  # extract point x and y from shapefile as pandas dataframe        df_records = tools.read_shapefile(shp_path=in_occurrence_feat)          # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Preparing occurrence points...')        arcpy.SetProgressorPosition(5)                # subset columns        df_presence = tools.subset_records(df_records=df_records,                                            p_a_column=None)        # drop presence column        df_presence = df_presence.drop('actual', axis='columns')                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Generating pseudoabsence points...')        arcpy.SetProgressorPosition(6)        # generate absences using dataset pixels and occurrence coords        df_absence = nicher.generate_absences(ds=ds,                                               num_abse=in_num_pseudos,                                               occur_shp_path=in_occurrence_feat,                                              buff_m=in_exclusion_buffer,                                               res_factor=3)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Extracting topographic variable values to points...')        arcpy.SetProgressorPosition(7)                # extract values for presence points        df_presence_data = tools.extract_xr_values(ds=ds,                                                    coords=df_presence,                                                    keep_xy=False,                                                    res_factor=3)                # do same for absence points        df_absence_data = tools.extract_xr_values(ds=ds,                                                   coords=df_absence,                                                   keep_xy=False,                                                   res_factor=3)                                                                                                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Removing NoData values from points points...')        arcpy.SetProgressorPosition(8)           # remove all presence records containing nodata values        df_presence_data = tools.remove_nodata_records(df_records=df_presence_data,                                                       nodata_value=ds.nodatavals)                                                               # remove all absence records containing nodata values        df_absence_data = tools.remove_nodata_records(df_records=df_absence_data,                                                       nodata_value=ds.nodatavals)        # # # # #        # if user wants to equalise points, do it        if in_equalise_absence:                        # progress bar            arcpy.SetProgressorLabel('Equalising number of absence points...')            arcpy.SetProgressorPosition(8)                          # equalise absence to match number of presence            df_absence_data = gdvsdm.equalise_abse_records(df_presence=df_presence_data,                                                            df_absence=df_absence_data)                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Combinging presence and absence points...')        arcpy.SetProgressorPosition(9)                         # take pres and abse records and combine, add new pres/abse column        df_pres_abse_data = nicher.combine_pres_abse_records(df_presence=df_presence_data,                                                              df_absence=df_absence_data)                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Computing Pearson correlation matrix...')        arcpy.SetProgressorPosition(10)         # display general rule of thumb text         msg = 'Correlation matrix collinearity: < 0.6 weak , 0.6-0.8 moderate, >= 0.8 strong.'        arcpy.AddMessage(msg)        # generate matrix and capture result for arcgis        f = StringIO()        with redirect_stdout(f):            nicher.generate_correlation_matrix(df_records=df_pres_abse_data,                                               show_fig=False,                                               show_text=True)            arcpy.AddMessage(f.getvalue())        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Computing Variance Inflation Factor...')        arcpy.SetProgressorPosition(11)         # display general rule of thumb text         msg = 'Variance Inflation Factor score: 1 = No multicolinearity, 1-5 = ' \              'moderate, > 5 = high, > 10 = Remove'        arcpy.AddMessage(msg)            # generate vif and capture result for arcgis        f = StringIO()        with redirect_stdout(f):            nicher.generate_vif_scores(df_records=df_pres_abse_data)                       arcpy.AddMessage(f.getvalue())                                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Performing Species Distribution Modelling...')        arcpy.SetProgressorPosition(12)             # create a random forest estimator using default sklearn parameters        estimator = nicher.create_estimator(estimator_type=in_estimator,                                             n_estimators=in_num_estimator)                                                    # generate SDM and capture outputs to arcgis        f = StringIO()        with redirect_stdout(f):            ds = nicher.generate_sdm(ds=ds,                                      df_records=df_pres_abse_data,                                      estimator=estimator,                                      rast_cont_list=rast_cont_list,                                      rast_cate_list=rast_cate_list,                                      replicates=in_num_replicates,                                      test_ratio=in_test_ratio,                                      equalise_test_set=False,                                      calc_accuracy_stats=True,                                     plot_stats=False)                                 arcpy.AddMessage(f.getvalue())                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(13)                # append attrbutes on to dataset and bands        ds.attrs = ds_attrs        for var in list(ds.data_vars):            ds[var].attrs = ds_band_attrs                                                        # manually create albers attributes        ds = tools.build_xr_attributes(ds)                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(14)                                               # export netcdf file        tools.export_xr_as_nc(ds=ds, filename=out_nc)                        # # # # #        # add multi-dim raster to current map        if in_add_result_to_map:                        # notify and increment progess bar            arcpy.SetProgressorLabel('Adding SDM results to current ArcGIS map...')            arcpy.SetProgressorPosition(15)                                    # create output folder with dt            dt = datetime.datetime.now().strftime("%d%m%Y%H%M%S")            out_folder = os.path.join(os.path.dirname(out_nc), 'sdm' + '_' + dt)            os.makedirs(out_folder)                        try:                # try to get current map, fail if does not exist                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap                                # enable auto-add to map                #arcpy.env.addOutputToMap = False                                # setup a group layer via template                grp_lyr = arcpy.mp.LayerFile(GRP_LYR_FILE)                grp = m.addLayer(grp_lyr)[0]                grp.name = 'sdm'                                # loop each var and export a seperate crf                for var in list(ds.data_vars):                                    # create temporary netcdfil for one var (prevents 2.9 bug)                    with tempfile.NamedTemporaryFile() as tmp:                        tmp_nc = '{}_{}.nc'.format(tmp.name, var)                        ds[var].to_dataset().to_netcdf(tmp_nc)                                # build in-memory crf for temp netcdf                    out_crf = os.path.join(out_folder, 'temp_{}.crf'.format(var))                    lyr = arcpy.md.MakeMultidimensionalRasterLayer(in_multidimensional_raster=tmp_nc,                                                                    out_multidimensional_raster_layer=out_crf)                                        # export final tif                    out_tif = os.path.join(out_folder, '{}.tif'.format(var))                    arcpy.management.CopyRaster(in_raster=lyr, out_rasterdataset=out_tif)                                                                          # add to current map                    m.addDataFromPath(out_tif)                                        # apply symbology to layer                    sym = arc.apply_cmap(aprx=aprx,                                          lyr_name='{}.tif'.format(var),                                         cmap_name='Spectrum By Wavelength-Full Bright',                                         cutoff_pct=0.0)                                        # rename lyr, add to group, remove second instance                    m.addLayerToGroup(grp, sym)                    m.removeLayer(sym)                                except:                arcpy.AddWarning('Could not visualise output.')        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(16)                # close and del dataset        ds.close()        del ds        # notify user        arcpy.AddMessage('Generated SDM successfully.')class Nicher_Masker(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = "Nicher Masker"        self.description = "Use another raster layer to mask out areas " \                           "from SDM outputs."        self.canRunInBackground = False    def getParameterInfo(self):        """        Set various ArcGIS Pro UI controls. Data validation        is enforced via ArcGIS Pro API.        """                # input sdm netcdf file        par_sdm_nc_path = arcpy.Parameter(                            displayName='Input SDM NetCDF file',                            name='in_sdm_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Input')        par_sdm_nc_path.filter.list = ['nc']                # input mask tif or netcdf file        par_mask_file_path = arcpy.Parameter(                            displayName='Input mask NetCDF or GeoTIFF file',                            name='in_mask_file_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Input')        par_mask_file_path.filter.list = ['tif', 'nc']                # output netcdf file        par_out_nc_path = arcpy.Parameter(                            displayName='Output masked SDM Netcdf file',                            name='out_masked_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_path.filter.list = ['nc']                # input minimum value        par_min_value = arcpy.Parameter(                          displayName='Minimum value',                          name='in_min_value',                          datatype='GPDouble',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_min_value.value = 2        # input max value        par_max_value = arcpy.Parameter(                          displayName='Maximum value',                          name='in_max_value',                          datatype='GPDouble',                          parameterType='Optional',                          direction='Input',                          multiValue=False)                # input replacement value        par_replacement_value = arcpy.Parameter(                                  displayName='Replacement value',                                  name='in_replacement_value',                                  datatype='GPDouble',                                  parameterType='Required',                                  direction='Input',                                  multiValue=False)        par_replacement_value.value = 0                parameters = [            par_sdm_nc_path,            par_mask_file_path,            par_out_nc_path,            par_min_value,            par_max_value,            par_replacement_value        ]        return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the Nicher Mask module.        """                # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)                # safe imports        import os, sys                         # arcgis comes with these        import datetime                        # arcgis comes with this        import numpy as np                     # arcgis comes with this        # risk imports (non-native to arcgis)        try:            import xarray as xr  # not in arcgis        except:            arcpy.AddError('Python library Xarray is not installed.')            raise                # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                          # module folder            sys.path.append(FOLDER_MODULES)            import nicher, cog                      except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            raise        # grab parameter values         in_sdm_nc_path = parameters[0].valueAsText     # input sdm netcdf        in_mask_file_path = parameters[1].valueAsText  # input mask geotiff / netcdf        out_nc_path = parameters[2].valueAsText        # output masked sdm netcdf        in_min_value = parameters[3].value             # input min value        in_max_value = parameters[4].value             # input max value        in_replacement_value = parameters[5].value     # input replacement value                        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning Nicher Species Distribution Model Masker.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                             min_range=0, max_range=8)                                    # get mask file type               mask_filetype = 'tif' if in_mask_file_path.endswith('.tif') else 'nc'                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Reading SDM NetCDF...')        arcpy.SetProgressorPosition(1)                # load netcdf. set nodata to nan to mimic dea odc        ds_sdm = satfetcher.load_local_nc(nc_path=in_sdm_nc_path,                                           use_dask=True,                                           conform_nodata_to=np.nan)                                              # check netcdf if it has bands, get attributes for ds and a band               if len(ds_sdm.data_vars) == 0:            arcpy.AddError('No bands/variables detected in input NetCDF.')            raise        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Getting Dataset attributes...')        arcpy.SetProgressorPosition(2)                # get attributes from dataset        ds_attrs = ds_sdm.attrs        ds_band_attrs = ds_sdm[list(ds_sdm.data_vars)[0]].attrs        ds_spatial_ref_attrs = ds_sdm['spatial_ref'].attrs                           # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Reading SDM Mask NetCDF/GeoTIFF...')        arcpy.SetProgressorPosition(2)                        # open input depeneding on file type        if mask_filetype == 'tif':            ds_mask = satfetcher.load_local_rasters(rast_path_list=in_mask_file_path,                                                     use_dask=True,                                                     conform_nodata_to=np.nan)        else:            ds_mask = satfetcher.load_local_nc(nc_path=in_mask_file_path,                                                use_dask=True,                                                conform_nodata_to=np.nan)                # check netcdf if it has bands, get attributes for ds and a band        if isinstance(ds_mask, (xr.DataArray)):            ds_mask = ds_mask.to_dataset(dim='variable')        elif len(ds_mask.data_vars) == 0:            arcpy.AddError('No bands/variables detected in input mask file.')            raise                    # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Clipping and resampling SDM Mask to match SDM NetCDF...')        arcpy.SetProgressorPosition(3)           # clip and resample the mask to extent and size of sdm netcdf        ds_mask = tools.clip_xr_to_xr(ds_a=ds_mask, ds_b=ds_sdm, inplace=True)        ds_mask = tools.resample_xr(ds_from=ds_mask, ds_to=ds_sdm, resampling='nearest')        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Computing SDM and SDM Mask datasets into memory...')        arcpy.SetProgressorPosition(4)          # compute into memory        ds_sdm = ds_sdm.compute()        ds_mask = ds_mask.compute()                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Preparing SDM Mask...')        arcpy.SetProgressorPosition(5)                       # prepare mask        if in_max_value is not None:            ds_mask = xr.where((ds_mask > in_min_value) &                                (ds_mask < in_max_value), True, False)        else:            ds_mask = xr.where(ds_mask > in_min_value, True, False)                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Applying mask to SDM data...')        arcpy.SetProgressorPosition(6)                # if mask is still a dataset, convert to array        if isinstance(ds_mask, xr.Dataset):            ds_mask = ds_mask.to_array().squeeze(drop=True)                                # mask out lowest value and set to replacement value        ds_sdm = ds_sdm.where(ds_mask, in_replacement_value)                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(7)                # append attrbutes on to dataset and bands        ds_sdm.attrs = ds_attrs        ds_sdm['spatial_ref'].attrs = ds_spatial_ref_attrs        for var in list(ds_sdm.data_vars):            ds_sdm[var].attrs = ds_band_attrs                        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Exporting NetCDF...')        arcpy.SetProgressorPosition(8)                        # export netcdf file        tools.export_xr_as_nc(ds=ds_sdm, filename=out_nc_path)                        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(9)                # close and del dataset        ds_sdm.close()        ds_mask.close()        del ds_sdm, ds_mask        # notify user        arcpy.AddMessage('Generated SDM Mask successfully.')                returnclass VegFrax_Fractional_Cover(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = "VegFrax Fractional Cover"        self.description = "Extrapolate small areas of high resolution " \                           "classifed imagery across larger areas of lower " \                           "resolution imagery, such as Landsat or Sentinel."        self.canRunInBackground = False    def getParameterInfo(self):        """        Set various ArcGIS Pro UI controls. Data validation        is enforced via ArcGIS Pro API.        """                # input low res satellite netcdf/tif file        par_low_res_path = arcpy.Parameter(                             displayName='Input low-resolution Satellite data NetCDF',                             name='in_low_res_path',                             datatype='DEFile',                             parameterType='Required',                             direction='Input')        par_low_res_path.filter.list = ['nc']        # input high res satellite tif file        par_high_res_path = arcpy.Parameter(                              displayName='Input classified high-resolution GeoTIF',                              name='in_high_res_path',                              datatype='DEFile',                              parameterType='Required',                              direction='Input')        par_high_res_path.filter.list = ['tif']                        # output netcdf file        par_out_nc_path = arcpy.Parameter(                            displayName='Output Fractional Cover Netcdf',                            name='out_vegfrax_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_path.filter.list = ['nc']                      # input start year for low res netcdf/file        par_from_date = arcpy.Parameter(                                 displayName='Subset date of low resolution data from',                                 name='in_from_date',                                 datatype='GPDate',                                 parameterType='Required',                                 direction='Input',                                 multiValue=False)        par_from_date.values = '2015/01/01'                # input end year for low res netcdf/file        par_date_to = arcpy.Parameter(                        displayName='Subset date of low resolution data to',                        name='in_to_date',                        datatype='GPDate',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_date_to.values = '2020/12/31'                # input aggregator        par_aggregator = arcpy.Parameter(                                    displayName='Aggregator',                                    name='in_aggregator',                                    datatype='GPString',                                    parameterType='Required',                                    direction='Input',                                    multiValue=False)        par_aggregator.filter.type = 'ValueList'        par_aggregator.filter.list = [            'Mean',            'Median'            ]        par_aggregator.value = 'Median'                 # input classes list from high res netcdf/tif         par_classes = arcpy.Parameter(                        displayName='Set fractonal classes',                        name='in_classes',                        datatype='GPString',                        parameterType='Required',                        direction='Input',                        multiValue=True,                        enabled=False)        classes = ['No Classes']        par_classes.filter.type = 'ValueList'              par_classes.filter.list = classes        par_classes.values = ['Class: NoClass']                # input merge selected classes        par_merge_classes = arcpy.Parameter(                              displayName='Merge selected classes',                              name='in_merge_classes',                              datatype='GPBoolean',                              parameterType='Required',                              direction='Input',                              multiValue=False,                              enabled=False)        par_merge_classes.value = False                # input number of samples        par_num_samples = arcpy.Parameter(                            displayName='Number of random samples per class',                            name='in_num_samples',                            datatype='GPLong',                            parameterType='Required',                            direction='Input',                            category='VegFrax Options',                            multiValue=False)        par_num_samples.filter.type = 'Range'        par_num_samples.filter.list = [10, 10000]        par_num_samples.value = 200                # input number of model estimators        par_num_estimators = arcpy.Parameter(                               displayName='Number of model estimators',                               name='in_num_estimators',                               datatype='GPLong',                               parameterType='Required',                               direction='Input',                               category='VegFrax Options',                               multiValue=False)        par_num_estimators.filter.type = 'Range'        par_num_estimators.filter.list = [5, 1000]        par_num_estimators.value = 100                # input number of model validations        par_num_validations = arcpy.Parameter(                                displayName='Number of model validations',                                name='in_num_validations',                                datatype='GPLong',                                parameterType='Required',                                direction='Input',                                category='VegFrax Options',                                multiValue=False)        par_num_validations.filter.type = 'Range'        par_num_validations.filter.list = [1, 100]        par_num_validations.value = 10                # input oa fmask         par_fmask_flags = arcpy.Parameter(                            displayName='Include flags',                            name='in_fmask_flags',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Satellite Quality Options',                            multiValue=True)        flags = ['NoData', 'Valid', 'Cloud', 'Shadow', 'Snow', 'Water']        par_fmask_flags.filter.type = 'ValueList'              par_fmask_flags.filter.list = flags        par_fmask_flags.values = ['Valid', 'Snow', 'Water']                # input max cloud cover        par_max_cloud = arcpy.Parameter(                          displayName='Maximum cloud cover',                          name='in_max_cloud',                          datatype='GPDouble',                          parameterType='Optional',                          direction='Input',                          category='Satellite Quality Options',                          multiValue=False)        par_max_cloud.filter.type = 'Range'        par_max_cloud.filter.list = [0.0, 100.0]        par_max_cloud.value = 10.0                # input add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True                # combine parameters        parameters = [            par_low_res_path,            par_high_res_path,            par_out_nc_path,            par_from_date,            par_date_to,            par_aggregator,            par_classes,            par_merge_classes,            par_num_samples,            par_num_estimators,            par_num_validations,            par_fmask_flags,            par_max_cloud,            par_add_result_to_map            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # risk imports (non-native to arcgis)        try:            import numpy as np   # this is in arcgis, but ok            import xarray as xr  # not in arcgis            import rasterio      # not in arcgis        except:            arcpy.AddError('Python library Xarray is not installed.')            raise                    # update ui datetimes input ds altered - only do on first input change        if parameters[0].altered and not parameters[0].hasBeenValidated:            try:                # get nc path, load nc                 nc_path = parameters[0].valueAsText                ds = xr.open_dataset(nc_path)                                # convert array to dataset if array detected                 if isinstance(ds, xr.DataArray):                    ds = ds.to_dataset(name='variable')                      # check if got time dim, if so, pluck dts                if 'time' in list(ds.dims):                    start_dt = ds['time'].isel(time=0).dt.strftime('%Y-%m-%d').values                    end_dt = ds['time'].isel(time=-1).dt.strftime('%Y-%m-%d').values                # update date controls                parameters[3].value = str(start_dt)                parameters[4].value = str(end_dt)            except:                arcpy.AddError('Could not open low resolution input NetCDF.')                raise                        # modify ui classes selector list if high res altered        if parameters[1].altered and not parameters[1].hasBeenValidated:            try:                # open provided high res tif                 tif_path = parameters[1].valueAsText                ds = xr.open_rasterio(tif_path)                                                   # check bands, if single, create classes list (strings)                if len(ds) != 1:                    arcpy.AddError('High resolution raster can not be multiband.')                    raise                                # check if nodata value embedded in xr                if not hasattr(ds, 'nodatavals'):                    arcpy.AddError('Dataset does not have nodata value attribute.')                    raise                elif ds.nodatavals == 'unknown':                    arcpy.AddError('Dataset nodata value is unknown.')                    raise                                    # get all unique classes in dataset and remove nodata                classes = np.unique(ds)                classes = classes[classes != ds.nodatavals]                                # check if we got something                if len(classes) <= 0:                    arcpy.AddError('No classes detected in dataset.')                    raise                                    # finally, load all non-nodata classes into ui                text_classes = []                for c in classes:                    text_classes.append('Class: {}'.format(c))                                        # do best to sort alphabetically                text_classes.sort()                # update classes ui control with new classes and enable                parameters[6].enabled = True                parameters[6].filter.list = text_classes                parameters[6].values = text_classes                            except:                arcpy.AddError('Could not open high resolution input GeoTIFF.')                raise                  # update ui merge selected classes when classes selector is enabled        if parameters[6].enabled:            parameters[7].enabled = True  # enable merge classes option        else:            parameters[7].enabled = False  # disable merge classes option                      return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the VegFrax Fractional Cover module.        """                # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)                # safe imports        import os, sys                           # arcgis comes with these        import datetime                          # arcgis comes with this        import numpy as np                       # arcgis comes with this        import pandas as pd                      # arcgis comes with this        import tempfile                          # arcgis comes with this        from io import StringIO                  # arcgis comes with this        from contextlib import redirect_stdout   # arcgis comes with this        # risk imports (non-native to arcgis)        try:            import xarray as xr  # not in arcgis        except:            arcpy.AddError('Python library Xarray is not installed.')            raise                # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                          # module folder            sys.path.append(FOLDER_MODULES)            import vegfrax, cog                      except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            raise        # grab parameter values         in_low_res_nc = parameters[0].valueAsText         # raw input low res satellite netcdf        in_high_res_tif = parameters[1].valueAsText       # raw input high res satellite tif        out_nc = parameters[2].valueAsText                # output vegfrax netcdf        in_from_date = parameters[3].value                # start date of aggregate        in_to_date = parameters[4].value                  # end date of aggregate        in_aggregator = parameters[5].value               # aggregator        in_classes = parameters[6].valueAsText            # selected classes        in_merge_classes = parameters[7].value            # merge selected classes               in_num_samples = parameters[8].value              # number of samples        in_num_estimators = parameters[9].value           # number of model estimators        in_num_validations = parameters[10].value         # number of model validations        in_fmask_flags = parameters[11].valueAsText       # fmask flag values        in_max_cloud = parameters[12].value               # max cloud percentage        in_add_result_to_map = parameters[13].value       # add result to map        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning VegFrax Fractional Cover.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                             min_range=0, max_range=20)        # convert datetime strings to numpy datetime64        in_from_date = arc.datetime_to_numpy(in_from_date)        in_to_date = arc.datetime_to_numpy(in_to_date)               # convert arcgis multi-value format to list of values and notify               in_classes = arc.prepare_vegfrax_classes(in_classes)                # convert fmask flags as text to numeric code equivalents        in_fmask_flags = [e for e in in_fmask_flags.split(';')]        in_fmask_flags = arc.convert_fmask_codes(in_fmask_flags)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Loading and checking low resolution netcdf...')        arcpy.SetProgressorPosition(1)                # load low res netcdf. set nodata to nan to mimic dea odc        ds_low = satfetcher.load_local_nc(nc_path=in_low_res_nc,                                           use_dask=True,                                           conform_nodata_to=np.nan)                                              # check netcdf if it has bands, get attributes for ds and a band        if len(ds_low.data_vars) == 0:            arcpy.AddError('Input NetDF must be a xr dataset.')            raise        elif len(ds_low.data_vars) == 0:            arcpy.AddError('Input NetDF has no data/variables/bands.')            raise                                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Getting Dataset attributes...')        arcpy.SetProgressorPosition(2)                        # get attributes from dataset        ds_low_attrs = ds_low.attrs        ds_low_band_attrs = ds_low[list(ds_low.data_vars)[0]].attrs        ds_low_spatial_ref_attrs = ds_low['spatial_ref'].attrs                        # check if expected band name exists (landsat/sentinel differences)        ds_low_mask_band = arc.get_name_of_mask_band(list(ds_low.data_vars))        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Removing invalid pixels and dates from low res NetCDF...')        arcpy.SetProgressorPosition(3)                # remove invalid pixels and empty scenes        ds_low = cog.remove_fmask_dates(ds=ds_low,                                         valid_class=in_fmask_flags,                                         max_invalid=in_max_cloud,                                         mask_band=ds_low_mask_band,                                         nodata_value=np.nan,                                         drop_fmask=True)                                                     # conform and prepare low res dataset         ds_low = vegfrax.prepare_raw_xr(ds_low,                                         dtype='float32',                                         conform_nodata_to=-999)                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Calculating tasselled cap...')        arcpy.SetProgressorPosition(4)                # get platform name from attributes, error if no attributes        in_platform = arc.get_platform_from_dea_attrs(ds_low_attrs)                # conform dea aws band names based on platform        ds_low = satfetcher.conform_dea_ard_band_names(ds=ds_low,                                                        platform=in_platform.lower())         # calculate tasselled cap index         ds_low = tools.calculate_indices(ds=ds_low,                                          index=['tcg', 'tcb', 'tcw'],                                          custom_name=None,                                          rescale=False,                                          drop=True)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Subsetting data to requested time range...')        arcpy.SetProgressorPosition(5)                        # restrict date range        ds_low = ds_low.where((ds_low['time'] >= in_from_date) &                               (ds_low['time'] <= in_to_date), drop=True)                                      # check if any data exists after subset         if len(ds_low['time']) == 0:            arcpy.AddError('Time range used to subset data returned nothing.')            raise                    # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Aggregating low res data and computing into memory...')        arcpy.SetProgressorPosition(5)                # aggregate and compute        if in_aggregator.lower() == 'mean':            ds_low = ds_low.mean('time', keep_attrs=True).compute()        elif in_aggregator.lower() == 'median':            ds_low = ds_low.median('time', keep_attrs=True).compute()                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to low resolution dataset...')        arcpy.SetProgressorPosition(6)                        # append attrbutes on to dataset and bands        ds_low.attrs = ds_low_attrs        ds_low['spatial_ref'].attrs = ds_low_spatial_ref_attrs        for var in list(ds_low.data_vars):            ds_low[var].attrs = ds_low_band_attrs                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Loading and checking high res GeoTIFF...')        arcpy.SetProgressorPosition(7)                # load raster as an xarray dataset and set nodata to -128        ds_high = satfetcher.load_local_rasters(rast_path_list=in_high_res_tif,                                                 use_dask=True,                                                 conform_nodata_to=-128)                # check dataset has one band, get attributes for ds (i.e., band)        if isinstance(ds_high, xr.DataArray):            ds_high = ds_high.to_dataset(dim='variable')        elif len(ds_high) != 1:            arcpy.AddError('More than one band in high resolution input GeoTIFF.')            raise                # do basic preparations (dtype, rename, checks)        ds_high = vegfrax.prepare_classified_xr(ds=ds_high)                 # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Clipping high resolution data lowest extent...')        arcpy.SetProgressorPosition(8)                # subset high to low extent        ds_high = tools.clip_xr_to_xr(ds_a=ds_high, ds_b=ds_low)                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Reclassifying all non-selected classes to 0...')        arcpy.SetProgressorPosition(9)                # add 0 if missing. chekc this, could be problematic        if 0 not in in_classes:            in_classes.append(0)                                      # reclassify all other classes to 0 (and leave nodata as is)        ds_high = vegfrax.reclassify_xr(ds=ds_high,                                         req_class=in_classes,                                        merge_classes=in_merge_classes,                                        inplace=True)                # get new list of current classes        in_classes = vegfrax.get_xr_classes(ds_high)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Computing high resolution data into memory...')        arcpy.SetProgressorPosition(10)                # load into memory now - we have values to modify!        ds_high = ds_high.compute()                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Generating random samples in overlap areas...')        arcpy.SetProgressorPosition(11)                # generate random samples within area overlap between raw and classified rasters        df_samples = vegfrax.generate_strat_random_samples(ds_raw=ds_low,                                                           ds_class=ds_high,                                                            req_class=in_classes,                                                           num_samples=in_num_samples)                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Extracting values from low res data...')        arcpy.SetProgressorPosition(12)                # extract pixel values from raw, low resolution rasters at each point        df_extract = tools.extract_xr_values(ds=ds_low,                                              coords=df_samples,                                              keep_xy=True)        # remove any points containing a nodata value        df_extract_clean = tools.remove_nodata_records(df_extract,                                                        nodata_value=ds_low.nodatavals)                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Building focal windows...')        arcpy.SetProgressorPosition(13)                # generate focal windows and extract pixels from class raster        df_windows = vegfrax.create_frequency_windows(ds_raw=ds_low,                                                       ds_class=ds_high,                                                       df_records=df_extract_clean)                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Converting window values to frequencies...')        arcpy.SetProgressorPosition(14)                        # transform raw focal window pixel class counts into frequencies        df_freqs = vegfrax.convert_window_counts_to_freqs(df_windows=df_windows,                                                           nodata_value=ds_high.nodatavals)                               # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Selecting requested classes for analysis...')        arcpy.SetProgressorPosition(15)                # convert classes to text. all classes in request list will be used        text_classes = [str(c) for c in in_classes]        # prepare data for analysis - prepare classes, nulls, normalise frequencies        df_data = vegfrax.prepare_freqs_for_analysis(ds_raw=ds_low,                                                      ds_class=ds_high,                                                      df_freqs=df_freqs,                                                      override_classes=text_classes)                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Performing Fractional Cover Analysis...')        arcpy.SetProgressorPosition(16)                # perform fca and show results in arcgis        f = StringIO()        with redirect_stdout(f):            ds_vegfrax = vegfrax.perform_fca(ds_raw=ds_low,                                              ds_class=ds_high,                                              df_data=df_data,                                              df_extract_clean=df_extract_clean,                                              n_estimators=in_num_estimators,                                             n_validations=in_num_validations)            arcpy.AddMessage(f.getvalue())                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(17)                # append top-level attributes        ds_vegfrax.attrs = ds_low_attrs                # append spatial ref back on        if 'spatial_ref' not in list(ds_vegfrax.coords):            crs = tools.get_xr_crs(ds_low)            ds_vegfrax = ds_vegfrax.assign_coords({'spatial_ref': crs})        ds_vegfrax['spatial_ref'].attrs = ds_low_spatial_ref_attrs                # add band-level attributes back on        for var in list(ds_vegfrax.data_vars):            ds_vegfrax[var].attrs = ds_low_band_attrs        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(18)                   # export netcdf file        tools.export_xr_as_nc(ds=ds_vegfrax, filename=out_nc)                                  # # # # #        # add multi-dim raster to current map        if in_add_result_to_map:                        # notify and increment progess bar            arcpy.SetProgressorLabel('Adding fractional cover classes to current ArcGIS map...')            arcpy.SetProgressorPosition(19)                                    # create output folder with dt            dt = datetime.datetime.now().strftime("%d%m%Y%H%M%S")            out_folder = os.path.join(os.path.dirname(out_nc), 'vegfrax' + '_' + dt)            os.makedirs(out_folder)                        try:                # try to get current map, fail if does not exist                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap	                                # enable auto-add to map                #arcpy.env.addOutputToMap = False                                # setup a group layer via template                grp_lyr = arcpy.mp.LayerFile(GRP_LYR_FILE)                grp = m.addLayer(grp_lyr)[0]                grp.name = 'vegfrax'                # loop each var and export a seperate crf                for var in list(ds_vegfrax.data_vars):                                    # create temporary netcdfil for one var (prevents 2.9 bug)                    with tempfile.NamedTemporaryFile() as tmp:                        tmp_nc = '{}_{}.nc'.format(tmp.name, var)                        ds_vegfrax[var].to_dataset().to_netcdf(tmp_nc)                                # build in-memory crf for temp netcdf                    out_crf = os.path.join(out_folder, 'temp_{}.crf'.format(var))                    lyr = arcpy.md.MakeMultidimensionalRasterLayer(in_multidimensional_raster=tmp_nc,                                                                    out_multidimensional_raster_layer=out_crf)                    # export final tif                    out_tif = os.path.join(out_folder, '{}.tif'.format(var))                    arcpy.management.CopyRaster(in_raster=lyr, out_rasterdataset=out_tif)                    # add to current map                    m.addDataFromPath(out_tif)                                        # apply symbology to layer                    sym = arc.apply_cmap(aprx=aprx,                                          lyr_name='{}.tif'.format(var),                                         cmap_name='Spectrum By Wavelength-Full Bright',                                         cutoff_pct=0.0)                    # rename lyr, add to group, remove second instance                        m.addLayerToGroup(grp, sym)                    m.removeLayer(sym)            except:                arcpy.AddWarning('Could not visualise output.')        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(20)                # close and del dataset        ds_low.close()        ds_high.close()        ds_vegfrax.close()        del ds_low, ds_high, ds_vegfrax        # notify user        arcpy.AddMessage('Generated fractional covers successfully.')        returnclass Ensemble_Model(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = 'Ensemble Model'        self.description = 'Combine two or more evidence layers into ' \                           'belief/disbelief/plausability/confidence outputs .' \                           'The benefit of this is areas of certainty and ' \                           'uncertainty can be derived, providing a better ' \                           'understanding of where the model can be trusted.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set various ArcGIS Pro UI controls. Data validation        is enforced via ArcGIS Pro API.        """                # input rasters, belief/disbelief, and inflections        par_ensemble_layers = arcpy.Parameter(                                displayName='Input evidence layers as NetCDFs',                                name='in_ensemble_layers',                                datatype='GPValueTable',                                parameterType='Required',                                direction='Input',                                multiValue=False)        par_ensemble_layers.columns = [                                       ['DEFile', 'NetCDF File'],                                        ['GPString', 'NetCDF Variable'],                                        ['GPString', 'Layer Type'],                                        ['GPString', 'A'],                                        ['GPString', 'BC'],                                        ['GPString', 'D']                                       ]        par_ensemble_layers.filters[0].list = ['nc']        par_ensemble_layers.filters[1].type = 'ValueList'        par_ensemble_layers.filters[1].list = []             par_ensemble_layers.filters[2].type = 'ValueList'        par_ensemble_layers.filters[2].list = ['Belief', 'Disbelief']                # output netcdf file        par_out_nc_path = arcpy.Parameter(                            displayName='Output Ensemble Netcdf file',                            name='out_ensemble_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_path.filter.list = ['nc']        # input resample to        par_resample_to = arcpy.Parameter(                            displayName='Resample to',                            name='in_resample_to',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Ensemble Options',                            multiValue=False)        resample_to = ['Highest', 'Lowest']        par_resample_to.filter.type = 'ValueList'              par_resample_to.filter.list = resample_to        par_resample_to.value = 'Highest'                # input add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True        params = [            par_ensemble_layers,            par_out_nc_path,            par_resample_to,            par_add_result_to_map        ]                return params    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when         controls are changed on ArcGIS Pro panel.        """                # imports        try:            import xarray as xr            import dask        except:            arcpy.AddError('Could not import Xarray or Dask.')            raise                            # if user changed inputs (added new input netcdf)        if parameters[0].altered:                                        # get all columns and values as valuetable and list objects            vals_table = arcpy.ValueTable(len(parameters[0].columns))            vals_table.loadFromString(parameters[0].valueAsText)            vals_list = parameters[0].values                        # iterate each row in list and update            for i, item in enumerate(vals_list):                                # if a nc file entered...                if item[0].value:                    ds_vars = []                    try:                        # ... try and get its variable names                        ds = xr.open_dataset(item[0].value)                        for ds_var in list(ds.data_vars):                            ds_vars.append(ds_var)                                                    # if vars returned, add to controls if new                        if len(ds_vars) > 0:                            existing_vars = parameters[0].filters[1].list                                                        new_vars = []                            for ds_var in ds_vars:                                if ds_var not in existing_vars:                                    new_vars.append(ds_var)                                                                    # add to existing selection                                parameters[0].filters[1].list = existing_vars + new_vars                    except:                        arcpy.AddError('Could not read NetCDF: {}'.format(item[0].value))                        return                                            # set path value                    newvalue = "'{}'".format(item[0].value)                                        # update variable list, selection and save var for min/max stats                                       var_name = ''                                        # if first run, get min and max                    if item[1] == '':                                            # add first var name, default type                        var_name = ds_vars[0]                                               newvalue += " '{}'".format(var_name)                        newvalue += " '{}'".format('Belief')                        newvalue += " '{}'".format('Min')                        newvalue += " '{}'".format('Max')                        newvalue += " '{}'".format('NA')                                            else:                                           # keep first few controls as is                        newvalue += " '{}'".format(item[1])                        newvalue += " '{}'".format(item[2])                                                # check if a is valid, else na                        a = item[3]                        if a.isnumeric() or a.lower() in ['na', "''"]:                            a = a.upper()                        elif a.lower() in ['min', 'max']:                            a = a.title()                        else:                            a = 'NA'                                                    # check if bc is valid, else na                        bc = item[4]                        if bc.isnumeric() or bc.lower() in ['na', "''"]:                            bc = bc.upper()                        elif bc.lower() in ['min', 'max']:                            bc = bc.title()                        else:                            bc = 'NA'                                                    # check if d is valid, else na                        d = item[5]                        if d.isnumeric() or d.lower() in ['na', "''"]:                            d = d.upper()                        elif d.lower() in ['min', 'max']:                            d = d.title()                        else:                            d = 'NA'                                                       # update with clean a, bc and d values                        newvalue += " '{}'".format(a)                        newvalue += " '{}'".format(bc)                        newvalue += " '{}'".format(d)                                                 # replace                         newvalue = newvalue.replace("''", 'NA')                    # update row                    vals_table.setRow(i, newvalue)                            # reconstruct the valuetable on the ui            parameters[0].value = vals_table.exportToString()           return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the Ensemble Modelling module.        """                # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)                # safe imports        import os, sys                         # arcgis comes with these        import datetime                        # arcgis comes with this        import numpy as np                     # arcgis comes with this        import pandas as pd                    # arcgis comes with this        import tempfile                        # arcgis comes with this        from io import StringIO                # arcgis comes with this        from contextlib import redirect_stdout # arcgis comes with this        # risk imports (non-native to arcgis)        try:            import xarray as xr  # not in arcgis        except:            arcpy.AddError('Python library Xarray is not installed.')            raise                # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                          # module folder            sys.path.append(FOLDER_MODULES)            import ensemble, cog                      except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            raise        # grab parameter values         in_lyrs = parameters[0].value                 # get ncs, vars, types, a, bc, d        out_nc = parameters[1].valueAsText            # output nc        in_resample_to = parameters[2].value          # resample from to        in_add_result_to_map = parameters[3].value    # add result to map                # may need in ui, leave here for now        in_nodataval = np.nan                # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning Ensemble Modelling.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                             min_range=0, max_range=10)                # convert netcdf file to path and convert string a, bc, d to numerics        in_lyrs = arc.convert_ensemble_parameters(in_lyrs)                          # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Checking and loading input data...')        arcpy.SetProgressorPosition(1)           # check if at least one belief and disbelief layer, else error        is_invalid = ensemble.check_belief_disbelief_exist(in_lyrs)        if is_invalid:            raise ValueError('Must select at least one belief and disbelief layer.')                    # check and load datasets and add to last element        for lyr in in_lyrs:            ds = ensemble.prepare_data(file_list=lyr[0],                                       var=lyr[1],                                       nodataval=np.nan)            lyr.append(ds) if ds is not None else lyr.append(None)           # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Getting NetCDF attributes...')        arcpy.SetProgressorPosition(2)        # get attributes from dataset        ds_temp = in_lyrs[0][-1]        ds_attrs = ds_temp.attrs        ds_band_attrs = ds_temp[list(ds_temp.data_vars)[0]].attrs        ds_spatial_ref_attrs = ds_temp['spatial_ref'].attrs        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Rescaling values via fuzzy sigmoidal functions...')        arcpy.SetProgressorPosition(3)                # loop each input item and apply sigmoidal automatically        in_lyrs = ensemble.apply_auto_sigmoids(items=in_lyrs)                                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Seperating belief and disbelief datasets...')        arcpy.SetProgressorPosition(4)        # seperate into belief, disbelief lists and append label to datasets        belief_list, disbelief_list = ensemble.seperate_ds_belief_disbelief(in_lyrs)        belief_list = ensemble.append_dempster_attr(ds_list=belief_list, dempster_label='belief')        disbelief_list = ensemble.append_dempster_attr(ds_list=disbelief_list, dempster_label='disbelief')                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Resampling dataset to conform pixel sizes...')        arcpy.SetProgressorPosition(5)                # resample all datasets to lowest resolution        ds_list = ensemble.resample_datasets(ds_list=belief_list + disbelief_list,                                              resample_to=in_resample_to.lower(),                                              resampling='nearest')        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Performing Ensemble Modelling...')        arcpy.SetProgressorPosition(6)                # perform dempster shafer modelling and compute dataset        ds_dempster = ensemble.perform_dempster(ds_list=ds_list)        ds_dempster = ds_dempster.compute()                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(7)                # append top-level attributes        ds_dempster.attrs = ds_attrs                # append spatial ref back on        if 'spatial_ref' not in list(ds_dempster.coords):            crs = ds_attrs.get('crs')  # consider tools.xr_get_crs            ds_dempster = ds_dempster.assign_coords({'spatial_ref': crs})        ds_dempster['spatial_ref'].attrs = ds_spatial_ref_attrs                # add band-level attributes back on        for var in list(ds_dempster.data_vars):            ds_dempster[var].attrs = ds_band_attrs                             # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(8)          # export netcdf file        tools.export_xr_as_nc(ds=ds_dempster, filename=out_nc)        # # # # #        # add multi-dim raster to current map        if in_add_result_to_map:                        # notify and increment progess bar            arcpy.SetProgressorLabel('Adding ensemble layers to current ArcGIS map...')            arcpy.SetProgressorPosition(9)                                    # create output folder with dt            dt = datetime.datetime.now().strftime("%d%m%Y%H%M%S")            out_folder = os.path.join(os.path.dirname(out_nc), 'ensemble' + '_' + dt)            os.makedirs(out_folder)                        try:                # try to get current map, fail if does not exist                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap	                                # enable auto-add to map                #arcpy.env.addOutputToMap = False                                # setup a group layer via template                grp_lyr = arcpy.mp.LayerFile(GRP_LYR_FILE)                grp = m.addLayer(grp_lyr)[0]                grp.name = 'ensemble'                # loop each var and export a seperate crf                for var in list(ds_dempster.data_vars):                                # create temporary netcdfil for one var (prevents 2.9 bug)                    with tempfile.NamedTemporaryFile() as tmp:                        tmp_nc = '{}_{}.nc'.format(tmp.name, var)                        ds_dempster[var].to_dataset().to_netcdf(tmp_nc)                                        # build in-memory crf for temp netcdf                    out_crf = os.path.join(out_folder, 'temp_{}.crf'.format(var))                    lyr = arcpy.md.MakeMultidimensionalRasterLayer(in_multidimensional_raster=tmp_nc,                                                                    out_multidimensional_raster_layer=out_crf)                    # export final tif                    out_tif = os.path.join(out_folder, '{}.tif'.format(var))                    arcpy.management.CopyRaster(in_raster=lyr, out_rasterdataset=out_tif)                    # add to current map                    m.addDataFromPath(out_tif)                                        # apply symbology to layer                    sym = arc.apply_cmap(aprx=aprx,                                          lyr_name='{}.tif'.format(var),                                         cmap_name='Spectrum By Wavelength-Full Bright',                                         cutoff_pct=0.0)                    # rename lyr, add to group, remove second instance                        m.addLayerToGroup(grp, sym)                    m.removeLayer(sym)            except:                arcpy.AddWarning('Could not visualise output.')        # close dempster dataset         ds_dempster.close()        del ds_dempster                # close all xr datasets in input list        for ds in ds_list:            ds.close()            del ds        returnclass Ensemble_Masker(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = "Ensemble Masker"        self.description = "Use another raster layer to mask out areas " \                           "from Ensemble outputs."        self.canRunInBackground = False    def getParameterInfo(self):        """        Set various ArcGIS Pro UI controls. Data validation        is enforced via ArcGIS Pro API.        """                # input ensemble netcdf file        par_ensemble_nc_path = arcpy.Parameter(                            displayName='Input Ensemble NetCDF file',                            name='in_ensemble_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Input')        par_ensemble_nc_path.filter.list = ['nc']                # input mask tif or netcdf file        par_mask_file_path = arcpy.Parameter(                            displayName='Input mask NetCDF or GeoTIFF file',                            name='in_mask_file_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Input')        par_mask_file_path.filter.list = ['tif', 'nc']                # output netcdf file        par_out_nc_path = arcpy.Parameter(                            displayName='Output masked Ensemble Netcdf file',                            name='out_masked_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_path.filter.list = ['nc']                # input minimum value        par_min_value = arcpy.Parameter(                          displayName='Minimum value',                          name='in_min_value',                          datatype='GPDouble',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_min_value.value = 2        # input max value        par_max_value = arcpy.Parameter(                          displayName='Maximum value',                          name='in_max_value',                          datatype='GPDouble',                          parameterType='Optional',                          direction='Input',                          multiValue=False)                # input replacement value        par_replacement_value = arcpy.Parameter(                                  displayName='Replacement value',                                  name='in_replacement_value',                                  datatype='GPDouble',                                  parameterType='Required',                                  direction='Input',                                  multiValue=False)        par_replacement_value.value = 0                parameters = [            par_ensemble_nc_path,            par_mask_file_path,            par_out_nc_path,            par_min_value,            par_max_value,            par_replacement_value        ]        return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the Nicher Mask module.        """                # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)                # safe imports        import os, sys                         # arcgis comes with these        import datetime                        # arcgis comes with this        import numpy as np                     # arcgis comes with this        # risk imports (non-native to arcgis)        try:            import xarray as xr  # not in arcgis        except:            arcpy.AddError('Python library Xarray is not installed.')            raise                # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                          # module folder            sys.path.append(FOLDER_MODULES)            import nicher, cog                      except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            raise        # grab parameter values         in_ensemble_nc_path = parameters[0].valueAsText     # input ensemble netcdf        in_mask_file_path = parameters[1].valueAsText       # input mask geotiff / netcdf        out_nc_path = parameters[2].valueAsText             # output masked sdm netcdf        in_min_value = parameters[3].value                  # input min value        in_max_value = parameters[4].value                  # input max value        in_replacement_value = parameters[5].value          # input replacement value                        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning Ensemble Model Masker.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                             min_range=0, max_range=8)                                    # get mask file type               mask_filetype = 'tif' if in_mask_file_path.endswith('.tif') else 'nc'                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Reading Ensemble NetCDF...')        arcpy.SetProgressorPosition(1)                # load netcdf. set nodata to nan to mimic dea odc        ds_ensemble = satfetcher.load_local_nc(nc_path=in_ensemble_nc_path,                                           use_dask=True,                                           conform_nodata_to=np.nan)                                              # check netcdf if it has bands, get attributes for ds and a band               if len(ds_ensemble.data_vars) == 0:            arcpy.AddError('No bands/variables detected in input NetCDF.')            raise        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Getting Dataset attributes...')        arcpy.SetProgressorPosition(2)                # get attributes from dataset        ds_attrs = ds_ensemble.attrs        ds_band_attrs = ds_ensemble[list(ds_ensemble.data_vars)[0]].attrs        ds_spatial_ref_attrs = ds_ensemble['spatial_ref'].attrs                           # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Reading Ensemble Mask NetCDF/GeoTIFF...')        arcpy.SetProgressorPosition(2)                        # open input depeneding on file type        if mask_filetype == 'tif':            ds_mask = satfetcher.load_local_rasters(rast_path_list=in_mask_file_path,                                                     use_dask=True,                                                     conform_nodata_to=np.nan)        else:            ds_mask = satfetcher.load_local_nc(nc_path=in_mask_file_path,                                                use_dask=True,                                                conform_nodata_to=np.nan)                # check netcdf if it has bands, get attributes for ds and a band        if isinstance(ds_mask, (xr.DataArray)):            ds_mask = ds_mask.to_dataset(dim='variable')        elif len(ds_mask.data_vars) == 0:            arcpy.AddError('No bands/variables detected in input mask file.')            raise                    # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Clipping and resampling Mask to match Ensemble NetCDF...')        arcpy.SetProgressorPosition(3)           # clip and resample the mask to extent and size of sdm netcdf        ds_mask = tools.clip_xr_to_xr(ds_a=ds_mask, ds_b=ds_ensemble, inplace=True)        ds_mask = tools.resample_xr(ds_from=ds_mask, ds_to=ds_ensemble, resampling='nearest')        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Computing Ensemble and Ensemble Mask datasets into memory...')        arcpy.SetProgressorPosition(4)          # compute into memory        ds_ensemble = ds_ensemble.compute()        ds_mask = ds_mask.compute()                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Preparing Ensemble Mask...')        arcpy.SetProgressorPosition(5)                       # prepare mask        if in_max_value is not None:            ds_mask = xr.where((ds_mask > in_min_value) &                                (ds_mask < in_max_value), True, False)        else:            ds_mask = xr.where(ds_mask > in_min_value, True, False)                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Applying mask to Ensemble data...')        arcpy.SetProgressorPosition(6)                # if mask is still a dataset, convert to array        if isinstance(ds_mask, xr.Dataset):            ds_mask = ds_mask.to_array().squeeze(drop=True)                                # mask out lowest value and set to replacement value        ds_ensemble = ds_ensemble.where(ds_mask, in_replacement_value)                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(7)                # append attrbutes on to dataset and bands        ds_ensemble.attrs = ds_attrs        ds_ensemble['spatial_ref'].attrs = ds_spatial_ref_attrs        for var in list(ds_ensemble.data_vars):            ds_ensemble[var].attrs = ds_band_attrs                        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Exporting NetCDF...')        arcpy.SetProgressorPosition(8)                        # export netcdf file        tools.export_xr_as_nc(ds=ds_ensemble, filename=out_nc_path)                        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(9)                # close and del dataset        ds_ensemble.close()        ds_mask.close()        del ds_ensemble, ds_mask        # notify user        arcpy.AddMessage('Generated Ensemble Mask successfully.')                returnclass NRT_Create_Project(object):    def __init__(self):        """Define the tool (tool name is the name of the class)."""        self.label = "NRT Create Project"        self.description = "Create a new database to hold monitoring areas."        self.canRunInBackground = False    def getParameterInfo(self):        """        Set various ArcGIS Pro UI controls. Data validation        is enforced via ArcGIS Pro API.        """                # output gdb folder        par_out_folder = arcpy.Parameter(                           displayName='Output Project Folder',                           name='out_folder',                           datatype='DEFolder',                           parameterType='Required',                           direction='Input')                # output gdb filename        par_out_filename = arcpy.Parameter(                             displayName='Output Project Name',                             name='out_filename',                             datatype='GPString',                             parameterType='Required',                             direction='Input')                                     # combine parameters        parameters = [            par_out_folder,            par_out_filename        ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the NRT Create Project module.        """                # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)                # safe imports        import arcpy        # arcgis comes with these                        # import tools        try:            # module folder            sys.path.append(FOLDER_MODULES)            import nrt        except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            raise                 # grab parameter values         out_folder = parameters[0].valueAsText      # output gdb folder path        out_filename = parameters[1].valueAsText    # output gdb filename                # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning NRT Create Project.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=2)                                    # check inputs are not none and strings        if out_folder is None or out_filename is None:            arcpy.AddError('Blank folder or filename provided.')            raise        elif not isinstance(out_folder, str) or not isinstance(out_folder, str):            arcpy.AddError('Folder or filename not strings.')            raise        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Creating new NRT geodatabase...')        arcpy.SetProgressorPosition(1)                # load raw netcdf (set nodata to nan)        nrt.create_nrt_project(out_folder=out_folder,                                out_filename=out_filename)          # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(2)        # notify user        arcpy.AddMessage('Created new NRT Project.')        return# needs tidying up for progressor, qolclass NRT_Monitor_Areas(object):    def __init__(self):        """Define the tool (tool name is the name of the class)."""        self.label = "NRT Monitor Areas"        self.description = "Monitor designated monitoring areas."        self.canRunInBackground = False    def getParameterInfo(self):        """        Set various ArcGIS Pro UI controls. Data validation        is enforced via ArcGIS Pro API.        """                # input monitoring area features        par_in_feat = arcpy.Parameter(                        displayName='Input monitoring area features',                        name='in_feat',                        datatype='GPFeatureLayer',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_in_feat.filter.list = ['Polygon']                # input continuous monitoring        par_ongoing = arcpy.Parameter(                        displayName='Continuously monitor areas',                        name='in_ongoing',                        datatype='GPBoolean',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_ongoing.value = False                # time interval months        par_time_interval = arcpy.Parameter(                              displayName='Set hours between monitoring checks',                              name='in_time_interval',                              datatype='GPLong',                              parameterType='Required',                              direction='Input',                              multiValue=False)        par_time_interval.filter.type = 'Range'        par_time_interval.filter.list = [1, 10000]        par_time_interval.value = 12        # email from         par_email_from = arcpy.Parameter(                           displayName='Host email address',                           name='in_email_from',                           datatype='GPString',                           parameterType='Optional',                           direction='Input',                           category='Email Alert Service',                           multiValue=False)        par_email_from.value = 'mrlewie@outlook.com'        # smtp email server        par_smtp_server = arcpy.Parameter(                           displayName='SMTP server address',                           name='in_smtp_server',                           datatype='GPString',                           parameterType='Optional',                           direction='Input',                           category='Email Alert Service',                           multiValue=False)        par_smtp_server.value = 'smtp.office365.com'                # smtp port        par_smtp_port = arcpy.Parameter(                           displayName='SMTP server port',                           name='in_smtp_port',                           datatype='GPLong',                           parameterType='Optional',                           direction='Input',                           category='Email Alert Service',                           multiValue=False)        par_smtp_port.value = 587        # smtp username        par_smtp_username = arcpy.Parameter(                              displayName='SMTP server username',                              name='in_smtp_username',                              datatype='GPString',                              parameterType='Optional',                              direction='Input',                              category='Email Alert Service',                              multiValue=False)        par_smtp_username.value = 'mrlewie@outlook.com'                # smtp username        par_smtp_password = arcpy.Parameter(                              displayName='SMTP server password',                              name='in_smtp_password',                              datatype='GPString',                              parameterType='Optional',                              direction='Input',                              category='Email Alert Service',                              multiValue=False)        par_smtp_password.value = 'halfLife1985micr'        # output epsg        par_output_epsg = arcpy.Parameter(                            displayName='Output EPSG',                            name='in_output_epsg',                            datatype='GPLong',                            parameterType='Required',                            direction='Input',                            category='Warping Options',                            multiValue=False)        par_output_epsg.filter.list = [3577]        par_output_epsg.values = 3577                # set oa class values        par_fmask_flags = arcpy.Parameter(displayName='Include pixels flags',                            name='in_fmask_flags',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Satellite Quality Options',                            multiValue=True)        flags = [            'NoData',             'Valid',             'Cloud',             'Shadow',             'Snow',             'Water'            ]        par_fmask_flags.filter.type = 'ValueList'                par_fmask_flags.filter.list = flags        par_fmask_flags.values = ['Valid', 'Snow', 'Water']                # max cloud cover        par_max_cloud = arcpy.Parameter(                          displayName='Maximum cloud cover',                          name='in_max_cloud',                          datatype='GPDouble',                          parameterType='Required',                          direction='Input',                          category='Satellite Quality Options',                          multiValue=False)        par_max_cloud.filter.type = 'Range'        par_max_cloud.filter.list = [0.0, 100.0]        par_max_cloud.value = 0.0                # input interpolate        par_interpolate = arcpy.Parameter(                            displayName='Interpolate NoData pixels',                            name='in_interpolate',                            datatype='GPBoolean',                            parameterType='Required',                            direction='Input',                            category='Satellite Quality Options',                            multiValue=False)        par_interpolate.value = True                # combine parameters        parameters = [            par_in_feat,            par_ongoing,            par_time_interval,            par_email_from,            par_smtp_server,            par_smtp_port,            par_smtp_username,            par_smtp_password,            par_output_epsg,            par_fmask_flags,            par_max_cloud,            par_interpolate        ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the NRT Monitor Areas module.        """                # set gdal global environ        import os        os.environ['GDAL_DISABLE_READDIR_ON_OPEN'] = 'EMPTY_DIR'        os.environ['CPL_VSIL_CURL_ALLOWED_EXTENSIONS '] = 'tif'        os.environ['VSI_CACHE '] = 'TRUE'        os.environ['GDAL_HTTP_MULTIRANGE '] = 'YES'        os.environ['GDAL_HTTP_MERGE_CONSECUTIVE_RANGES '] = 'YES'                # also set rasterio env variables        rasterio_env = {            'GDAL_DISABLE_READDIR_ON_OPEN': 'EMPTY_DIR',            'CPL_VSIL_CURL_ALLOWED_EXTENSIONS':'tif',            'VSI_CACHE': True,            'GDAL_HTTP_MULTIRANGE': 'YES',            'GDAL_HTTP_MERGE_CONSECUTIVE_RANGES': 'YES'        }        # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)                # safe imports        import sys                      # arcgis comes with these        import shutil                   # arcgis comes with these        import datetime                 # arcgis comes with these        import numpy as np              # arcgis comes with these        import arcpy                    # arcgis comes with these        import tempfile                 # arcgis comes with these        from datetime import datetime   # arcgis comes with these                # risky imports (not native to arcgis)        try:            import xarray as xr            import dask            import rasterio            import pystac_client            import osr            from odc import stac            from osgeo import gdal            from osgeo import ogr            from osgeo import osr        except:            arcpy.AddError('Python libraries xarray, dask, rasterio, pystac, or odc not installed.')            raise                    # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools            # module folder            sys.path.append(FOLDER_MODULES)            import nrt, cog_odc, cog        except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            raise                             # grab parameter values         in_feat = parameters[0]                            # input monitoring area polygon features        in_ongoing = parameters[1].value                   # perform ongoing monitoring        in_time_interval = parameters[2].value             # hours between checks        in_email_from = parameters[3].value                # email from         in_smtp_server = parameters[4].value               # email smtp server         in_smtp_port = parameters[5].value                 # email smtp port         in_smtp_username = parameters[6].value             # email smtp username         in_smtp_password = parameters[7].value             # email smtp password         in_epsg = parameters[8].value                      # input epsg        in_fmask_flags = parameters[9].valueAsText         # fmask flag values        in_max_cloud = parameters[10].value                # max cloud percentage        in_interpolate = parameters[11].value              # interpolate missing pixels        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning NRT Monitoring of areas.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=20)        # convert fmask as text to numeric code equivalents              in_fmask_flags = [e for e in in_fmask_flags.split(';')]                in_fmask_flags = arc.convert_fmask_codes(in_fmask_flags)                # check if cloud cover is valid        if in_max_cloud < 0 or in_max_cloud > 100:            arcpy.AddError('Cloud cover must be between 0 and 100.')            raise                    # check if time interval is > 0        in_time_interval = in_time_interval * 60 * 60        if in_time_interval <= 0:            arcpy.AddError('Time interval must be above 0 hours.')            raise                    # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Checking parameters...')        arcpy.SetProgressorPosition(1)        # prepare features shapefile        shp_desc = arcpy.Describe(in_feat)        in_feat = os.path.join(shp_desc.path, shp_desc.name)                                    # validate monitoring area feature class        if not nrt.validate_monitoring_areas(in_feat):            arcpy.AddError('Monitoring areas feature is invalid.')            raise                    # get input featureclass file, get dir and filename        in_name = os.path.basename(in_feat)     # name of monitor fc        in_gdb = os.path.dirname(in_feat)       # path of gdb                # check gdv extension        if not in_gdb.endswith('.gdb'):            arcpy.AddError('Feature class is not in a geodatabase.')            raise        else:            in_path = os.path.splitext(in_gdb)[0]   # path of gdb without ext            in_data_path = in_path + '_' + 'cubes'  # associated cube data folder                    # check if cubes folder exists        if not os.path.exists(in_data_path):            arcpy.AddError('Could not find cube folder for selected monitoring areas.')            raise                        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading monitoring area features...')        arcpy.SetProgressorPosition(2)                # get features (will always have at least one, as we validated earlier)        try:            driver = ogr.GetDriverByName('OpenFileGDB')             data_source = driver.Open(os.path.dirname(in_feat), 0)            feats = data_source.GetLayer('monitoring_areas')        except:            raise ValueError('Could not open monitoring areas feature.')                        # # # # #        # notify and set progress bar to defaukt        arcpy.SetProgressor(type='default', message='Iterating through monitoring areas...')                        # begin monitoring process iteration        continue_monitoring = True        while continue_monitoring:            for feat in feats:                            # # # # #                # notify                 arcpy.AddMessage('Preparing parameters for area: {}...'.format(feat['area_id']))                                # set up expected netcdf file name and path                out_nc = os.path.join(in_data_path, 'cube' + '_' + feat['area_id'] + '.nc')                                # get start year and latest year for end from dataset                 s_year = '{}-01-01'.format(feat['s_year'])                e_year = '{}-12-31'.format(datetime.now().year)  # now                                # get parameters for platform                params = nrt.get_satellite_params(platform=feat['platform'])                                                 # transform from albers to wgs84 if needed                geom = feat.geometry()                geom_prj = ogr.CreateGeometryFromWkb(geom.ExportToIsoWkb())  # quick copy                if 'GDA_1994_Australia_Albers' not in geom_prj.ExportToWkt():                    geom_prj = nrt.reproject_ogr_geom(geom=geom_prj, from_epsg=3577, to_epsg=4326)                                    # get bbox in wgs84 from geometry                bbox = geom_prj.GetEnvelope()                bbox = [bbox[0], bbox[2], bbox[1], bbox[3]]                                                 # # # # #                # notify                 arcpy.AddMessage('Checking monitoring area: {}...'.format(feat['area_id']))                                # check if feature is valid                is_valid = nrt.validate_monitoring_area(area_id=feat['area_id'],                                                        platform=feat['platform'],                                                         s_year=feat['s_year'],                                                         e_year=feat['e_year'],                                                         index=feat['index'])                                                                        # check if monitoring area is valid                if not is_valid:                    arcpy.AddWarning('Invalid monitoring area: {}, skipping.'.format(feat['area_id']))                    continue                # # # # #                # notify                 arcpy.AddMessage('Creating / Syncing monitoring area cube: {}...'.format(feat['area_id']))                                # open existing cube if exists, get date information                ds_existing = None                if os.path.exists(out_nc):                    try:                        ds_existing = xr.open_dataset(out_nc)                        num_times = len(ds_existing['time'])                        s_year = ds_existing.isel(time=-1)                        s_year = str(s_year['time'].dt.strftime('%Y-%m-%d').values)                    except:                        arcpy.AddWarning('Could not open cube: {}, skipping.'.format(feat['area_id']))                        continue                                # sync cube to now                ds = nrt.sync_nrt_cube(out_nc=out_nc,                                       collections=params.get('collections'),                                       bands=params.get('bands'),                                       start_dt=s_year,                                       end_dt=e_year,                                       bbox=bbox,                                       in_epsg=in_epsg,                                       slc_off=False,                                       resolution=params.get('resolution'),                                       ds_existing=ds_existing,                                       chunks={})                                                                                         # download, overwrite existing, and compute                with rasterio.Env(**rasterio_env):                    tools.export_xr_as_nc(ds=ds.astype('int16'), filename=out_nc)                    ds = ds.compute()                                                        # if existingt was found, notify and close                 if ds_existing is not None:                    arcpy.AddMessage('Added {} images to cube.'.format(len(ds['time']) - num_times))                # # # # #                # notify                 arcpy.AddMessage('Extracting attributes from cube...')                                # get dataset and band attributes                ds_attrs = ds.attrs                ds_band_attrs = ds[list(ds.data_vars)[0]].attrs                ds_spatial_ref_attrs = ds['spatial_ref'].attrs                                    # # # # #                # notify                 arcpy.AddMessage('Removing invalid pixels and empty dates...')                                # check if expected band name exists                mask_band = arc.get_name_of_mask_band(list(ds.data_vars))                # remove invalid pixels and empty scenes                ds = cog.remove_fmask_dates(ds=ds,                                             valid_class=in_fmask_flags,                                             max_invalid=in_max_cloud,                                             mask_band=mask_band,                                             nodata_value=np.nan,                                             drop_fmask=True)                # # # # #                # notify                 arcpy.AddMessage('Calculating vegetation index: {}...'.format(feat['index']))                                # conform dea aws band names based on platform                ds = satfetcher.conform_dea_ard_band_names(ds=ds, platform=feat['platform'].lower())                                 # calculate vegetation index                 ds = tools.calculate_indices(ds=ds,                                              index=feat['index'].lower(),                                              custom_name='veg_idx',                                              rescale=False,                                              drop=True)                # append original attributes on to new band                ds['veg_idx'].attrs = ds_band_attrs                                arcpy.AddMessage(ds)                                                                # # # # #                # notify                 arcpy.AddMessage('Resampling cube...TODO!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')                                # todo                 # todo                 # todo                 # # # # #                # notify                 arcpy.AddMessage('Building and applying edge mask...')                                arcpy.AddMessage(ds)                                                # convert feature to layer and use to mask                geom = ogr.Open(feat.ExportToJson(), 0)                lyr = geom.GetLayer()                mask = nrt.mask_xr_via_polygon(geom=lyr,                                                x=ds['x'].data,                                                y=ds['y'].data,                                                bbox=ds.geobox.extent.boundingbox,                                                transform=ds.geobox.transform,                                                ncols=len(ds['x']),                                                nrows=len(ds['y']),                                                mask_value=1)                                                               arcpy.AddMessage(mask)                # apply mask to current dataset, set everything outside to nan                ds = ds.where(mask)                                arcpy.AddMessage(ds)                                # # # # #                # notify                 arcpy.AddMessage('Performing static and dynamic change detection...')                                #ds.to_netcdf(r'C:\Users\Lewis\Desktop\test_change\change_{}_trained.nc'.format(feat['area_id']))                #raise                                                # perform static, dynamic and summary methods                 ds_change = nrt.build_change_cube(ds,                                                   training_start_year=int(feat['s_year']),                                                   training_end_year=int(feat['e_year']))                                # # # # #                # notify                 arcpy.AddMessage('Cleaning up static and dynamic change detection outputs...')                # re-mask summary and change cubes as nan pixels now set to non-nan                ds_change = ds_change.where(mask)                                # append attributes back on to dataset                 ds_change.attrs = ds_attrs                ds_change['spatial_ref'].attrs = ds_spatial_ref_attrs                for var in list(ds_change.data_vars):                    ds_change[var].attrs = ds_band_attrs                                    # temp                               ds_change.to_netcdf(r'C:\Users\Lewis\Desktop\test_change\change_{}.nc'.format(feat['area_id']))                                                # # # # #                # notify                 arcpy.AddMessage('Applying traffic light algorithm...')                # todo                 # todo                # todo                                                 # # # # #                # email user alert if detected and requested                if feat['alert'] == 'Yes':                                        # if traffic light == True:                                    # notify                     arcpy.AddMessage('Emailing alrert...')                                        #in_email_from = parameters[3].value                # email from                     #in_smtp_server = parameters[4].value               # email smtp server                     #in_smtp_port = parameters[5].value                 # email smtp port                     #in_smtp_username = parameters[6].value             # email smtp username                     #in_smtp_password = parameters[7].value             # email smtp password                                         # send email                    #nrt.send_email_alert(sent_from='mrlewie@outlook.com',                                          #sent_to='mrlewie@outlook.com',                                          #subject='Area ID: {} has been flagged.'.format(feat['area_id']),                                          #body_text='Area ID: {} has been found to have a decline. Please check.'.format(feat['area_id']),                                          #smtp_server='smtp.office365.com',                                          #smtp_port=587,                                          #username='mrlewie@outlook.com',                                          #password='halfLife1985micr')                # close dataset                  ds.close()                ds_change.close()                del ds                del ds_change                                # safely close existing                if ds_existing is not None:                    ds_existing.close()                    del ds_existing                                                                        # set processing to false if not requested to end this            in_time_interval = 0.01                                    if in_ongoing:                arcpy.AddMessage('Hibernating for {} hours. Press stop to cancel.'.format(in_time_interval / 60 / 60))                time.sleep(in_time_interval)            else:                continue_monitoring = False                            # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(3)        # notify user        arcpy.AddMessage('Monitoring areas synced successfully.')        returnclass Tool(object):    def __init__(self):        """Define the tool (tool name is the name of the class)."""        self.label = "Tool"        self.description = "Tool Template"        self.canRunInBackground = False    def getParameterInfo(self):        """Define parameter definitions"""                params = None        return params    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """The source code of the tool."""                            return