# -*- coding: utf-8 -*-# importsimport osimport uuidimport certifiimport arcpy# set default gdal and ertifi envs (dev)#os.environ['GDAL_DATA']  = r'C:\Program Files\ArcGIS\Pro\Resources\pedata\gdaldata'#os.environ.setdefault("CURL_CA_BUNDLE", certifi.where())# set default gdal and ertifi envs (non-dev)try:    install_dir = arcpy.GetInstallInfo().get('InstallDir')  # get arcgis install dir    os.environ['GDAL_DATA'] = os.path.join(install_dir, 'Resources\pedata\gdaldata')  # join to gdal install    os.environ.setdefault("CURL_CA_BUNDLE", certifi.where())  # set certifiexcept:    arcpy.AddError('Could not get install directory for ArcGIS Pro or set certifi.')    raise# get location of tenement-tool toolboxtbx_filename = os.path.realpath(__file__)tbx_folder = os.path.dirname(tbx_filename)folder = os.path.dirname(tbx_folder)# globals (non-dev)#FOLDER_MODULES = os.path.join(folder, 'modules')#FOLDER_SHARED = os.path.join(folder, 'shared')#GRP_LYR_FILE = os.path.join(folder, r'arc\lyr\group_template.lyrx')#MON_LYR_FILE = os.path.join(folder, r'arc\lyr\monitoring_zones_template.lyrx')# globals (dev)FOLDER_MODULES = r'C:\Users\Lewis\Documents\GitHub\tenement-tools\modules'  FOLDER_SHARED = r'C:\Users\Lewis\Documents\GitHub\tenement-tools\shared'GRP_LYR_FILE = r'C:\Users\Lewis\Documents\GitHub\tenement-tools\arc\lyr\group_template.lyrx'MON_LYR_FILE = r'C:\Users\Lewis\Documents\GitHub\tenement-tools\arc\lyr\monitoring_zones_template.lyrx'# globals (dea aws)STAC_ENDPOINT = 'https://explorer.sandbox.dea.ga.gov.au/stac/search'STAC_ENDPOINT_ODC = 'https://explorer.sandbox.dea.ga.gov.au/stac'AWS_KEY = ''AWS_SECRET = ''RESULT_LIMIT = 250# globalsGDVSPECTRA_THRESHOLD = {}             # persist gdv threshold valuesGDVSPECTRA_TREND = {}                 # persist gdv trend valuesGDVSPECTRA_CVA = {}                   # persist gdv cva valuesPHENOLOPY_METRICS = {}                # persist phenolopy metrics valuesENSEMBLE_SIGMOIDS = {}                # persist ensemble signoidal valuesENSEMBLE_MASKER = {}                  # persist ensemble masker valuesNRT_CREATE_AREA = {}                  # persist nrt create monitoring areaNRT_MODIFY_AREA = {}                  # persist nrt modify monitoring areaNRT_DELETE_AREA = {}                  # persist nrt delete monitoring areaclass Toolbox(object):    def __init__(self):        """Define the toolbox (the name of the toolbox is the name of the        .pyt file)."""           self.label = "Toolbox"        self.alias = "toolbox"        # list of tool classes associated with this toolbox        self.tools = [            COG_Fetch,             COG_Fetch_ODC,            COG_Explore,             GDVSpectra_Likelihood,             GDVSpectra_Threshold,             GDVSpectra_Trend,            GDVSpectra_CVA,            Phenolopy_Metrics,            Nicher_SDM,            Nicher_Masker,            VegFrax_Fractional_Cover,            Ensemble_Sigmoider,            Ensemble_Model,            Ensemble_Masker,            NRT_Create_Project,            NRT_Create_Monitoring_Areas,            NRT_Modify_Monitoring_Areas,            NRT_Delete_Monitoring_Areas,            NRT_Monitor_Areas,            NRT_Build_Graphs,            Test            ]# deprecatedclass COG_Fetch(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = "COG Fetch"        self.description = "COG contains functions that " \                           "allow for efficient download of " \                           "analysis ready data (ARD) Landsat " \                           "5, 7, 8 or Sentinel 2A, 2B images " \                           "from the Digital Earth Australia " \                           "(DEA) public AWS server."        self.canRunInBackground = False    def getParameterInfo(self):                # input study area shapefile        par_studyarea_feat = arcpy.Parameter(                                displayName="Input study area feature",                                name="in_studyarea_feat",                                datatype="GPFeatureLayer",                                parameterType="Required",                                direction="Input"                                )                                        # set study area to be polygon only        par_studyarea_feat.filter.list = ['Polygon']                                        # output file location        par_out_nc_path = arcpy.Parameter(                                displayName="Output NetCDF file",                                name="out_nc_path",                                datatype="DEFile",                                parameterType="Required",                                direction="Output"                                )                                        # set file type to be netcdf only        par_out_nc_path.filter.list = ['nc']        # in_platform        par_platform = arcpy.Parameter(                            displayName="Satellite platform",                            name="in_platform",                            datatype="GPString",                            parameterType="Required",                            direction="Input",                            multiValue=False                            )                                    # set default platform        par_platform.values = 'Landsat'        par_platform.filter.list = ['Landsat', 'Sentinel']  # 'Sentinel 2A', 'Sentinel 2B'                # in_from_date        par_date_from = arcpy.Parameter(                            displayName="Date from",                            name="in_from_date",                            datatype="GPDate",                            parameterType="Required",                            direction="Input",                            multiValue=False                            )                                    # set in_from_date value        par_date_from.values = '2015/01/01'                # in_to_date        par_date_to = arcpy.Parameter(                        displayName="Date to",                        name="in_to_date",                        datatype="GPDate",                        parameterType="Required",                        direction="Input",                        multiValue=False                        )        # set in_from_date value        par_date_to.values = '2020/12/31'        # set bands        par_bands = arcpy.Parameter(                        displayName="Bands",                        name="in_bands",                        datatype="GPString",                        parameterType="Required",                        direction="Input",                        category='Satellite Bands',                        multiValue=True                        )                         # set landsat bands        bands = [            'Blue',             'Green',             'Red',             'NIR',             'SWIR1',             'SWIR2',             'OA_Mask'            ]                # set default bands        par_bands.filter.type = "ValueList"                par_bands.filter.list = bands        par_bands.values = bands                # set slc-off        par_slc_off = arcpy.Parameter(                        displayName="SLC Off",                        name="in_slc_off",                        datatype="GPBoolean",                        parameterType="Required",                        direction="Input",                        multiValue=False                        )                # set slc-off value        par_slc_off.value = False                       # set output datatype        par_output_dtype = arcpy.Parameter(                            displayName="Output data type",                            name="in_output_dtype",                            datatype="GPString",                            parameterType="Required",                            direction="Input",                            category='Warping Options',                            multiValue=False                            )                                    # set default platform        par_output_dtype.filter.list = ['int8', 'int16', 'float32', 'float64']        par_output_dtype.values = 'int16'                # todo make this changeh when sent/landsat changed        # set output resolution        par_output_res = arcpy.Parameter(                            displayName="Output pixel resolution",                            name="in_output_res",                            datatype="GPLong",                            parameterType="Required",                            direction="Input",                            category='Warping Options',                            multiValue=False                            )                                    # set default platform        par_output_res.filter.type = 'Range'        par_output_res.filter.list = [0, 1000]        par_output_res.value = 30         # todo allow this to handle np.nan        # set output nodata value        par_output_fill_value = arcpy.Parameter(                                    displayName="Output NoData value",                                    name="in_output_fill_value",                                    datatype="GPString",                                    parameterType="Required",                                    direction="Input",                                    category='Warping Options',                                    multiValue=False                                    )                                    # set default nodata value        par_output_fill_value.value = "-999"                # set output epsg        par_output_epsg = arcpy.Parameter(                            displayName="Output EPSG",                            name="in_output_epsg",                            datatype="GPLong",                            parameterType="Required",                            direction="Input",                            category='Warping Options',                            multiValue=False                            )                                    # set default epsg        par_output_epsg.filter.list = [3577]        par_output_epsg.values = 3577                # set resampling type        par_output_resampling = arcpy.Parameter(                            displayName="Resampling type",                            name="in_output_resampling",                            datatype="GPString",                            parameterType="Required",                            direction="Input",                            category='Warping Options',                            multiValue=False                            )                                    # set default resampling        par_output_resampling.filter.list = ['Nearest', 'Bilinear']        par_output_resampling.values = 'Nearest'                # set snap boundary        par_output_snap = arcpy.Parameter(                            displayName="Snap boundaries",                            name="in_snap_bounds",                            datatype="GPBoolean",                            parameterType="Required",                            direction="Input",                            category='Warping Options',                            multiValue=False                            )                # set snap boundary value        par_output_snap.value = True                # set rescale        par_output_rescale = arcpy.Parameter(                        displayName="Rescale",                        name="in_rescale",                        datatype="GPBoolean",                        parameterType="Required",                        direction="Input",                        category='Warping Options',                        multiValue=False                        )                # set rescale value        par_output_rescale.value = True                # set cell alignment        par_output_cell_align = arcpy.Parameter(                            displayName="Cell alignment",                            name="in_output_cell_align",                            datatype="GPString",                            parameterType="Required",                            direction="Input",                            category='Warping Options',                            multiValue=False                            )                                    # set default cell align        par_output_cell_align.filter.list = ['Top-left', 'Center']        par_output_cell_align.values = 'Top-left'                # set chunks        par_output_chunk_size = arcpy.Parameter(                            displayName="Chunk size",                            name="in_output_chunk_size",                            datatype="GPLong",                            parameterType="Required",                            direction="Input",                            category='Parallelisation',                            multiValue=False                            )                                    # set default chunksize        par_output_chunk_size.value = -1                # set dea aws stac url        par_output_stac_url = arcpy.Parameter(                                displayName="Digital Earth Australia STAC URL",                                name="in_output_stac_url",                                datatype="GPString",                                parameterType="Required",                                direction="Input",                                category='STAC Options',                                multiValue=False                                )                 # set default dea aws stac url        par_output_stac_url.value = STAC_ENDPOINT        # set dea aws key        par_output_aws_key = arcpy.Parameter(                                displayName="Digital Earth Australia AWS Key",                                name="in_output_aws_key",                                datatype="GPString",                                parameterType="Optional",                                direction="Input",                                category='STAC Options',                                multiValue=False                                )                 # set default dea aws key value        par_output_aws_key.value = AWS_KEY         # set dea aws secret        par_output_aws_secret = arcpy.Parameter(                                displayName="Digital Earth Australia AWS Secret Key",                                name="in_output_aws_secret",                                datatype="GPString",                                parameterType="Optional",                                direction="Input",                                category='STAC Options',                                multiValue=False                                )                 # set default dea aws secret value        par_output_aws_secret.value = AWS_SECRET                # combine parameters        parameters = [            par_studyarea_feat,            par_out_nc_path,            par_platform,            par_date_from,            par_date_to,            par_bands,            par_slc_off,            par_output_dtype,            par_output_res,            par_output_fill_value,            par_output_epsg,            par_output_resampling,            par_output_snap,            par_output_rescale,            par_output_cell_align,            par_output_chunk_size,            par_output_stac_url,            par_output_aws_key,            par_output_aws_secret        ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # modify bands list when platform changed        if parameters[2].value == 'Landsat' and not parameters[2].hasBeenValidated:                    # enable slc-off control            parameters[6].enabled = True                        # update landsat band list            bands = [                'Blue',                 'Green',                 'Red',                 'NIR',                 'SWIR1',                 'SWIR2',                 'OA_Mask'                ]                        # update bands and set to default resolution            parameters[5].filter.list = bands            parameters[5].values = bands            parameters[8].value = 30        elif 'Sentinel' in parameters[2].value and not parameters[2].hasBeenValidated:                    # disable slc-off control            parameters[6].enabled = False                        # update sentinel band list            bands = [                'Blue',                 'Green',                 'Red',                 'NIR1',                 'SWIR2',                 'SWIR3',                 'OA_Mask'                ]                        # update values in control            parameters[5].filter.list = bands            parameters[5].values = bands                        # set resolution to original 10x10m            parameters[8].value = 10        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the GDV Spectra Likelihood module.        """                # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)                                # safe imports        import os, sys        import io        import time        import numpy as np        import arcpy                # risky imports (not native to arcgis)        try:            import xarray as xr            import dask            import dask.array as da        except:            arcpy.AddError('Python libraries xarray, dask not installed.')            return        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, tools, satfetcher                    # module folder            sys.path.append(FOLDER_MODULES)            import cog        except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return                # notify         arcpy.AddMessage('Performing COG Fetch.')                                                    # grab parameter values         in_studyarea_feat = parameters[0].value         # study area feat         out_nc = parameters[1].valueAsText              # output nc         in_platform = parameters[2].value               # platform name        in_from_date = parameters[3].value              # from date        in_to_date = parameters[4].value                # to date        in_bands = parameters[5].valueAsText            # bands        in_slc_off = parameters[6].value                # slc off         in_dtype = parameters[7].value                  # output pixel dtype        in_res = parameters[8].value                    # output pixel resolution        in_fill_value = parameters[9].value             # todo processing string to int, float or np.nan        in_epsg = parameters[10].value                  # output epsg         in_resampling = parameters[11].value            # output resampler method         in_snap = parameters[12].value                  # output snap alignment         in_rescale = parameters[13].value               # output rescale        in_cell_align = parameters[14].value            # output cell alignmnent             in_chunk_size = parameters[15].value            # chunk size        in_stac_endpoint = parameters[16].value         # stac endpoint        in_aws_key = parameters[17].value               # dea aws key        in_aws_secret = parameters[18].value            # dea aws secret                # let user know that aws key and secret not yet implemented        if in_aws_key is not None or in_aws_secret is not None:            arcpy.AddWarning('AWS Credentials not yet supported. Using DEA AWS.')                # set up progess bar        arcpy.SetProgressor(type='default', message='Preparing query parameters...')                        # get minimum bounding geom from input         bbox = arc.get_selected_layer_extent(in_studyarea_feat)                # get collections based on platform         collections = arc.prepare_collections_list(in_platform)                    # prepare start and end date times        in_from_date = arc.datetime_to_string(in_from_date)        in_to_date = arc.datetime_to_string(in_to_date)                # fetch stac data         arcpy.SetProgressorLabel('Performing STAC query...')        feats = cog.fetch_stac_data(stac_endpoint=in_stac_endpoint,                                     collections=collections,                                     start_dt=in_from_date,                                     end_dt=in_to_date,                                     bbox=bbox,                                    slc_off=in_slc_off,                                    limit=RESULT_LIMIT)                # count number of items        arcpy.AddMessage('Found {} {} scenes.'.format(len(feats), in_platform))        # prepare band (i.e. stac assets) names        assets = arc.prepare_band_names(in_bands=in_bands,                                         in_platform=in_platform)                                                            # convert raw stac into dict with coord reproject, etc.        arcpy.SetProgressorLabel('Converting STAC data into useable format...')        meta, asset_table = cog.prepare_data(feats,                                              assets=assets,                                             bounds_latlon=bbox,                                              bounds=None,                                              epsg=in_epsg,                                              resolution=in_res,                                              snap_bounds=in_snap,                                             force_dea_http=True)                                                     # prepare resample and fill value types        resampling = in_resampling.lower()        fill_value = arc.prepare_fill_value_type(in_fill_value)                                                                                                  # convert assets to dask array        arcpy.SetProgressorLabel('Parallelising data...')        darray = cog.convert_to_dask(meta=meta,                                      asset_table=asset_table,                                      chunksize=in_chunk_size,                                     resampling=resampling,                                      dtype=in_dtype,                                      fill_value=fill_value,                                      rescale=in_rescale)                                             # prepare alignment type        cell_align = arc.prepare_cell_align_type(in_cell_align)        # generate coordinates and dimensions from metadata        arcpy.SetProgressorLabel('Building dataset metadata...')        coords, dims = cog.build_coords(feats=feats,                                        assets=assets,                                         meta=meta,                                        pix_loc=cell_align)                # build final xarray data array        arcpy.SetProgressorLabel('Finalising dataset...')        ds_name = 'stac-' + dask.base.tokenize(darray)        ds = xr.DataArray(darray,                          coords=coords,                          dims=dims,                          name=ds_name                          )                                 # comvert to cleaner xarray dataset        ds = ds.to_dataset(dim='band')                # append attributes onto dataset        ds = cog.build_attributes(ds=ds,                                  meta=meta,                                   collections=collections,                                   bands=assets,                                  slc_off=in_slc_off,                                   bbox=bbox,                                  dtype=in_dtype,                                  snap_bounds=in_snap,                                  fill_value=fill_value,                                   rescale=in_rescale,                                  cell_align=in_cell_align,                                  resampling=in_resampling)                                             # set up proper progress bar        arcpy.SetProgressor(type='step',                             message='Preparing data download...',                             min_range=0,                             max_range=len(ds.data_vars) + 1)        # get list of dataset vars and iterate compute on each        for counter, data_var in enumerate(list(ds.data_vars), start=1):                    # start clock            start = time.time()                    # update progress bar            arcpy.SetProgressorLabel('Downloading band: {}...'.format(data_var))            arcpy.SetProgressorPosition(counter)                    # compute!            ds[data_var] = ds[data_var].compute()                        # notify time             duration = round(time.time() - start, 2)            arcpy.AddMessage('Band: {} took: {}s to download.'.format(data_var, duration))                                          # wrap up         arcpy.SetProgressorLabel('Exporting NetCDF...')        arcpy.SetProgressorPosition(counter + 1)                        # export netcdf to output folder        tools.export_xr_as_nc(ds=ds, filename=out_nc)        # notify finish        arcpy.AddMessage('COG Fetch completed successfully.')                return# implement resamplingclass COG_Fetch_ODC(object):    def __init__(self):        """        Initialise tool.        """            # set tool name, description, options        self.label = 'COG Fetch ODC'        self.description = 'COG Fetch implements the COG Open ' \                           'Data Cube (ODC) STAC module created ' \                           'by Digital Earth Australia (DEA). ' \                           'This allows easy and efficient ' \                           'downloading of analysis-ready Landsat ' \                           '5, 7, 8 and Sentinel 2 satellite imagery ' \                           'for any area in Australia.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """                # input feature        par_in_feat = arcpy.Parameter(                        displayName='Input area of interest feature',                        name='in_feat',                        datatype='GPFeatureLayer',                        parameterType='Required',                        direction='Input')        par_in_feat.filter.list = ['Polygon']                                        # output file        par_out_nc = arcpy.Parameter(                       displayName='Output satellite NetCDF file',                       name='out_nc_path',                       datatype='DEFile',                       parameterType='Required',                       direction='Output')        par_out_nc.filter.list = ['nc']        # platform        par_platform = arcpy.Parameter(                         displayName='Satellite platform',                         name='in_platform',                         datatype='GPString',                         parameterType='Required',                         direction='Input',                         multiValue=False)        par_platform.filter.list = ['Landsat', 'Sentinel']        par_platform.values = 'Landsat'                # include slc off        par_slc_off = arcpy.Parameter(                        displayName='Include "SLC-off" data',                        name='in_slc_off',                        datatype='GPBoolean',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_slc_off.value = False                # start date        par_date_start = arcpy.Parameter(                          displayName='Start date',                          name='in_from_date',                          datatype='GPDate',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_date_start.values = '2018/01/01'                # end date        par_date_end = arcpy.Parameter(                        displayName='End date',                        name='in_to_date',                        datatype='GPDate',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_date_end.values = '2021/12/31'        # bands        par_bands = arcpy.Parameter(                      displayName='Bands',                      name='in_bands',                      datatype='GPString',                      parameterType='Required',                      direction='Input',                      multiValue=True)        bands = [            'Blue',             'Green',             'Red',             'NIR',             'SWIR1',             'SWIR2',             'OA_Mask'            ]        par_bands.filter.type = 'ValueList'                par_bands.filter.list = bands        par_bands.values = bands        # resolution        par_res = arcpy.Parameter(                    displayName='Resolution',                    name='in_res',                    datatype='GPLong',                    parameterType='Required',                    direction='Input',                    multiValue=False)        par_res.filter.type = 'Range'        par_res.filter.list = [1, 10000]        par_res.value = 30                # resampling        par_resampling = arcpy.Parameter(                           displayName='Resampling method',                           name='in_resampling',                           datatype='GPString',                           parameterType='Required',                           direction='Input',                           multiValue=False)        par_resampling.filter.list = ['Nearest', 'Bilinear', 'Cubic', 'Average']        par_resampling.values = 'Nearest'                # alignment        par_align = arcpy.Parameter(                      displayName='Alignment',                      name='in_align',                      datatype='GPLong',                      parameterType='Optional',                      direction='Input',                      multiValue=False)        par_align.filter.type = 'Range'        par_align.filter.list = [0, 10000]        par_align.value = None        # combine parameters        parameters = [            par_in_feat,            par_out_nc,            par_platform,            par_slc_off,            par_date_start,            par_date_end,            par_bands,            par_res,            par_resampling,            par_align        ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # if satellite platform has been validated...        if parameters[2].hasBeenValidated is False:                    # modify bands list when platform changed            if parameters[2].value == 'Landsat':                            # enable slc-off control                parameters[3].enabled = True                            # set band list                bands = [                    'Blue',                     'Green',                     'Red',                     'NIR',                     'SWIR1',                     'SWIR2',                     'OA_Mask'                    ]                                    # set bands list and select all                parameters[6].filter.list = bands                parameters[6].values = bands                # set default resolution                parameters[7].value = 30            elif 'Sentinel' in parameters[2].value:                                # disable slc-off control                parameters[3].enabled = False                                # set band list                bands = [                    'Blue',                     'Green',                     'Red',                     'NIR1',                     'SWIR2',                     'SWIR3',                     'OA_Mask'                    ]                # set bands list and select all                parameters[6].filter.list = bands                parameters[6].values = bands                # set default resolution                parameters[7].value = 10        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the COG Fetch (ODC) module.        """                # safe imports        import os, sys                   # arcgis comes with these        import numpy as np               # arcgis comes with these        import arcpy                     # arcgis comes with these                # set rasterio env         rio_env = {            'GDAL_DISABLE_READDIR_ON_OPEN': 'EMPTY_DIR',            'CPL_VSIL_CURL_ALLOWED_EXTENSIONS': 'tif',            'VSI_CACHE': 'TRUE',            'GDAL_HTTP_MULTIRANGE': 'YES',            'GDAL_HTTP_MERGE_CONSECUTIVE_RANGES': 'YES'            }                # apply rio env settings         for k, v in rio_env.items():            os.environ[k] = v                # risky imports (not native to arcgis)        try:            import xarray as xr            import dask            import rasterio            import pystac_client            from odc import stac        except Exception as e:            arcpy.AddError('Python libraries xarray, dask, rasterio, odc not installed.')            arcpy.AddMessage(str(e))            return                    # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                    # module folder            sys.path.append(FOLDER_MODULES)            import cog_odc        except Exception as e:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            arcpy.AddMessage(str(e))            return                    # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        warnings.simplefilter(action='ignore', category=dask.array.core.PerformanceWarning)        # grab parameter values        in_feat = parameters[0].valueAsText             # study area feature        out_nc = parameters[1].valueAsText              # output nc         in_platform = parameters[2].value               # platform name        in_slc_off = parameters[3].value                # slc off         in_start_date = parameters[4].value             # start date        in_end_date = parameters[5].value               # end date        in_bands = parameters[6].valueAsText            # bands        in_res = parameters[7].value                    # resolution        in_resampling = parameters[8].value             # resampling method         in_align = parameters[9].value                  # alignment            # # # # #        # notify and start non-progress bar        arcpy.SetProgressor(type='default', message='Performing STAC query...')        # check if key parameters are valid        if in_feat is None:            arcpy.AddError('No input feature provided.')            return        if in_platform not in ['Landsat', 'Sentinel']:            arcpy.AddError('Platform is not supported.')            return        elif in_slc_off not in [True, False]:            arcpy.AddError('SLC off not provided.')            return        elif in_start_date is None or in_end_date is None:            arcpy.AddError('No start and/or end date provided.')            return        elif in_bands is None:            arcpy.AddError('No platform bands provided.')            return        try:            # prepare collections and bands            collections = arc.prepare_collections_list(in_platform)            bands = arc.prepare_band_names(in_bands, in_platform)                        # check collections and bands valid             if len(collections) == 0 or len(bands) == 0:                arcpy.AddError('Platform and/or bands not provided.')                return                        # prepare start, end dates            in_start_date = in_start_date.strftime('%Y-%m-%d')            in_end_date = in_end_date.strftime('%Y-%m-%d')                        # check date range si valid            if in_start_date >= in_end_date:                arcpy.AddError('End date must be greater than start date.')                return                        # get bbox (bl, tr) from layer in wgs84             bbox = arc.get_layer_bbox(in_feat)                           # check bbox is valid             if len(bbox) != 4:                arcpy.AddError('Bounding box is invalid.')                return                        # fetch stac items            items = cog_odc.fetch_stac_items_odc(stac_endpoint=STAC_ENDPOINT_ODC,                                                  collections=collections,                                                  start_dt=in_start_date,                                                  end_dt=in_end_date,                                                  bbox=bbox,                                                 slc_off=in_slc_off,                                                 limit=RESULT_LIMIT)                        # replace s3 prefix with https (pro-friendly)            items = cog_odc.replace_items_s3_to_https(items=items,                                                       from_prefix='s3://dea-public-data',                                                       to_prefix='https://data.dea.ga.gov.au')                        # notify user of number of images            arcpy.AddMessage('Found {} satellite items.'.format(len(items)))                 except Exception as e:            arcpy.AddError('Could not obtain items from DEA AWS, see messages for details.')            arcpy.AddMessage(str(e))            return        # ensure any items exist        if len(items) == 0:            arcpy.AddError('No satellite items returned.')            return        # # # # #        # notify and start non-progress bar        arcpy.SetProgressor(type='default', message='Converting items into dataset...')        # check if required parameters are valid        if in_res < 1:            arcpy.AddError('Resolution value must be > 0.')            return        elif in_resampling not in ['Nearest', 'Bilinear', 'Cubic', 'Average']:            arcpy.AddError('Resampling method not supported.')            return        elif in_align is not None and (in_align < 0 or in_align > in_res):            arcpy.AddError('Alignment must be > 0 but < resolution.')            return                try:            # convert items to xarray dataset            ds = cog_odc.build_xr_odc(items=items,                                      bbox=bbox,                                      bands=bands,                                      crs=3577,                                      res=in_res,                                      resampling=in_resampling,                                      align=in_align,                                      group_by='solar_day',                                      chunks={},                                      like=None)                except Exception as e:            arcpy.AddError('Could not construct xarray dataset, see messages for details.')            arcpy.AddMessage(str(e))            return                                            # # # # #        # notify user and set up progress bar        arcpy.SetProgressor(type='step',                             message='Downloading data, please wait...',                            min_range=0,                             max_range=len(ds) + 1)        try:            # iter dataset bands...            for idx, var in enumerate(ds):                            # increment progress bar                arcpy.SetProgressorLabel('Downloading band: {}...'.format(var))                arcpy.SetProgressorPosition(idx)                                # compute!                ds[var] = ds[var].compute()                except Exception as e:            arcpy.AddError('Could not download data, see messages for details.')            arcpy.AddMessage(str(e))            return                            # # # # #        # notify and start non-progress bar        arcpy.SetProgressor(type='default', message='Cleaning up dataset...')                try:            # force re-encoding of date time to prevent export bug            dts = ds['time'].dt.strftime('%Y-%m-%dT%H:%M:%S')            ds['time'] = dts.astype('datetime64[ns]')                        # set to signed int16             ds = ds.astype('int16')                    # set all non-mask bands nodata values (0) to -999            for var in ds:                if 'mask' not in var:                    ds[var] = ds[var].where(ds[var] != 0, -999)                except Exception as e:             arcpy.AddError('Could not finalise dataset, see messages for details.')            arcpy.AddMessage(str(e))            return                        # # # # #        # notify and start non-progress bar        arcpy.SetProgressor(type='default', message='Assigning extra attributes to dataset...')                try:            # set up additional attrs            attrs = {                'transform': tuple(ds.geobox.transform),                'res': in_res,                 'nodatavals': -999,                'orig_bbox': tuple(bbox),                'orig_collections': tuple(collections),                'orig_bands': tuple(bands),                'orig_dtype': 'int16',                'orig_slc_off': str(in_slc_off),                'orig_resample': in_resampling            }                        # assign attrs             ds = ds.assign_attrs(attrs)        except Exception as e:             arcpy.AddError('Could not assign extra attributes, see messages for details.')            arcpy.AddMessage(str(e))            return                                # # # # #        # notify and start non-progress bar        arcpy.SetProgressor(type='default', message='Exporting NetCDF file...')        try:            # export netcdf file            tools.export_xr_as_nc(ds=ds, filename=out_nc)        except Exception as e:             arcpy.AddError('Could not export dataset.')            arcpy.AddMessage(str(e))            return                # # # # #        # notify and start non-progress bar        arcpy.SetProgressor(type='default', message='Finalising process...')                # close main dataset and del datasets        ds.close()        del ds         # notify user        arcpy.AddMessage('Generated COG Fetch (ODC) successfully.')                return# deprecatedclass COG_Sync(object):    def __init__(self):            # set tool name        self.label = "COG Sync"                # set tool description        self.description = "Sync COG to update cube with latest " \                           "data."                                   # set false for pro        self.canRunInBackground = False    def getParameterInfo(self):            # input netcdf file        par_nc_file = arcpy.Parameter(                        displayName="Input NetCDF file",                        name="in_nc_file",                        datatype="DEFile",                        parameterType="Required",                        direction="Input"                        )                                        # set options        par_nc_file.filter.list = ['nc']                # combine parameters        parameters = [            par_nc_file            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):                # imports        import os, sys        #import io        #import time        import pandas as pd        import numpy as np        import xarray as xr        import arcpy        # import tools        sys.path.append(FOLDER_SHARED)        import arc, tools, satfetcher                # import gdvspectra module        sys.path.append(FOLDER_MODULES)        import cog                # globals         AWS_KEY = ''        AWS_SECRET = ''        STAC_ENDPOINT = 'https://explorer.sandbox.dea.ga.gov.au/stac/search'        RESULT_LIMIT = 250                # notify         arcpy.AddMessage('Performing COG Sync.')                                                    # grab parameter values         in_nc = parameters[0].valueAsText      # raw netcdf        # set up progess bar        arcpy.SetProgressor(type='default', message='Loading and checking netcdf...')                # load netcdf file as xr        ds = satfetcher.load_local_nc(nc_path=in_nc,                                       use_dask=True,                                       conform_nodata_to=np.nan)  #nodatavals?                # checks        if 'time' not in ds:            arcpy.AddError('No time dimension detected.')                #tod other checks                 # get original query attributes         arcpy.SetProgressorLabel('Getting original query parameters...')        # check attributes        in_bands = list(ds.data_vars)        collections = list(ds.orig_collections)        bbox = list(ds.orig_bbox)        in_res = ds.res # use get xr res method        crs = ds.crs        in_slc_off = ds.orig_slc_off        resampling = ds.orig_resample        nodatavals = ds.nodatavals        # need to do        in_epsg = int(crs.split(':')[1])        in_platform = 'Landsat'        dtype = 'int16'        fill_value = -999        in_snap = True        rescale = True        cell_align = 'Top-left'        chunk_size = -1                # get datetimes        arcpy.SetProgressorLabel('Assessing dates...')        # get now, earliest, latest datetimes in dataset        dt_now = np.datetime64('now')        dt_first = ds['time'].isel(time=0).values        dt_last = ds['time'].isel(time=-1).values        # conver to stac format        in_from_date = arc.datetime_to_string(pd.Timestamp(dt_last))        in_to_date = arc.datetime_to_string(pd.Timestamp(dt_now))        # check if xr dt less than now (will be for now, but not if override)        if dt_last < dt_now:                        # fetch cog            arcpy.SetProgressorLabel('Performing STAC query...')            feats = cog.fetch_stac_data(stac_endpoint=STAC_ENDPOINT,                                         collections=collections,                                         start_dt=in_from_date,                                         end_dt=in_to_date,                                         bbox=bbox,                                        slc_off=in_slc_off,                                        limit=RESULT_LIMIT)                                                    # count number of items            arcpy.AddMessage('Found {} {} scenes.'.format(len(feats), in_platform))                                # prepare band (i.e. stac assets) names            assets = in_bands            #assets = arc.prepare_band_names(in_bands=in_bands,                                             #in_platform=in_platform)                            # convert raw stac into dict with coord reproject, etc.            arcpy.SetProgressorLabel('Converting STAC data into useable format...')            meta, asset_table = cog.prepare_data(feats,                                                  assets=assets,                                                 bounds_latlon=bbox,                                                  bounds=None,                                                  epsg=in_epsg,                                                  resolution=in_res,                                                  snap_bounds=in_snap,                                                 force_dea_http=True)                                                                                    else:            arcpy.AddMessage('No new scenes available. No sync required.')            returnclass COG_Explore(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = 'COG Explore'        self.description = 'Explore an existing multidimensional raster layer ' \                           'downloaded prior using the COG Fetch tool.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """            # input netcdf data file        par_nc_file = arcpy.Parameter(                        displayName='Input satellite NetCDF file',                        name='in_nc_file',                        datatype='DEFile',                        parameterType='Required',                        direction='Input')        par_nc_file.filter.list = ['nc']        # output crf data folder        par_out_folder = arcpy.Parameter(                           displayName='Output folder',                           name='out_folder',                           datatype='DEFolder',                           parameterType='Required',                           direction='Input')        # input vegetation index         par_veg_idx = arcpy.Parameter(                        displayName='Vegetation index',                        name='in_veg_idx',                        datatype='GPString',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_veg_idx.filter.type = 'ValueList'        par_veg_idx.filter.list = [            'NDVI',            'EVI',             'SAVI',            'MSAVI',            'SLAVI',            'MAVI',            'kNDVI',            'TCG'            ]        par_veg_idx.value = 'MAVI'                # mask flags        par_fmask_flags = arcpy.Parameter(displayName='Include pixels flags',                            name='in_fmask_flags',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Satellite Quality Options',                            multiValue=True)        flags = [            'NoData',             'Valid',             'Cloud',             'Shadow',             'Snow',             'Water'            ]        par_fmask_flags.filter.type = 'ValueList'                par_fmask_flags.filter.list = flags        par_fmask_flags.values = ['Valid', 'Snow', 'Water']                        # max cloud cover        par_max_cloud = arcpy.Parameter(                          displayName='Maximum cloud cover',                          name='in_max_cloud',                          datatype='GPDouble',                          parameterType='Required',                          direction='Input',                          category='Satellite Quality Options',                          multiValue=False)        par_max_cloud.filter.type = 'Range'        par_max_cloud.filter.list = [0.0, 100.0]        par_max_cloud.value = 10.0                # input interpolate        par_interpolate = arcpy.Parameter(                            displayName='Interpolate NoData pixels',                            name='in_interpolate',                            datatype='GPBoolean',                            parameterType='Required',                            direction='Input',                            category='Satellite Quality Options',                            multiValue=False                            )        par_interpolate.value = True        # combine parameters        parameters = [            par_nc_file,            par_out_folder,            par_veg_idx,            par_fmask_flags,            par_max_cloud,            par_interpolate        ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the COG Explore module.        """            # safe imports        import os, sys       # arcgis comes with these        import numpy as np   # arcgis comes with these        import tempfile      # arcgis comes with this        import arcpy         # arcgis comes with these                # risky imports (not native to arcgis)        try:            import xarray as xr            import dask        except Exception as e:            arcpy.AddError('Python libraries xarray and dask not installed.')            arcpy.AddMessage(str(e))            return                  # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, tools, satfetcher                    # shared modules            sys.path.append(FOLDER_MODULES)            import cog        except Exception as e:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            arcpy.AddMessage(str(e))            return           # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        warnings.simplefilter(action='ignore', category=dask.array.core.PerformanceWarning)                                                           # grab parameter values         in_nc = parameters[0].valueAsText            # raw input satellite netcdf        out_folder = parameters[1].valueAsText       # output crf folder        in_veg_idx = parameters[2].value             # vege index name        in_fmask_flags = parameters[3].valueAsText   # fmask flag values        in_max_cloud = parameters[4].value           # max cloud percentage        in_interpolate = parameters[5].value         # interpolate missing pixels        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning COG Explore.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=11)        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading and checking netcdf...')        arcpy.SetProgressorPosition(1)        try:            # do quick lazy load of netcdf for checking            ds = xr.open_dataset(in_nc)        except Exception as e:            arcpy.AddError('Could not quick load input satellite NetCDF data.')            arcpy.AddMessage(str(e))            return        # check xr type, vars, coords, dims, attrs        if not isinstance(ds, xr.Dataset):            arcpy.AddError('Input NetCDF must be a xr dataset.')            return        elif len(ds) == 0:            arcpy.AddError('Input NetCDF has no data/variables/bands.')            return        elif 'x' not in ds.dims or 'y' not in ds.dims or 'time' not in ds.dims:            arcpy.AddError('Input NetCDF must have x, y and time dimensions.')            return        elif 'x' not in ds.coords or 'y' not in ds.coords or 'time' not in ds.coords:            arcpy.AddError('Input NetCDF must have x, y and time coords.')            return        elif 'spatial_ref' not in ds.coords:            arcpy.AddError('Input NetCDF must have a spatial_ref coord.')            return        elif len(ds['x']) == 0 or len(ds['y']) == 0 or len(ds['time']) == 0:            arcpy.AddError('Input NetCDF must have all at least one x, y and time index.')            return        elif 'oa_fmask' not in ds and 'fmask' not in ds:            arcpy.AddError('Expected cloud mask band not found in NetCDF.')            return        elif not hasattr(ds, 'time.year') or not hasattr(ds, 'time.month'):            arcpy.AddError('Input NetCDF must have time with year and month component.')            return        elif ds.attrs == {}:            arcpy.AddError('NetCDF must have attributes.')            return        elif not hasattr(ds, 'crs'):            arcpy.AddError('NetCDF CRS attribute not found. CRS required.')            return        elif ds.crs != 'EPSG:3577':            arcpy.AddError('NetCDF CRS is not in GDA94 Albers (EPSG:3577).')                        return         elif not hasattr(ds, 'nodatavals'):            arcpy.AddError('NetCDF nodatavals attribute not found.')                        return                 # efficient: if all nan, 0 at first var, assume rest same, so abort        if ds[list(ds)[0]].isnull().all() or (ds[list(ds)[0]] == 0).all():            arcpy.AddError('NetCDF has empty variables. Please download again.')                        return         try:            # now, do proper open of netcdf properly (and set nodata to nan)            ds = satfetcher.load_local_nc(nc_path=in_nc,                                           use_dask=True,                                           conform_nodata_to=np.nan)        except Exception as e:            arcpy.AddError('Could not properly load input satellite NetCDF data.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Getting NetCDF attributes...')        arcpy.SetProgressorPosition(2)        # get attributes from dataset        ds_attrs = ds.attrs        ds_band_attrs = ds[list(ds)[0]].attrs        ds_spatial_ref_attrs = ds['spatial_ref'].attrs                        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Grouping dates, if required...')        arcpy.SetProgressorPosition(3)                # remove potential datetime duplicates (group by day)        ds = satfetcher.group_by_solar_day(ds)        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Removing invalid pixels and empty dates...')        arcpy.SetProgressorPosition(4)          # convert fmask as text to numeric code equivalents        in_fmask_flags = [e for e in in_fmask_flags.split(';')]        in_fmask_flags = arc.convert_fmask_codes(in_fmask_flags)        # check if flags selected, if not, select all         if len(in_fmask_flags) == 0:            arcpy.AddWarning('No flags selected, using default.')            in_fmask_flags = [1, 4, 5]        # check numeric flags are valid         for flag in in_fmask_flags:            if flag not in [0, 1, 2, 3, 4, 5]:                arcpy.AddError('Pixel flag not supported.')                return        # check for duplicate flags         u, c = np.unique(in_fmask_flags, return_counts=True)        if len(u[c > 1]) > 0:            arcpy.AddError('Duplicate pixel flags detected.')            return        # get name of mask band        mask_band = arc.get_name_of_mask_band(list(ds))        try:            # remove invalid pixels and empty scenes            ds = cog.remove_fmask_dates(ds=ds,                                         valid_class=in_fmask_flags,                                         max_invalid=in_max_cloud,                                         mask_band=mask_band,                                         nodata_value=np.nan,                                         drop_fmask=True)        except Exception as e:            arcpy.AddError('Could not mask pixels.')            arcpy.AddMessage(str(e))            return                                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Conforming satellite band names...')        arcpy.SetProgressorPosition(5)        try:            # get platform name from attributes, error if no attributes            in_platform = arc.get_platform_from_dea_attrs(ds_attrs)            # conform dea aws band names based on platform            ds = satfetcher.conform_dea_ard_band_names(ds=ds,                                                        platform=in_platform.lower())           except Exception as e:             arcpy.AddError('Could not get platform from attributes.')            arcpy.AddMessage(str(e))            return        # check if all expected bands are in dataset         for band in ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']:            if band not in ds:                arcpy.AddError('NetCDF is missing band: {}. Need all bands.'.format(band))                return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Calculating vegetation index...')        arcpy.SetProgressorPosition(6)         # check if veg idx supported         if in_veg_idx.lower() not in ['ndvi', 'evi', 'savi', 'msavi', 'slavi', 'mavi', 'kndvi']:            arcpy.AddError('Vegetation index not supported.')            return         try:            # calculate vegetation index            ds = tools.calculate_indices(ds=ds,                                          index=in_veg_idx.lower(),                                          custom_name='veg_idx',                                          rescale=False,                                          drop=True)            # add band attrs back on            ds['veg_idx'].attrs = ds_band_attrs           except Exception as e:             arcpy.AddError('Could not calculate vegetation index.')            arcpy.AddMessage(str(e))            return                    # check if we sufficient data temporaly        if len(ds['time']) == 0:            arcpy.AddError('No dates remaining in data.')            return                      # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Computing data into memory, please wait...')        arcpy.SetProgressorPosition(7)        try:            # compute!             ds = ds.compute()        except Exception as e:             arcpy.AddError('Could not compute dataset. See messages for details.')            arcpy.AddMessage(str(e))            return            # check if all nan again        if ds.to_array().isnull().all():            arcpy.AddError('NetCDF is empty. Please download again.')                        return                 # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Interpolating dataset, if requested...')        arcpy.SetProgressorPosition(8)         # if requested...        if in_interpolate:            try:                # interpolate along time dimension (linear)                ds = tools.perform_interp(ds=ds, method='full')            except Exception as e:                 arcpy.AddError('Could not interpolate dataset.')                arcpy.AddMessage(str(e))                return                                                         # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(9)        # append attrbutes on to dataset and bands        ds.attrs = ds_attrs        ds['spatial_ref'].attrs = ds_spatial_ref_attrs        for var in ds:            ds[var].attrs = ds_band_attrs                               # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Adding output to map...')        arcpy.SetProgressorPosition(10)        try:            # for current project, open current map            aprx = arcpy.mp.ArcGISProject('CURRENT')            m = aprx.activeMap                    # remove explore layer if already exists            for layer in m.listLayers():                if layer.name == 'cog_explore.crf':                    m.removeLayer(layer)            # create temp netcdf (prevents 2.9 bug)            with tempfile.NamedTemporaryFile() as tmp:                tmp_nc = '{}_{}.nc'.format(tmp.name, 'cog_explore')                ds.to_netcdf(tmp_nc)            # disable visualise on map temporarily            arcpy.env.addOutputsToMap = False                    # create crf filename and copy it            out_file = os.path.join(out_folder, 'cog_explore.crf')            crf = arcpy.CopyRaster_management(in_raster=tmp_nc,                                               out_rasterdataset=out_file)            # add to map                              m.addDataFromPath(crf)           except Exception as e:            arcpy.AddWarning('Could not visualise output, aborting visualisation.')            arcpy.AddMessage(str(e))            pass                    try:            # get symbology, update it            layer = m.listLayers('cog_explore.crf')[0]            sym = layer.symbology            # if layer has stretch coloriser, apply color            if hasattr(sym, 'colorizer'):                if sym.colorizer.type == 'RasterStretchColorizer':                    # apply percent clip type                    sym.colorizer.stretchType = 'PercentClip'                    sym.colorizer.minPercent = 0.5                    sym.colorizer.maxPercent = 0.5                    # apply color map                    cmap = aprx.listColorRamps('Precipitation')[0]                    sym.colorizer.colorRamp = cmap                    # apply other basic options                    sym.colorizer.invertColorRamp = False                    sym.colorizer.gamma = 1.0                    # update symbology                    layer.symbology = sym        except Exception as e:            arcpy.AddWarning('Could not colorise output, aborting colorisation.')            arcpy.AddMessage(str(e))            pass                                            # # # # #        # clean up variables        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(11)        # close main dataset and del datasets        ds.close()        del ds         # notify user        arcpy.AddMessage('Generated COG Explore successfully.')        returnclass GDVSpectra_Likelihood(object):    def __init__(self):        """        Initialise tool.        """            # set tool name, description, options        self.label = 'GDVSpectra Likelihood'        self.description = 'GDVSpectra Likelihood derives potential groundwater ' \                           'dependent vegetation (GDV) areas from three or more years of ' \                           'Landsat or Sentinel NetCDF data. This functions results in ' \                           'a NetCDF of GDV likelihood with values ranging ' \                           'from 0 to 1, with 1 being highest probability of GDV.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """                # input netcdf data file        par_nc_file = arcpy.Parameter(                        displayName='Input satellite NetCDF file',                        name='in_nc_file',                        datatype='DEFile',                        parameterType='Required',                        direction='Input')        par_nc_file.filter.list = ['nc']                # output netcdf location        par_out_nc_file = arcpy.Parameter(                            displayName='Output GDV Likelihood NetCDF file',                            name='out_likelihood_nc_file',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_file.filter.list = ['nc']                # input wet month(s)         par_wet_months = arcpy.Parameter(                           displayName='Wet month(s)',                           name='in_wet_months',                           datatype='GPLong',                           parameterType='Required',                           direction='Input',                           category='Wet Period',                           multiValue=True)        par_wet_months.filter.type = 'ValueList'        par_wet_months.filter.list = [m for m in range(1, 13)]        par_wet_months.value = [1, 2, 3]                        # input dry month(s)        par_dry_months = arcpy.Parameter(                           displayName='Dry month(s)',                           name='in_dry_months',                           datatype='GPLong',                           parameterType='Required',                           direction='Input',                           category='Dry Period',                           multiValue=True)        par_dry_months.filter.type = 'ValueList'        par_dry_months.filter.list = [m for m in range(1, 13)]        par_dry_months.value = [9, 10, 11]                # input vegetation index         par_veg_idx = arcpy.Parameter(                        displayName='Vegetation index',                        name='in_veg_idx',                        datatype='GPString',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_veg_idx.filter.type = 'ValueList'        par_veg_idx.filter.list = [            'NDVI',            'EVI',             'SAVI',            'MSAVI',            'SLAVI',            'MAVI',            'kNDVI',            'TCG'            ]        par_veg_idx.value = 'MAVI'                # input moisture index         par_mst_idx = arcpy.Parameter(                        displayName='Moisture index',                        name='in_mst_idx',                        datatype='GPString',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_mst_idx.filter.type = 'ValueList'        par_mst_idx.filter.list = ['NDMI', 'GVMI']        par_mst_idx.value = 'NDMI'                # aggregate likelihood layers        par_aggregate = arcpy.Parameter(                          displayName='Combine outputs',                          name='in_aggregate',                          datatype='GPBoolean',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_aggregate.value = True                # pvalue for zscore        par_zscore_pvalue = arcpy.Parameter(                              displayName='Z-Score p-value',                              name='in_zscore_pvalue',                              datatype='GPDouble',                              parameterType='Optional',                              direction='Input',                              category='Outlier Correction',                              multiValue=False)        par_zscore_pvalue.filter.type = 'ValueList'        par_zscore_pvalue.filter.list = [0.01, 0.05, 0.1]        par_zscore_pvalue.value = None                       # q upper for standardisation        par_ivt_qupper = arcpy.Parameter(                           displayName='Upper percentile',                           name='in_stand_qupper',                           datatype='GPDouble',                           parameterType='Required',                           direction='Input',                           category='Invariant Standardisation',                           multiValue=False)        par_ivt_qupper.filter.type = 'Range'        par_ivt_qupper.filter.list = [0.0, 1.0]        par_ivt_qupper.value = 0.99                         # q lower for standardisation        par_ivt_qlower = arcpy.Parameter(                           displayName='Lower percentile',                           name='in_stand_qlower',                           datatype='GPDouble',                           parameterType='Required',                           direction='Input',                           category='Invariant Standardisation',                           multiValue=False)        par_ivt_qlower.filter.type = 'Range'        par_ivt_qlower.filter.list = [0.0, 1.0]        par_ivt_qlower.value = 0.05                                              # mask flags        par_fmask_flags = arcpy.Parameter(displayName='Include pixels flags',                            name='in_fmask_flags',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Satellite Quality Options',                            multiValue=True)        flags = [            'NoData',             'Valid',             'Cloud',             'Shadow',             'Snow',             'Water'            ]        par_fmask_flags.filter.type = 'ValueList'                par_fmask_flags.filter.list = flags        par_fmask_flags.values = ['Valid', 'Snow', 'Water']                # max cloud cover        par_max_cloud = arcpy.Parameter(                          displayName='Maximum cloud cover',                          name='in_max_cloud',                          datatype='GPDouble',                          parameterType='Required',                          direction='Input',                          category='Satellite Quality Options',                          multiValue=False)        par_max_cloud.filter.type = 'Range'        par_max_cloud.filter.list = [0.0, 100.0]        par_max_cloud.value = 10.0                # input interpolate        par_interpolate = arcpy.Parameter(                            displayName='Interpolate NoData pixels',                            name='in_interpolate',                            datatype='GPBoolean',                            parameterType='Required',                            direction='Input',                            category='Satellite Quality Options',                            multiValue=False)        par_interpolate.value = True                # input add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True        # combine parameters        parameters = [            par_nc_file,            par_out_nc_file,            par_wet_months,             par_dry_months,             par_veg_idx,             par_mst_idx,             par_aggregate,            par_zscore_pvalue,            par_ivt_qupper,            par_ivt_qlower,            par_fmask_flags,            par_max_cloud,            par_interpolate,            par_add_result_to_map            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""                        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the GDV Spectra Likelihood module.        """        # safe imports        import os, sys        # arcgis comes with these        import datetime       # arcgis comes with these        import numpy as np    # arcgis comes with these        import arcpy          # arcgis comes with these        # risky imports (not native to arcgis)        try:            import xarray as xr            import dask        except Exception as e:            arcpy.AddError('Python libraries xarray and dask not installed.')            arcpy.AddMessage(str(e))            return        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                    # module folder            sys.path.append(FOLDER_MODULES)            import gdvspectra, cog         except Exception as e:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            arcpy.AddMessage(str(e))            return                    # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        warnings.simplefilter(action='ignore', category=dask.array.core.PerformanceWarning)                 # grab parameter values         in_nc = parameters[0].valueAsText            # raw input satellite netcdf        out_nc = parameters[1].valueAsText           # output gdv likelihood netcdf        in_wet_months = parameters[2].valueAsText    # wet months         in_dry_months = parameters[3].valueAsText    # dry months         in_veg_idx = parameters[4].value             # vege index name        in_mst_idx = parameters[5].value             # moisture index name               in_aggregate = parameters[6].value           # aggregate output        in_zscore_pvalue = parameters[7].value       # zscore pvalue        in_ivt_qupper = parameters[8].value          # upper quantile for standardisation        in_ivt_qlower = parameters[9].value          # lower quantile for standardisation        in_fmask_flags = parameters[10].valueAsText  # fmask flag values        in_max_cloud = parameters[11].value          # max cloud percentage        in_interpolate = parameters[12].value        # interpolate missing pixels        in_add_result_to_map = parameters[13].value  # add result to map        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning GDVSpectra Likelihood.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=20)        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading and checking netcdf...')        arcpy.SetProgressorPosition(1)                try:            # do quick lazy load of netcdf for checking            ds = xr.open_dataset(in_nc)        except Exception as e:            arcpy.AddError('Could not quick load input satellite NetCDF data.')            arcpy.AddMessage(str(e))            return        # check xr type, vars, coords, dims, attrs        if not isinstance(ds, xr.Dataset):            arcpy.AddError('Input NetCDF must be a xr dataset.')            return        elif len(ds) == 0:            arcpy.AddError('Input NetCDF has no data/variables/bands.')            return        elif 'x' not in ds.dims or 'y' not in ds.dims or 'time' not in ds.dims:            arcpy.AddError('Input NetCDF must have x, y and time dimensions.')            return        elif 'x' not in ds.coords or 'y' not in ds.coords or 'time' not in ds.coords:            arcpy.AddError('Input NetCDF must have x, y and time coords.')            return        elif 'spatial_ref' not in ds.coords:            arcpy.AddError('Input NetCDF must have a spatial_ref coord.')            return        elif len(ds['x']) == 0 or len(ds['y']) == 0 or len(ds['time']) == 0:            arcpy.AddError('Input NetCDF must have all at least one x, y and time index.')            return        elif 'oa_fmask' not in ds and 'fmask' not in ds:            arcpy.AddError('Expected cloud mask band not found in NetCDF.')            return        elif not hasattr(ds, 'time.year') or not hasattr(ds, 'time.month'):            arcpy.AddError('Input NetCDF must have time with year and month component.')            return        elif len(ds.groupby('time.year')) < 3:            arcpy.AddError('Input NetCDF must have >= 3 years of data.')            return        elif ds.attrs == {}:            arcpy.AddError('NetCDF must have attributes.')            return        elif not hasattr(ds, 'crs'):            arcpy.AddError('NetCDF CRS attribute not found. CRS required.')            return        elif ds.crs != 'EPSG:3577':            arcpy.AddError('NetCDF CRS is not in GDA94 Albers (EPSG:3577).')                        return         elif not hasattr(ds, 'nodatavals'):            arcpy.AddError('NetCDF nodatavals attribute not found.')                        return                     # efficient: if all nan, 0 at first var, assume rest same, so abort        if ds[list(ds)[0]].isnull().all() or (ds[list(ds)[0]] == 0).all():            arcpy.AddError('NetCDF has empty variables. Please download again.')                        return         try:            # now, do proper open of netcdf properly (and set nodata to nan)            ds = satfetcher.load_local_nc(nc_path=in_nc,                                           use_dask=True,                                           conform_nodata_to=np.nan)        except Exception as e:            arcpy.AddError('Could not properly load input satellite NetCDF data.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Getting NetCDF attributes...')        arcpy.SetProgressorPosition(2)                    # get attributes from dataset        ds_attrs = ds.attrs        ds_band_attrs = ds[list(ds)[0]].attrs        ds_spatial_ref_attrs = ds['spatial_ref'].attrs                                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Grouping dates, if required...')        arcpy.SetProgressorPosition(3)                # remove potential datetime duplicates (group by day)        ds = satfetcher.group_by_solar_day(ds)                                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Removing invalid pixels and empty dates...')        arcpy.SetProgressorPosition(4)                  # convert fmask as text to numeric code equivalents        in_fmask_flags = [e for e in in_fmask_flags.split(';')]        in_fmask_flags = arc.convert_fmask_codes(in_fmask_flags)                # check if flags selected, if not, select all         if len(in_fmask_flags) == 0:            arcpy.AddWarning('No flags selected, using default.')            in_fmask_flags = [1, 4, 5]                # check numeric flags are valid         for flag in in_fmask_flags:            if flag not in [0, 1, 2, 3, 4, 5]:                arcpy.AddError('Pixel flag not supported.')                return                # check for duplicate flags         u, c = np.unique(in_fmask_flags, return_counts=True)        if len(u[c > 1]) > 0:            arcpy.AddError('Duplicate pixel flags detected.')            return         # get name of mask band        mask_band = arc.get_name_of_mask_band(list(ds))        try:            # remove invalid pixels and empty scenes            ds = cog.remove_fmask_dates(ds=ds,                                         valid_class=in_fmask_flags,                                         max_invalid=in_max_cloud,                                         mask_band=mask_band,                                         nodata_value=np.nan,                                         drop_fmask=True)        except Exception as e:            arcpy.AddError('Could not mask pixels.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Conforming satellite band names...')        arcpy.SetProgressorPosition(5)                try:            # get platform name from attributes, error if no attributes            in_platform = arc.get_platform_from_dea_attrs(ds_attrs)                        # conform dea aws band names based on platform            ds = satfetcher.conform_dea_ard_band_names(ds=ds,                                                        platform=in_platform.lower())           except Exception as e:             arcpy.AddError('Could not get platform from attributes.')            arcpy.AddMessage(str(e))            return        # check if all expected bands are in dataset         for band in ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']:            if band not in ds:                arcpy.AddError('NetCDF is missing band: {}. Need all bands.'.format(band))                return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Reducing dataset to wet and dry months...')        arcpy.SetProgressorPosition(6)                   # prepare wet, dry season lists        if in_wet_months == '' or in_dry_months == '':            arcpy.AddError('Must include at least one wet and dry month.')            return                # unpack months        wet_month = [int(e) for e in in_wet_months.split(';')]        dry_month = [int(e) for e in in_dry_months.split(';')]                # check if same months in wet and dry        for v in wet_month:            if v in dry_month:                arcpy.AddError('Cannot use same month in wet and dry months.')                return                        try:            # reduce xr dataset into only wet, dry months            ds = gdvspectra.subset_months(ds=ds,                                          month=wet_month + dry_month,                                          inplace=True)        except Exception as e:             arcpy.AddError('Could not subset dataset into wet and dry months.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Calculating vegetation and moisture indices...')        arcpy.SetProgressorPosition(7)                 # check if veg idx supported         if in_veg_idx.lower() not in ['ndvi', 'evi', 'savi', 'msavi', 'slavi', 'mavi', 'kndvi', 'tcg']:            arcpy.AddError('Vegetation index not supported.')            return         elif in_mst_idx.lower() not in ['ndmi', 'gvmi']:            arcpy.AddError('Moisture index not supported.')            return         try:            # calculate vegetation and moisture index            ds = tools.calculate_indices(ds=ds,                                          index=[in_veg_idx.lower(), in_mst_idx.lower()],                                          custom_name=['veg_idx', 'mst_idx'],                                          rescale=True,                                          drop=True)                                                     # add band attrs back on            ds['veg_idx'].attrs = ds_band_attrs               ds['mst_idx'].attrs = ds_band_attrs                    except Exception as e:             arcpy.AddError('Could not calculate indices.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Computing data into memory, please wait...')        arcpy.SetProgressorPosition(8)                try:            # compute!             ds = ds.compute()        except Exception as e:             arcpy.AddError('Could not compute dataset. See messages for details.')            arcpy.AddMessage(str(e))            return                    # check if all nan again        if ds.to_array().isnull().all():            arcpy.AddError('NetCDF is empty. Please download again.')                        return         # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Interpolating dataset, if requested...')        arcpy.SetProgressorPosition(9)                    # if requested...        if in_interpolate:            try:                # interpolate along time dimension (linear)                ds = tools.perform_interp(ds=ds, method='full')            except Exception as e:                 arcpy.AddError('Could not interpolate dataset.')                arcpy.AddMessage(str(e))                return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Resampling dataset to annual wet/dry medians...')        arcpy.SetProgressorPosition(10)             # extract datetimes for wet and dry seasons         dts_wet = ds['time'].where(ds['time.month'].isin(wet_month), drop=True)        dts_dry = ds['time'].where(ds['time.month'].isin(dry_month), drop=True)                # check if wet/dry months exist in the dataset, arent all empty        if len(dts_wet) == 0 or len(dts_dry) == 0:            arcpy.AddError('No wet and/or dry months captured in NetCDF.')            return        elif ds.sel(time=dts_wet).to_array().isnull().all():            arcpy.AddError('Entire wet season is devoid of values in NetCDF.')            return        elif ds.sel(time=dts_dry).to_array().isnull().all():            arcpy.AddError('Entire dry season is devoid of values in NetCDF.')            return        try:            # resample data to annual seasons            ds = gdvspectra.resample_to_wet_dry_medians(ds=ds,                                                         wet_month=wet_month,                                                         dry_month=dry_month,                                                        inplace=True)        except Exception as e:                 arcpy.AddError('Could not resample annualised wet and dry seasons.')                arcpy.AddMessage(str(e))                return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Removing outliers, if requested...')        arcpy.SetProgressorPosition(11)                # prepare zscore selection        if in_zscore_pvalue not in [0.01, 0.05, 0.1, None]:            arcpy.AddWarning('Z-score value not supported. Setting to default.')            in_zscore_pvalue = None                # if requested...        if in_zscore_pvalue is not None:            try:                # remove outliers                ds = gdvspectra.nullify_wet_dry_outliers(ds=ds,                                                          wet_month=wet_month,                                                          dry_month=dry_month,                                                          p_value=in_zscore_pvalue,                                                         inplace=True)                 except Exception as e:                 arcpy.AddError('Could not remove outliers.')                arcpy.AddMessage(str(e))                return                                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Cleaning years with insufficient seasonality...')        arcpy.SetProgressorPosition(12)         try:            # remove any years missing wet, dry season             ds = gdvspectra.drop_incomplete_wet_dry_years(ds=ds)        except Exception as e:             arcpy.AddError('Could not drop years with insufficient seasons.')            arcpy.AddMessage(str(e))            return                    # check if we still have sufficient number of years         if len(ds.groupby('time.year')) < 3:            arcpy.AddError('Input NetCDF needs more years. Expand time range in NetCDF.')            return                    try:            # fill any empty first, last years using manual back/forward fill            ds = gdvspectra.fill_empty_wet_dry_edges(ds=ds,                                                     wet_month=wet_month,                                                      dry_month=dry_month,                                                     inplace=True)        except Exception as e:             arcpy.AddError('Could not fill empty wet and dry edge dates.')            arcpy.AddMessage(str(e))            return                    try:            # interpolate missing values             ds = gdvspectra.interp_empty_wet_dry(ds=ds,                                                 wet_month=wet_month,                                                 dry_month=dry_month,                                                 method='full',                                                 inplace=True)        except Exception as e:             arcpy.AddError('Could not interpolate empty wet and dry edge dates.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Standardising data to dry season invariant targets...')        arcpy.SetProgressorPosition(13)                # check upper quantile        if in_ivt_qlower < 0 or in_ivt_qlower >= 0.5:            arcpy.AddMessage('Lower quantile must be between 0, 0.5. Setting to default.')            in_ivt_qlower = 0.05                # do same for upper quantile        if in_ivt_qupper <= 0.5 or in_ivt_qupper > 1.0:            arcpy.AddMessage('Upper quantile must be between 0.5, 1.0. Setting to default.')            in_ivt_qlower = 0.99                 # check if upper <= lower         if in_ivt_qupper <= in_ivt_qlower:            arcpy.AddError('Upper quantile must be > than lower quantile value.')            return                    try:            # standardise data to invariant targets derived from dry times            ds = gdvspectra.standardise_to_dry_targets(ds=ds,                                                        dry_month=dry_month,                                                        q_upper=in_ivt_qupper,                                                       q_lower=in_ivt_qlower,                                                       inplace=True)        except Exception as e:             arcpy.AddError('Could not standardise data to invariant targets.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Calculating seasonal similarity...')        arcpy.SetProgressorPosition(14)          try:            # calculate seasonal similarity            ds_similarity = gdvspectra.calc_seasonal_similarity(ds=ds,                                                                wet_month=wet_month,                                                                dry_month=dry_month,                                                                q_mask=0.9,                                                                inplace=True)        except Exception as e:             arcpy.AddError('Could not generate similarity.')            arcpy.AddMessage(str(e))            return                    # check similarity dataset is not empty         if ds_similarity.to_array().isnull().all():            arcpy.AddError('Similarity modelling returned no data.')            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Calculating GDV Likelihood...')        arcpy.SetProgressorPosition(15)          try:            # calculate gdv likelihood            ds = gdvspectra.calc_likelihood(ds=ds,                                             ds_similarity=ds_similarity,                                            wet_month=wet_month,                                             dry_month=dry_month)                                                        # convert dataset back to float32            ds = ds.astype('float32')                    except Exception as e:             arcpy.AddError('Could not generate likelihood data.')            arcpy.AddMessage(str(e))            return                    # check likelihood dataset is not empty         if ds.to_array().isnull().all():            arcpy.AddError('Likelihood modelling returned no data.')            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Aggreating dataset, if requested...')        arcpy.SetProgressorPosition(16)                 # if requested...        if in_aggregate is True:            try:                # reducing full dataset down to one median image without time                 ds = ds.median('time')            except Exception as e:                 arcpy.AddError('Could not aggregate dataset.')                arcpy.AddMessage(str(e))                return        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(17)                # append attrbutes on to dataset and bands        ds.attrs = ds_attrs        ds['spatial_ref'].attrs = ds_spatial_ref_attrs        for var in ds:            ds[var].attrs = ds_band_attrs                                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(18)           try:            # export netcdf file            tools.export_xr_as_nc(ds=ds, filename=out_nc)        except Exception as e:                 arcpy.AddError('Could not export dataset.')                arcpy.AddMessage(str(e))                return                                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Adding output to map, if requested...')        arcpy.SetProgressorPosition(19)                # if requested...        if in_add_result_to_map:            try:                # for current project, open current map                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap                                # remove likelihood layer if already exists                for layer in m.listLayers():                    if layer.name == 'likelihood.crf':                        m.removeLayer(layer)                            # create output folder using datetime as name                dt = datetime.datetime.now().strftime('%d%m%Y%H%M%S')                out_folder = os.path.join(os.path.dirname(out_nc), 'likelihood' + '_' + dt)                os.makedirs(out_folder)                            # disable visualise on map temporarily                arcpy.env.addOutputsToMap = False                                # create crf filename and copy it                out_file = os.path.join(out_folder, 'likelihood.crf')                crf = arcpy.CopyRaster_management(in_raster=out_nc,                                                   out_rasterdataset=out_file)                                                    # add to map                                  m.addDataFromPath(crf)                               except Exception as e:                arcpy.AddWarning('Could not visualise output, aborting visualisation.')                arcpy.AddMessage(str(e))                pass                            try:                # get symbology, update it                layer = m.listLayers('likelihood.crf')[0]                sym = layer.symbology                                # if layer has stretch coloriser, apply color                if hasattr(sym, 'colorizer'):                    if sym.colorizer.type == 'RasterStretchColorizer':                        # apply percent clip type                        sym.colorizer.stretchType = 'PercentClip'                        sym.colorizer.minPercent = 0.5                        sym.colorizer.maxPercent = 0.5                        # apply color map                        cmap = aprx.listColorRamps('Bathymetric Scale')[0]                        sym.colorizer.colorRamp = cmap                        # apply other basic options                        sym.colorizer.invertColorRamp = False                        sym.colorizer.gamma = 1.0                        # update symbology                        layer.symbology = sym            except Exception as e:                arcpy.AddWarning('Could not colorise output, aborting colorisation.')                arcpy.AddMessage(str(e))                pass                        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(20)                # close main dataset and del datasets        ds.close()        del ds                 # close similarity dataset        ds_similarity.close()        del ds_similarity        # notify user        arcpy.AddMessage('Generated GDV Likelihood successfully.')        returnclass GDVSpectra_Threshold(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = 'GDVSpectra Threshold'        self.description = 'Threshold an existing GDV Likelihood NetCDF using a ' \                           'shapefile of points or standard deviation. The output ' \                           'is a layer representing areas of high potential GDV ' \                           'Likelihood only.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """            # input netcdf data file        par_nc_file = arcpy.Parameter(                        displayName='Input GDV Likelihood NetCDF file',                        name='in_nc_file',                        datatype='DEFile',                        parameterType='Required',                        direction='Input')        par_nc_file.filter.list = ['nc']                # output netcdf location        par_out_nc_file = arcpy.Parameter(                            displayName='Output GDV Threshold NetCDF file',                            name='out_nc_file',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_file.filter.list = ['nc']                # aggregate all dates        par_aggregate = arcpy.Parameter(                          displayName='Combine all input years',                          name='in_aggregate',                          datatype='GPBoolean',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_aggregate.value = True                # set specific years        par_specific_years = arcpy.Parameter(                               displayName='Specific year(s) to threshold',                               name='in_specific_years',                               datatype='GPLong',                               parameterType='Optional',                               direction='Input',                               multiValue=True)        par_specific_years.filter.type = 'ValueList'        par_specific_years.filter.list = []                # threshold type        par_type = arcpy.Parameter(                     displayName='Threshold type',                     name='in_type',                     datatype='GPString',                     parameterType='Required',                     direction='Input',                     multiValue=False)        par_type.filter.type = 'ValueList'        par_type.filter.list = ['Standard Deviation', 'Occurrence Points']        par_type.value = 'Standard Deviation'                # standard dev        par_std_dev = arcpy.Parameter(                        displayName='Standard deviation of threshold',                        name='in_std_dev',                        datatype='GPDouble',                        parameterType='Optional',                        direction='Input',                        multiValue=False)        par_std_dev.filter.type = 'Range'        par_std_dev.filter.list = [0.0, 10.0]        par_std_dev.value = 2.0                # occurrence points        par_occurrence_feat = arcpy.Parameter(                                displayName='Occurrence point feature',                                name='in_occurrence_feat',                                datatype='GPFeatureLayer',                                parameterType='Optional',                                direction='Input')        par_occurrence_feat.filter.list = ['Point']                # field of presence/absence values        par_pa_column = arcpy.Parameter(                          displayName='Field with presence and absence labels',                          name='in_pa_column',                          datatype='GPString',                          parameterType='Optional',                          direction='Input',                          multiValue=False)        par_pa_column.filter.type = 'ValueList'        par_pa_column.filter.list = []                        # remove stray pixels        par_remove_stray = arcpy.Parameter(                             displayName='Remove stray pixels',                             name='in_remove_stray',                             datatype='GPBoolean',                             parameterType='Required',                             direction='Input',                             #category='Additional Options',                             multiValue=False)        par_remove_stray.value = True                # binarise checkbox        par_convert_binary = arcpy.Parameter(                               displayName='Binarise result',                               name='in_convert_binary',                               datatype='GPBoolean',                               parameterType='Required',                               direction='Input',                               #category='Additional Options',                               multiValue=False)        par_convert_binary.value = True                # add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True                # combine parameters        parameters = [            par_nc_file,            par_out_nc_file,            par_aggregate,            par_specific_years,            par_type,            par_std_dev,            par_occurrence_feat,            par_pa_column,            par_remove_stray,            par_convert_binary,            par_add_result_to_map            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""                return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # imports        try:            import numpy as np            import xarray as xr                   except:            arcpy.AddError('Python libraries xarray not installed.')            return        # globals         global GDVSPECTRA_THRESHOLD                # unpack global parameter values         curr_file = GDVSPECTRA_THRESHOLD.get('in_file')        curr_feat = GDVSPECTRA_THRESHOLD.get('in_feat')                        # if input file added, run        if parameters[0].value is not None:                        # if global has no matching file (or first run), reload all            if curr_file != parameters[0].valueAsText:                try:                    ds = xr.open_dataset(parameters[0].valueAsText)                    dts = np.unique(ds['time.year']).tolist()                    ds.close()                except:                    dts = []                                    # populate year list with new years, reset selection                parameters[3].filter.list = dts                parameters[3].value = None        # if occurrence point feat added, run         if parameters[6].value is not None:                        # if global has no matching feat (or first run), reload all            if curr_feat != parameters[6].valueAsText:                try:                    shp_path = parameters[6].valueAsText                    cols = [f.name for f in arcpy.ListFields(shp_path)]                except:                    cols = []                # populate field list with new names, reset selection                parameters[7].filter.list = cols                parameters[7].value = None        # update global values        GDVSPECTRA_THRESHOLD = {            'in_file': parameters[0].valueAsText,            'in_feat': parameters[6].valueAsText,        }        # enable specifc years based on combine checkbox         if parameters[2].value is False:            parameters[3].enabled = True        else:            parameters[3].enabled = False                # enable std dev or shapefile and field based on drop down        if parameters[4].value == 'Standard Deviation':            parameters[5].enabled = True             parameters[6].enabled = False            parameters[7].enabled = False         else:            parameters[5].enabled = False             parameters[6].enabled = True            parameters[7].enabled = True         return            def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""                return    def execute(self, parameters, messages):               """        Executes the GDV Spectra Threshold module.        """                # safe imports        import os, sys             # arcgis comes with these        import datetime            # arcgis comes with these        import numpy as np         # arcgis comes with these        import pandas as pd        # arcgis comes with these        import arcpy               # arcgis comes with these                # risky imports (not native to arcgis)        try:            import xarray as xr            import dask        except Exception as e:            arcpy.AddError('Python libraries xarray and dask not installed.')            arcpy.AddMessage(str(e))            return                        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                    # module folder            sys.path.append(FOLDER_MODULES)            import gdvspectra         except Exception as e:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            arcpy.AddMessage(str(e))            return                    # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        warnings.simplefilter(action='ignore', category=dask.array.core.PerformanceWarning)                # grab parameter values         in_nc = parameters[0].valueAsText                # likelihood netcdf        out_nc = parameters[1].valueAsText               # output netcdf        in_aggregate = parameters[2].value               # aggregate dates        in_specific_years = parameters[3].valueAsText    # set specific year         in_type = parameters[4].value                    # threshold type        in_std_dev = parameters[5].value                 # std dev threshold value         in_occurrence_feat = parameters[6]               # occurrence shp path         in_pa_column = parameters[7].value               # occurrence shp pres/abse col         in_remove_stray = parameters[8].value            # apply salt n pepper -- requires sa        in_convert_binary = parameters[9].value          # convert thresh to binary 1, nan        in_add_result_to_map = parameters[10].value      # add result to map                # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning GDVSpectra Threshold.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=13)        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading and checking netcdf...')        arcpy.SetProgressorPosition(1)                try:            # do quick lazy load of netcdf for checking            ds = xr.open_dataset(in_nc)        except Exception as e:            arcpy.AddWarning('Could not quick load input likelihood NetCDF data.')            arcpy.AddMessage(str(e))            return        # check xr type, vars, coords, dims, attrs        if not isinstance(ds, xr.Dataset):            arcpy.AddError('Input NetCDF must be a xr dataset.')            return        elif len(ds) == 0:            arcpy.AddError('Input NetCDF has no data/variables/bands.')            return        elif 'x' not in ds.dims or 'y' not in ds.dims:            arcpy.AddError('Input NetCDF must have x, y dimensions.')            return                elif 'x' not in ds.coords or 'y' not in ds.coords:            arcpy.AddError('Input NetCDF must have x, y coords.')            return        elif 'spatial_ref' not in ds.coords:            arcpy.AddError('Input NetCDF must have a spatial_ref coord.')            return        elif len(ds['x']) == 0 or len(ds['y']) == 0:            arcpy.AddError('Input NetCDF must have at least one x, y index.')            return        elif 'like' not in ds:            arcpy.AddError('Input NetCDF must have a "like" variable. Run GDVSpectra Likelihood.')            return        elif 'time' in ds and (not hasattr(ds, 'time.year') or not hasattr(ds, 'time.month')):            arcpy.AddError('Input NetCDF must have time with year and month component.')            return        elif 'time' in ds.dims and 'time' not in ds.coords:            arcpy.AddError('Input NetCDF has time dimension but not coordinate.')            return        elif ds.attrs == {}:            arcpy.AddError('NetCDF attributes not found. NetCDF must have attributes.')            return        elif not hasattr(ds, 'crs'):            arcpy.AddError('NetCDF CRS attribute not found. CRS required.')            return        elif ds.crs != 'EPSG:3577':            arcpy.AddError('NetCDF CRS is not EPSG:3577. EPSG:3577 required.')                        return         elif not hasattr(ds, 'nodatavals'):            arcpy.AddError('NetCDF nodatavals attribute not found.')                        return         # check if variables (should only be like) are empty        if ds['like'].isnull().all() or (ds['like'] == 0).all():            arcpy.AddError('NetCDF "like" variable is empty. Please download again.')                        return          try:            # now, do proper open of netcdf (set nodata to nan)            ds = satfetcher.load_local_nc(nc_path=in_nc,                                           use_dask=True,                                           conform_nodata_to=np.nan)        except Exception as e:            arcpy.AddError('Could not properly load input likelihood NetCDF data.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Getting NetCDF attributes...')        arcpy.SetProgressorPosition(2)                    # get attributes from dataset        ds_attrs = ds.attrs        ds_band_attrs = ds[list(ds)[0]].attrs        ds_spatial_ref_attrs = ds['spatial_ref'].attrs                                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Grouping dates, if required...')        arcpy.SetProgressorPosition(3)        # remove potential datetime duplicates (group by day)        if 'time' in ds:            ds = satfetcher.group_by_solar_day(ds)        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Reducing dataset based on time, if requested...')        arcpy.SetProgressorPosition(4)                        # if time is in dataset...        if 'time' in ds:                    # check aggregate and specified year(s) is valid            if in_aggregate is None:                arcpy.AddError('Did not specify aggregate parameter.')                return            elif in_aggregate is False and in_specific_years is None:                arcpy.AddError('Did not provide a specific year.')                return                            # if specific years set...            if in_aggregate is False:                in_specific_years = [int(e) for e in in_specific_years.split(';')]                           # aggregate depending on user choice             if in_aggregate is True:                ds = ds.median('time')            else:                ds = ds.where(ds['time.year'].isin(in_specific_years), drop=True)                ds = ds.median('time')        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Computing data into memory, please wait...')        arcpy.SetProgressorPosition(5)        try:            # compute!             ds = ds.compute()        except Exception as e:             arcpy.AddError('Could not compute dataset. See messages for details.')            arcpy.AddMessage(str(e))            return                                    # check if all nan again        if ds.to_array().isnull().all():            arcpy.AddError('NetCDF is empty. Please download again.')                        return                                             # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Preparing occurrence points, if provided...')        arcpy.SetProgressorPosition(6)        # we need nodataval attr, so ensure it exists         ds.attrs = ds_attrs                        # if requested...        if in_type == 'Occurrence Points':                    # check if both shapefile and field provided             if in_occurrence_feat.value is None or in_pa_column is None:                arcpy.AddError('No occurrence feature and/or field provided.')                return            try:                # get path to feature instead of map layer                 desc = arcpy.Describe(in_occurrence_feat)                in_occurrence_feat = os.path.join(desc.path, desc.name)                            # read shapefile via arcpy, convert feat into dataframe of x, y, actual                df_records = arc.read_shp_for_threshold(in_occurrence_feat=in_occurrence_feat,                                                         in_pa_column=in_pa_column)                # intersect points with dataset and extract likelihood values                df_records = tools.intersect_records_with_xr(ds=ds,                                                              df_records=df_records,                                                              extract=True,                                                              res_factor=3,                                                              if_nodata='any')                    # rename column to predicted and check                df_records = df_records.rename(columns={'like': 'predicted'})                                # check if any records intersected dataset                 if len(df_records.index) == 0:                    arcpy.AddError('No shapefile points intersect GDV likelihood dataset.')                    return                                    # remove any records where vars contain nodata                df_records = tools.remove_nodata_records(df_records,                                                          nodata_value=ds.nodatavals)                                                                         # check again if any records exist                if len(df_records.index) == 0:                    arcpy.AddError('No shapefile points remain after empty values removed.')                    return                                except Exception as e:                arcpy.AddError('Could not read shapefile, see messages for details.')                arcpy.AddMessage(str(e))                return                            # check if some 1s and 0s exist             unq = df_records['actual'].unique()            if not np.any(unq == 1) or not np.any(unq == 0):                arcpy.AddError('Insufficient presence/absence points within NetCDF bounds.')                return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Thresholding GDV Likelihood...')        arcpy.SetProgressorPosition(7)                      try:            # perform thresholding using either shapefile points or std dev            if in_type == 'Occurrence Points' and df_records is not None:                ds = gdvspectra.threshold_likelihood(ds=ds,                                                     df=df_records,                                                      res_factor=3,                                                      if_nodata='any')            else:                ds = gdvspectra.threshold_likelihood(ds=ds,                                                     num_stdevs=in_std_dev,                                                      res_factor=3,                                                      if_nodata='any')                                                                 # rename var, convert to float32            ds = ds.rename({'like': 'thresh'}).astype('float32')        except Exception as e:            arcpy.AddError('Could not threshold data.')            arcpy.AddMessage(str(e))            #print(str(e))            return                    # check if any data was returned after threshold        if ds.to_array().isnull().all():            arcpy.AddError('Threshold returned no values, try modifying threshold.')            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Removing stray pixels, if requested...')        arcpy.SetProgressorPosition(8)                 # if requested...        if in_remove_stray:            try:                # remove salt n pepper                 ds = gdvspectra.remove_salt_pepper(ds, iterations=1)            except Exception as e:                arcpy.AddError('Could not remove stray pixels.')                arcpy.AddMessage(str(e))                return        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Binarising values, if requested...')        arcpy.SetProgressorPosition(9)                # if requested...        if in_convert_binary:            # set all threshold non-nan values to 1            ds = ds.where(ds.isnull(), 1)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(10)                # append attrbutes on to dataset and bands        ds.attrs = ds_attrs        ds['spatial_ref'].attrs = ds_spatial_ref_attrs        for var in ds:            ds[var].attrs = ds_band_attrs        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(11)           try:            # export netcdf file            tools.export_xr_as_nc(ds=ds, filename=out_nc)        except Exception as e:            arcpy.AddError('Could not export dataset.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Adding output to map, if requested...')        arcpy.SetProgressorPosition(12)                # if requested...        if in_add_result_to_map:            try:                # for current project, open current map                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap                                # remove threshold layer if already exists                 for layer in m.listLayers():                    if layer.name == 'likelihood_threshold.crf':                        m.removeLayer(layer)                                # create output folder using datetime as name                dt = datetime.datetime.now().strftime('%d%m%Y%H%M%S')                out_folder = os.path.join(os.path.dirname(out_nc), 'likelihood_threshold' + '_' + dt)                os.makedirs(out_folder)                            # disable visualise on map temporarily                arcpy.env.addOutputsToMap = False                                # create crf filename and copy it                out_file = os.path.join(out_folder, 'likelihood_threshold.crf')                crf = arcpy.CopyRaster_management(in_raster=out_nc,                                                   out_rasterdataset=out_file)                                                    # add to map                                  m.addDataFromPath(crf)              except Exception as e:                arcpy.AddWarning('Could not visualise output, aborting visualisation.')                arcpy.AddMessage(str(e))                pass                            try:                           # get symbology, update it                layer = m.listLayers('likelihood_threshold.crf')[0]                sym = layer.symbology                                # if layer has stretch coloriser, apply color                if hasattr(sym, 'colorizer'):                                        # apply percent clip type                    sym.colorizer.stretchType = 'PercentClip'                    sym.colorizer.minPercent = 0.25                    sym.colorizer.maxPercent = 0.25                                    # colorise deopending on binary or continious                    if in_convert_binary is True:                        cmap = aprx.listColorRamps('Yellow to Red')[0]                    else:                        cmap = aprx.listColorRamps('Bathymetric Scale')[0]                                        # apply colormap                    sym.colorizer.colorRamp = cmap                    # apply other basic options                    sym.colorizer.invertColorRamp = False                    sym.colorizer.gamma = 1.0                                            # update symbology                    layer.symbology = sym                                 except Exception as e:                arcpy.AddWarning('Could not colorise output, aborting colorisation.')                arcpy.AddMessage(str(e))                pass        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(13)                # close main dataset        ds.close()        del ds                 # notify user        arcpy.AddMessage('Generated GDV Threshold successfully.')                          returnclass GDVSpectra_Trend(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = 'GDVSpectra Trend'        self.description = 'Perform a time-series trend analysis on an existing ' \                           'GDV Likelihood data cube. Produces a map of areas where ' \                           'vegetation has continuously increased, decreased or ' \                           'has not changed.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """                # input like netcdf file        par_like_nc_file = arcpy.Parameter(                             displayName='Input GDV Likelihood NetCDF file',                             name='in_like_nc_file',                             datatype='DEFile',                             parameterType='Required',                             direction='Input')        par_like_nc_file.filter.list = ['nc']                # input mask netcdf file        par_mask_nc_file = arcpy.Parameter(                             displayName='Input GDV Threshold mask NetCDF file',                             name='in_mask_nc_file',                             datatype='DEFile',                             parameterType='Optional',                             direction='Input')        par_mask_nc_file.filter.list = ['nc']                # output netcdf location        par_out_nc_file = arcpy.Parameter(                                  displayName='Output GDV Trend NetCDF file',                                  name='out_nc_file',                                  datatype='DEFile',                                  parameterType='Required',                                  direction='Output')        par_out_nc_file.filter.list = ['nc']                # use all years        par_use_all_years = arcpy.Parameter(                          displayName='Combine all input years',                          name='in_use_all_years',                          datatype='GPBoolean',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_use_all_years.value = True                # set specific start year        par_start_year = arcpy.Parameter(                               displayName='Start year of trend analysis',                               name='in_start_year',                               datatype='GPLong',                               parameterType='Optional',                               direction='Input',                               multiValue=False)        par_start_year.filter.type = 'ValueList'        par_start_year.filter.list = []                # set specific end year        par_end_year = arcpy.Parameter(                               displayName='End year of trend analysis',                               name='in_end_year',                               datatype='GPLong',                               parameterType='Optional',                               direction='Input',                               multiValue=False)        par_end_year.filter.type = 'ValueList'        par_end_year.filter.list = []                        # set analysis type        par_analysis_type = arcpy.Parameter(                              displayName='Trend analysis method',                              name='in_analysis_type',                              datatype='GPString',                              parameterType='Required',                              direction='Input',                              multiValue=False)        par_analysis_type.filter.type = 'ValueList'        par_analysis_type.filter.list = ['Mann-Kendall', 'Theil-Sen Slope']        par_analysis_type.value = 'Mann-Kendall'                # mk p-value        par_mk_pvalue = arcpy.Parameter(                          displayName='P-value',                          name='in_mk_pvalue',                          datatype='GPDouble',                          parameterType='Optional',                          direction='Input',                          multiValue=False)        par_mk_pvalue.filter.type = 'Range'        par_mk_pvalue.filter.list = [0.001, 1.0]        par_mk_pvalue.value = 0.05        # mk direction        par_mk_direction = arcpy.Parameter(                            displayName='Trend direction',                            name='in_mk_direction',                            datatype='GPString',                            parameterType='Optional',                            direction='Input',                            multiValue=False)        par_mk_direction.filter.type = 'ValueList'        par_mk_direction.filter.list = ['Both', 'Increasing', 'Decreasing']        par_mk_direction.value = 'Both'                # add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True                # combine parameters        parameters = [            par_like_nc_file,            par_mask_nc_file,            par_out_nc_file,            par_use_all_years,            par_start_year,            par_end_year,            par_analysis_type,            par_mk_pvalue,            par_mk_direction,            par_add_result_to_map        ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # imports        try:            import numpy as np            import xarray as xr                   except:            arcpy.AddError('Python libraries xarray not installed.')            return                # globals         global GDVSPECTRA_TREND        # unpack global parameter values         curr_file = GDVSPECTRA_TREND.get('in_file')                # if input file added, run        if parameters[0].value is not None:            # if global has no matching file (or first run), reload all            if curr_file != parameters[0].valueAsText:                try:                    ds = xr.open_dataset(parameters[0].valueAsText)                    dts = np.unique(ds['time.year']).tolist()                    ds.close()                except:                    dts = []                # populate start and end year lists                parameters[4].filter.list = dts                parameters[5].filter.list = dts                                # reset start and end year selections                if len(dts) != 0:                    parameters[4].value = dts[0]                    parameters[5].value = dts[-1]                else:                    parameters[4].value = None                    parameters[5].value = None                        # update global values        GDVSPECTRA_TREND = {            'in_file': parameters[0].valueAsText,        }                # enable start and end years based on all years checkbox         if parameters[3].value is False:            parameters[4].enabled = True            parameters[5].enabled = True        else:            parameters[4].enabled = False            parameters[5].enabled = False        # enable relevant controls when manken or theils        if parameters[6].valueAsText == 'Mann-Kendall':            parameters[7].enabled = True            parameters[8].enabled = True        else:            parameters[7].enabled = False            parameters[8].enabled = False        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the GDV Spectra Trend module.        """                # safe imports        import os, sys         # arcgis comes with these        import datetime        # arcgis comes with these        import numpy as np     # arcgis comes with these        import arcpy           # arcgis comes with these                # risky imports (not native to arcgis)        try:            import xarray as xr            import dask        except Exception as e:            arcpy.AddError('Python libraries xarray and dask not installed.')            arcpy.AddMessage(str(e))            return                        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import satfetcher, tools                    # module folder            sys.path.append(FOLDER_MODULES)            import gdvspectra         except Exception as e:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            arcpy.AddMessage(str(e))            return                    # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        warnings.simplefilter(action='ignore', category=dask.array.core.PerformanceWarning)                    # grab parameter values         in_like_nc = parameters[0].valueAsText         # likelihood netcdf        in_mask_nc = parameters[1].valueAsText         # thresh mask netcdf        out_nc = parameters[2].valueAsText             # output netcdf        in_use_all_years = parameters[3].value         # use all years        in_start_year = parameters[4].value            # start year        in_end_year = parameters[5].value              # end year        in_trend_method = parameters[6].value          # trend method        in_mk_pvalue = parameters[7].value             # mk pvalue        in_mk_direction = parameters[8].value          # mk direction        in_add_result_to_map = parameters[9].value    # add result to map                        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning GDVSpectra Trend.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=10)                        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading and checking likelihood netcdf...')        arcpy.SetProgressorPosition(1)                try:            # do quick lazy load of likelihood netcdf for checking            ds_like = xr.open_dataset(in_like_nc)        except Exception as e:            arcpy.AddWarning('Could not quick load input likelihood NetCDF data.')            arcpy.AddMessage(str(e))            return                # check xr type, vars, coords, dims, attrs        if not isinstance(ds_like, xr.Dataset):            arcpy.AddError('Input likelihood NetCDF must be a xr dataset.')            return        elif len(ds_like) == 0:            arcpy.AddError('Input likelihood NetCDF has no data/variables/bands.')            return        elif 'x' not in ds_like.dims or 'y' not in ds_like.dims or 'time' not in ds_like.dims:            arcpy.AddError('Input likelihood NetCDF must have x, y and time dimensions.')            return        elif 'x' not in ds_like.coords or 'y' not in ds_like.coords or 'time' not in ds_like.coords:            arcpy.AddError('Input likelihood NetCDF must have x, y and time coords.')            return        elif 'spatial_ref' not in ds_like.coords:            arcpy.AddError('Input likelihood NetCDF must have a spatial_ref coord.')            return        elif len(ds_like['x']) == 0 or len(ds_like['y']) == 0:            arcpy.AddError('Input likelihood NetCDF must have at least one x, y index.')            return                elif len(ds_like['time']) <= 3:            arcpy.AddError('Input likelihood NetCDF must have 4 or more times.')            return                            elif 'like' not in ds_like:            arcpy.AddError('Input likelihood NetCDF must have a "like" variable. Run GDVSpectra Likelihood.')            return        elif not hasattr(ds_like, 'time.year') or not hasattr(ds_like, 'time.month'):            arcpy.AddError('Input likelihood NetCDF must have time with year and month index.')            return        elif ds_like.attrs == {}:            arcpy.AddError('Input likelihood NetCDF attributes not found. NetCDF must have attributes.')            return        elif not hasattr(ds_like, 'crs'):            arcpy.AddError('Input likelihood NetCDF CRS attribute not found. CRS required.')            return        elif ds_like.crs != 'EPSG:3577':            arcpy.AddError('Input likelihood NetCDF CRS is not EPSG:3577. EPSG:3577 required.')                        return         elif not hasattr(ds_like, 'nodatavals'):            arcpy.AddError('Input likelihood NetCDF nodatavals attribute not found.')                        return         # check if variables (should only be like) are empty        if ds_like['like'].isnull().all() or (ds_like['like'] == 0).all():            arcpy.AddError('Input likelihood NetCDF "like" variable is empty. Please download again.')                        return         try:            # now, do proper open of likelihood netcdf (set nodata to nan)            ds_like = satfetcher.load_local_nc(nc_path=in_like_nc,                                                use_dask=True,                                                conform_nodata_to=np.nan)        except Exception as e:            arcpy.AddError('Could not properly load input likelihood NetCDF data.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Getting NetCDF attributes...')        arcpy.SetProgressorPosition(2)                    # get attributes from dataset        ds_attrs = ds_like.attrs        ds_band_attrs = ds_like['like'].attrs        ds_spatial_ref_attrs = ds_like['spatial_ref'].attrs                                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Grouping dates, if required...')        arcpy.SetProgressorPosition(3)                # remove potential datetime duplicates (group by day)        ds_like = satfetcher.group_by_solar_day(ds_like)                                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Reducing dataset based on time, if requested...')        arcpy.SetProgressorPosition(4)                # if requested...        if in_use_all_years is False:                    # check start, end year valid            if in_start_year is None or in_end_year is None:                arcpy.AddError('Did not specify start and/or end years.')                return                     elif in_start_year >= in_end_year:                arcpy.AddError('Start year must be < end year.')                return            elif abs(in_end_year - in_start_year) < 3:                arcpy.AddError('Require 4 or more years of data.')                return                            # check if both years in dataset            years = ds_like['time.year']            if in_start_year not in years or in_end_year not in years:                arcpy.AddError('Start year is not in likelihood NetCDF.')                return                            # subset likelihood dataset based on years             ds_like = ds_like.where((ds_like['time.year'] >= in_start_year) &                                    (ds_like['time.year'] <= in_end_year), drop=True)                                    # check if more than three years still exist             if len(ds_like['time']) < 4:                arcpy.AddError('Subset of likelihood NetCDF resulted in < 4 years of data.')                return                        # check if variables (should only be like) are empty            if ds_like['like'].isnull().all() or (ds_like['like'] == 0).all():                arcpy.AddError('Subset of likelihood NetCDF is empty. Increase year range.')                            return                                                        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Computing data into memory, please wait...')        arcpy.SetProgressorPosition(5)        try:            # compute!             ds_like = ds_like.compute()        except Exception as e:             arcpy.AddError('Could not compute likelihood dataset. See messages for details.')            arcpy.AddMessage(str(e))            return                       # check if we still have values        if ds_like.to_array().isnull().all():            arcpy.AddError('Input likelihood NetCDF is empty. Please download again.')                        return                                 # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading, checking and applying mask, if requested...')        arcpy.SetProgressorPosition(6)        # if requested...        if in_mask_nc is not None:                    try:                # do quick lazy load of mask netcdf for checking                ds_mask = xr.open_dataset(in_mask_nc)            except Exception as e:                arcpy.AddWarning('Could not quick load input mask NetCDF data.')                arcpy.AddMessage(str(e))                return                        # check xr type, vars, coords, dims, attrs            if not isinstance(ds_mask, xr.Dataset):                arcpy.AddError('Input mask NetCDF must be a xr dataset.')                return            elif len(ds_mask) == 0:                arcpy.AddError('Input mask NetCDF has no data/variables/bands.')                return            elif 'x' not in ds_mask.dims or 'y' not in ds_mask.dims:                arcpy.AddError('Input mask NetCDF must have x, y dimensions.')                return            elif 'x' not in ds_mask.coords or 'y' not in ds_mask.coords:                arcpy.AddError('Input mask NetCDF must have x, y coords.')                return            elif 'spatial_ref' not in ds_mask.coords:                arcpy.AddError('Input mask NetCDF must have a spatial_ref coord.')                return            elif len(ds_mask['x']) == 0 or len(ds_mask['y']) == 0:                arcpy.AddError('Input mask NetCDF must have at least one x, y index.')                return                    elif 'time' in ds_mask:                arcpy.AddError('Input mask NetCDF must not have a time dimension.')                return                                elif 'thresh' not in ds_mask:                arcpy.AddError('Input mask NetCDF must have a "thresh" variable. Run GDVSpectra Threshold.')                return            elif ds_mask.attrs == {}:                arcpy.AddError('Input mask NetCDF attributes not found. NetCDF must have attributes.')                return            elif not hasattr(ds_mask, 'crs'):                arcpy.AddError('Input mask NetCDF CRS attribute not found. CRS required.')                return            elif ds_mask.crs != 'EPSG:3577':                arcpy.AddError('Input mask NetCDF CRS is not EPSG:3577. EPSG:3577 required.')                            return             elif not hasattr(ds_mask, 'nodatavals'):                arcpy.AddError('Input mask NetCDF nodatavals attribute not found.')                            return             # check if variables (should only be thresh) are empty            if ds_mask['thresh'].isnull().all() or (ds_mask['thresh'] == 0).all():                arcpy.AddError('Input mask NetCDF "thresh" variable is empty. Please download again.')                            return             try:                # now, do proper open of mask netcdf (set nodata to nan)                ds_mask = satfetcher.load_local_nc(nc_path=in_mask_nc,                                                    use_dask=True,                                                    conform_nodata_to=np.nan)                                                                   # compute it!                ds_mask = ds_mask.compute()                            except Exception as e:                arcpy.AddError('Could not properly load input mask NetCDF data.')                arcpy.AddMessage(str(e))                return                            try:                # check if like and mask datasets overlap                 if not tools.all_xr_intersect([ds_like, ds_mask]):                    arcpy.AddError('Input datasets do not intersect.')                    arcpy.AddMessage(str(e))                    return                                # resample mask dataset to match likelihood                 ds_mask = tools.resample_xr(ds_from=ds_mask,                                             ds_to=ds_like,                                            resampling='nearest')                # squeeze it to be safe                ds_mask = ds_mask.squeeze(drop=True)                            except Exception as e:                arcpy.AddError('Could not intersect input datasets.')                arcpy.AddMessage(str(e))                return                            # we made it, so mask likelihood            ds_like = ds_like.where(~ds_mask['thresh'].isnull())                        # check if variables (should only be thresh) are empty            if ds_like['like'].isnull().all() or (ds_like['like'] == 0).all():                arcpy.AddError('Masked likelihood dataset is empty.')                            return                         # # # # #        # notify and set on-going progess bar        arcpy.SetProgressor('default', 'Performing trend analysis...')                # check if trend method is supported         if in_trend_method not in ['Mann-Kendall', 'Theil-Sen Slope']:            arcpy.AddError('Trend method not supported.')                        return                # check and perform mann-kendall or theil sen        if in_trend_method == 'Mann-Kendall':            # check mannkendall pvalue             if in_mk_pvalue is None:                arcpy.AddError('Mann-Kendall p-value not provided.')                            return            elif in_mk_pvalue < 0.001 or in_mk_pvalue > 1.0:                arcpy.AddError('Mann-Kendall p-value must be between 0.001 and 1.0.')                            return                            # check mannkendall direction             if in_mk_direction not in ['Increasing', 'Decreasing', 'Both']:                arcpy.AddError('Mann-Kendall direction not supported.')                            return                            # prepare mannkendal direction (must be inc, dec or both)            if in_mk_direction in ['Increasing', 'Decreasing']:                in_mk_direction = in_mk_direction.lower()[:3]            else:                in_mk_direction = 'both'                            try:                # perform mann-kendall trend analysis                ds_trend = gdvspectra.perform_mk_original(ds=ds_like,                                                           pvalue=in_mk_pvalue,                                                           direction=in_mk_direction)                                                                          # warn if no change values returned                if ds_trend['tau'].isnull().all():                    arcpy.AddError('Trend output is empty, check p-value, date range and/or mask.')                    return                                except Exception as e:                arcpy.AddError('Could not perform Mann-Kendall trend analysis.')                arcpy.AddMessage(str(e))                return            else:                        try:                # perform theil-sen trend analysis                ds_trend = gdvspectra.perform_theilsen_slope(ds=ds_like,                                                              alpha=0.95)                                                                             # warn if no change values returned                if ds_trend['theil'].isnull().all():                    arcpy.AddError('Trend output is empty, check date range and/or mask.')                    return                        except Exception as e:                arcpy.AddError('Could not perform Theil-Sen trend analysis.')                arcpy.AddMessage(str(e))                return            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(7)                # append attrbutes on to dataset and bands        ds_trend.attrs = ds_attrs        ds_trend['spatial_ref'].attrs = ds_spatial_ref_attrs        for var in ds_trend:            ds_trend[var].attrs = ds_band_attrs                                    # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(8)           try:            # export netcdf file            tools.export_xr_as_nc(ds=ds_trend, filename=out_nc)        except Exception as e:            arcpy.AddError('Could not export dataset.')            arcpy.AddMessage(str(e))            return                    # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Adding output to map, if requested...')        arcpy.SetProgressorPosition(9)        # if requested...        if in_add_result_to_map:            try:                # for current project, open current map                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap                                # remove trend layer if already exists                 for layer in m.listLayers():                    if layer.name == 'trend.crf':                        m.removeLayer(layer)                # create output folder using datetime as name                dt = datetime.datetime.now().strftime('%d%m%Y%H%M%S')                out_folder = os.path.join(os.path.dirname(out_nc), 'trend' + '_' + dt)                os.makedirs(out_folder)                                # disable visualise on map temporarily                arcpy.env.addOutputsToMap = False                                # create crf filename and copy it                out_file = os.path.join(out_folder, 'trend.crf')                crf = arcpy.CopyRaster_management(in_raster=out_nc,                                                   out_rasterdataset=out_file)                                                                  # add to map                                  m.addDataFromPath(crf)              except Exception as e:                arcpy.AddWarning('Could not visualise output, aborting visualisation.')                arcpy.AddMessage(str(e))                pass                        try:                # get symbology, update it                layer = m.listLayers('trend.crf')[0]                sym = layer.symbology                # if layer has stretch coloriser, apply color                if hasattr(sym, 'colorizer'):                    # apply percent clip type                    sym.colorizer.stretchType = 'PercentClip'                    sym.colorizer.minPercent = 0.75                    sym.colorizer.maxPercent = 0.75                    # set default trend cmap, override if mannkenn inc or dec used                    cmap = aprx.listColorRamps('Red-Blue (Continuous)')[0]                    if in_trend_method == 'Mann-Kendall':                        if in_mk_direction == 'inc':                            cmap = aprx.listColorRamps('Yellow-Green-Blue (Continuous)')[0]                        elif in_mk_direction == 'dec':                            cmap = aprx.listColorRamps('Yellow-Orange-Red (Continuous)')[0]                                            # apply colormap                    sym.colorizer.colorRamp = cmap                    # invert colormap if mannkenn decline                     if in_trend_method == 'Mann-Kendall' and in_mk_direction == 'dec':                        sym.colorizer.invertColorRamp = True                    else:                        sym.colorizer.invertColorRamp = False                                        # set gamma                    sym.colorizer.gamma = 1.0                    # update symbology                    layer.symbology = sym              except Exception as e:                arcpy.AddWarning('Could not colorise output, aborting colorisation.')                arcpy.AddMessage(str(e))                pass        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(10)                # close likelihood dataset        ds_like.close()        del ds_like                # close trend dataset        ds_trend.close()         del ds_trend                # close mask (if exists)        if in_mask_nc is not None:            ds_mask.close()            del ds_mask                    # notify user        arcpy.AddMessage('Generated GDV Trend successfully.')                          returnclass GDVSpectra_CVA(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = 'GDVSpectra CVA'        self.description = 'Perform a Change Vector Analysis (CVA) on a data cube.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """                # input netcdf file        par_raw_nc_path = arcpy.Parameter(                            displayName='Input satellite NetCDF file',                            name='in_raw_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Input')        par_raw_nc_path.filter.list = ['nc']                # input netcdf mask (thresh) file        par_mask_nc_path = arcpy.Parameter(                             displayName='Input GDV Threshold mask NetCDF file',                             name='in_mask_nc_path',                             datatype='DEFile',                             parameterType='Optional',                             direction='Input')        par_mask_nc_path.filter.list = ['nc']                # output folder location        par_out_nc_path = arcpy.Parameter(                            displayName='Output CVA NetCDF file',                            name='out_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_path.filter.list = ['nc']                # base start year        par_base_start_year = arcpy.Parameter(                                displayName='Baseline start year',                                name='in_base_start_year',                                datatype='GPLong',                                parameterType='Required',                                direction='Input',                                multiValue=False)        par_base_start_year.filter.type = 'ValueList'        par_base_start_year.filter.list = []                # base end year        par_base_end_year = arcpy.Parameter(                              displayName='Baseline end year',                              name='in_base_end_year',                              datatype='GPLong',                              parameterType='Required',                              direction='Input',                              multiValue=False)        par_base_end_year.filter.type = 'ValueList'        par_base_end_year.filter.list = []        # comparison start year        par_comp_start_year = arcpy.Parameter(                                displayName='Comparison start year',                                name='in_comp_start_year',                                datatype='GPLong',                                parameterType='Required',                                direction='Input',                                multiValue=False)        par_comp_start_year.filter.type = 'ValueList'        par_comp_start_year.filter.list = []                # comparison end year        par_comp_end_year = arcpy.Parameter(                              displayName='Comparison end year',                              name='in_comp_end_year',                              datatype='GPLong',                              parameterType='Required',                              direction='Input',                              multiValue=False)        par_comp_end_year.filter.type = 'ValueList'        par_comp_end_year.filter.list = []        # analysis months        par_analysis_months = arcpy.Parameter(                                displayName='Set analysis month(s)',                                name='in_analysis_months',                                datatype='GPLong',                                parameterType='Required',                                direction='Input',                                multiValue=True)        par_analysis_months.filter.type = 'ValueList'        par_analysis_months.filter.list = [m for m in range(1, 13)]        par_analysis_months.value = [9, 10, 11]        # cva magnitude threshold         par_tmf = arcpy.Parameter(                    displayName='Magnitude threshold',                    name='in_tmf',                    datatype='GPDouble',                    parameterType='Required',                    direction='Input',                    multiValue=False)        par_tmf.filter.type = 'Range'        par_tmf.filter.list = [0.0, 100.0]          par_tmf.value = 2.0          # set q upper for standardisation        par_ivt_qupper = arcpy.Parameter(                           displayName='Upper percentile',                           name='in_stand_qupper',                           datatype='GPDouble',                           parameterType='Required',                           direction='Input',                           category='Invariant Standardisation',                           multiValue=False)        par_ivt_qupper.filter.type = 'Range'        par_ivt_qupper.filter.list = [0.0, 1.0]        par_ivt_qupper.value = 0.99                         # set q lower for standardisation        par_ivt_qlower = arcpy.Parameter(                           displayName='Lower percentile',                           name='in_stand_qlower',                           datatype='GPDouble',                           parameterType='Required',                           direction='Input',                           category='Invariant Standardisation',                           multiValue=False)        par_ivt_qlower.filter.type = 'Range'        par_ivt_qlower.filter.list = [0.0, 1.0]        par_ivt_qlower.value = 0.05                     # set oa class values        par_fmask_flags = arcpy.Parameter(displayName='Include pixel flags',                            name='in_fmask_flags',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Satellite Quality Options',                            multiValue=True)        flags = [            'NoData',             'Valid',             'Cloud',             'Shadow',             'Snow',             'Water'            ]        par_fmask_flags.filter.type = 'ValueList'                par_fmask_flags.filter.list = flags        par_fmask_flags.values = ['Valid', 'Snow', 'Water']                # max cloud cover        par_max_cloud = arcpy.Parameter(                          displayName='Maximum cloud cover',                          name='in_max_cloud',                          datatype='GPDouble',                          parameterType='Required',                          direction='Input',                          category='Satellite Quality Options',                          multiValue=False)        par_max_cloud.filter.type = 'Range'        par_max_cloud.filter.list = [0.0, 100.0]        par_max_cloud.value = 10.0                # interpolate        par_interpolate = arcpy.Parameter(                            displayName='Interpolate NoData pixels',                            name='in_interpolate',                            datatype='GPBoolean',                            parameterType='Required',                            direction='Input',                            category='Satellite Quality Options',                            multiValue=False)        par_interpolate.value = True                # add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True                # combine parameters        parameters = [            par_raw_nc_path,            par_mask_nc_path,            par_out_nc_path,            par_base_start_year,            par_base_end_year,            par_comp_start_year,            par_comp_end_year,            par_analysis_months,            par_tmf,            par_ivt_qupper,            par_ivt_qlower,            par_fmask_flags,            par_max_cloud,            par_interpolate,            par_add_result_to_map            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # imports        try:            import numpy as np            import xarray as xr        except:            arcpy.AddError('Python library xarray not installed.')            return                    # globals         global GDVSPECTRA_CVA        # unpack global parameter values         curr_file = GDVSPECTRA_CVA.get('in_file')                        # if input file added, run        if parameters[0].value is not None:                    # if global has no matching file (or first run), reload all            if curr_file != parameters[0].valueAsText:                try:                    ds = xr.open_dataset(parameters[0].valueAsText)                    dts = np.unique(ds['time.year']).tolist()                    ds.close()                except:                    dts = []                # populate baseline start and end year lists                parameters[3].filter.list = dts                parameters[4].filter.list = dts                                # populate comparison start and end year lists                parameters[5].filter.list = dts                parameters[6].filter.list = dts                                # reset baseline start and end year selections                if len(dts) != 0:                    parameters[3].value = dts[0]                    parameters[4].value = dts[0]                    parameters[5].value = dts[-1]                    parameters[6].value = dts[-1]                  else:                    parameters[3].value = None                    parameters[4].value = None                    parameters[5].value = None                    parameters[6].value = None                            # update global values        GDVSPECTRA_CVA = {            'in_file': parameters[0].valueAsText,        }                    return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the GDV Spectra CVA module.        """                # safe imports        import os, sys        # arcgis comes with these        import datetime       # arcgis comes with these        import numpy as np    # arcgis comes with these        import pandas as pd   # arcgis comes with these        import arcpy          # arcgis comes with these                # risky imports (not native to arcgis)        try:            import xarray as xr            import dask        except Exception as e:            arcpy.AddError('Python libraries xarray and dask not installed.')            arcpy.AddMessage(str(e))            return                        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                    # module folder            sys.path.append(FOLDER_MODULES)            import gdvspectra, cog         except Exception as e:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            arcpy.AddMessage(str(e))            return                    # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        warnings.simplefilter(action='ignore', category=dask.array.core.PerformanceWarning)                # grab parameter values         in_raw_nc = parameters[0].valueAsText               # raw input satellite netcdf        in_mask_nc = parameters[1].valueAsText              # mask input satellite netcdf        out_nc = parameters[2].valueAsText                  # output gdv likelihood netcdf        in_base_start_year =  parameters[3].value           # base start year        in_base_end_year =  parameters[4].value             # base end year        in_comp_start_year =  parameters[5].value           # comp start year        in_comp_end_year =  parameters[6].value             # comp end year        in_analysis_months = parameters[7].valueAsText      # analysis months        in_tmf = parameters[8].value                        # magnitude threshold        in_ivt_qupper = parameters[9].value                 # upper quantile for standardisation        in_ivt_qlower = parameters[10].value                # lower quantile for standardisation        in_fmask_flags = parameters[11].valueAsText         # fmask flag values        in_max_cloud = parameters[12].value                 # max cloud percentage        in_interpolate = parameters[13].value               # interpolate missing pixels        in_add_result_to_map = parameters[14].value         # add result to map        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning GDVSpectra CVA.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=18)                                                                                            # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading and checking satellite netcdf...')        arcpy.SetProgressorPosition(1)                try:            # do quick lazy load of satellite netcdf for checking            ds = xr.open_dataset(in_raw_nc)        except Exception as e:            arcpy.AddWarning('Could not quick load input satellite NetCDF data.')            arcpy.AddMessage(str(e))            return                # check xr type, vars, coords, dims, attrs        if not isinstance(ds, xr.Dataset):            arcpy.AddError('Input satellite NetCDF must be a xr dataset.')            return        elif len(ds) == 0:            arcpy.AddError('Input NetCDF has no data/variables/bands.')            return        elif 'x' not in ds.dims or 'y' not in ds.dims or 'time' not in ds.dims:            arcpy.AddError('Input satellite NetCDF must have x, y and time dimensions.')            return        elif 'x' not in ds.coords or 'y' not in ds.coords or 'time' not in ds.coords:            arcpy.AddError('Input satellite NetCDF must have x, y and time coords.')            return        elif 'spatial_ref' not in ds.coords:            arcpy.AddError('Input satellite NetCDF must have a spatial_ref coord.')            return        elif len(ds['x']) == 0 or len(ds['y']) == 0 or len(ds['time']) == 0:            arcpy.AddError('Input satellite NetCDF must have all at least one x, y and time index.')            return        elif 'oa_fmask' not in ds and 'fmask' not in ds:            arcpy.AddError('Expected cloud mask band not found in satellite NetCDF.')            return        elif not hasattr(ds, 'time.year') or not hasattr(ds, 'time.month'):            arcpy.AddError('Input satellite NetCDF must have time with year and month component.')            return        elif len(ds.groupby('time.year')) < 2:            arcpy.AddError('Input satellite NetCDF must have >= 2 years of data.')            return        elif ds.attrs == {}:            arcpy.AddError('Satellite NetCDF must have attributes.')            return        elif not hasattr(ds, 'crs'):            arcpy.AddError('Satellite NetCDF CRS attribute not found. CRS required.')            return        elif ds.crs != 'EPSG:3577':            arcpy.AddError('Satellite NetCDF CRS is not in GDA94 Albers (EPSG:3577).')                        return         elif not hasattr(ds, 'nodatavals'):            arcpy.AddError('Satellite NetCDF nodatavals attribute not found.')                        return                     # efficient: if all nan, 0 at first var, assume rest same, so abort        if ds[list(ds)[0]].isnull().all() or (ds[list(ds)[0]] == 0).all():            arcpy.AddError('Satellite NetCDF has empty variables. Please download again.')                        return                     try:            # now, do proper open of satellite netcdf properly (and set nodata to nan)            ds = satfetcher.load_local_nc(nc_path=in_raw_nc,                                           use_dask=True,                                           conform_nodata_to=np.nan)        except Exception as e:            arcpy.AddError('Could not properly load input satellite NetCDF data.')            arcpy.AddMessage(str(e))            return                                          # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Getting NetCDF attributes...')        arcpy.SetProgressorPosition(2)        # get attributes from dataset        ds_attrs = ds.attrs        ds_band_attrs = ds[list(ds)[0]].attrs        ds_spatial_ref_attrs = ds['spatial_ref'].attrs        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Grouping dates, if required...')        arcpy.SetProgressorPosition(3)                # remove potential datetime duplicates (group by day)        ds = satfetcher.group_by_solar_day(ds)        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Removing invalid pixels and empty dates...')        arcpy.SetProgressorPosition(4)                  # convert fmask as text to numeric code equivalents              in_fmask_flags = [e for e in in_fmask_flags.split(';')]                in_fmask_flags = arc.convert_fmask_codes(in_fmask_flags)                # check if flags selected, if not, select all         if len(in_fmask_flags) == 0:            arcpy.AddWarning('No flags set, selecting default')            in_fmask_flags = [1, 4, 5]                # check numeric flags are valid         for flag in in_fmask_flags:            if flag not in [0, 1, 2, 3, 4, 5, 6]:                arcpy.AddError('Pixel flag not supported.')                return                # check if duplicate flags         u, c = np.unique(in_fmask_flags, return_counts=True)        if len(u[c > 1]) > 0:            arcpy.AddError('Duplicate pixel flags detected.')            return                    # check if mask band exists        mask_band = arc.get_name_of_mask_band(list(ds))                try:            # remove invalid pixels and empty scenes            ds = cog.remove_fmask_dates(ds=ds,                                         valid_class=in_fmask_flags,                                         max_invalid=in_max_cloud,                                         mask_band=mask_band,                                         nodata_value=np.nan,                                         drop_fmask=True)        except Exception as e:            arcpy.AddError('Could not cloud mask pixels.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Conforming satellite band names...')        arcpy.SetProgressorPosition(5)                        try:            # get platform name from attributes, error if no attributes            in_platform = arc.get_platform_from_dea_attrs(ds_attrs)            # conform dea aws band names based on platform            ds = satfetcher.conform_dea_ard_band_names(ds=ds,                                                        platform=in_platform.lower())           except Exception as e:             arcpy.AddError('Could not get platform from attributes.')            arcpy.AddMessage(str(e))            return        # check if all expected bands are in dataset         for band in ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']:            if band not in ds:                arcpy.AddError('Satellite NetCDF is missing band: {}. Need all bands.'.format(band))                return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Reducing dataset months to requested...')        arcpy.SetProgressorPosition(6)        # prepare analysis month(s)        if in_analysis_months == '':            arcpy.AddError('Must include at least one analysis month.')            return                    # unpack month(s)        analysis_months = [int(e) for e in in_analysis_months.split(';')]        try:            # reduce xr dataset into only analysis months            ds = gdvspectra.subset_months(ds=ds,                                           month=analysis_months,                                          inplace=True)        except Exception as e:             arcpy.AddError('Could not subset Satellite NetCDF by analysis months.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Calculating tasselled cap vegetation index...')        arcpy.SetProgressorPosition(7)                        try:            # calculate tasselled cap green and bare            ds = tools.calculate_indices(ds=ds,                                          index=['tcg', 'tcb'],                                          rescale=False,                                          drop=True)            # add band attrs back on            ds['tcg'].attrs = ds_band_attrs               ds['tcb'].attrs = ds_band_attrs        except Exception as e:             arcpy.AddError('Could not calculate tasselled cap index.')            arcpy.AddMessage(str(e))            return                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Reducing month(s) into annual medians...')        arcpy.SetProgressorPosition(8)                # reduce months into annual medians (year starts, YS)        try:            ds = gdvspectra.resample_to_freq_medians(ds=ds,                                                     freq='YS',                                                     inplace=True)                                                                 # add band attrs back on            ds['tcg'].attrs = ds_band_attrs               ds['tcb'].attrs = ds_band_attrs                    except Exception as e:             arcpy.AddError('Could not resample months in Satellite NetCDF.')            arcpy.AddMessage(str(e))            return                    # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Computing data into memory, please wait...')        arcpy.SetProgressorPosition(9)        try:            # compute!             ds = ds.compute()        except Exception as e:             arcpy.AddError('Could not compute dataset. See messages for details.')            arcpy.AddMessage(str(e))            return                # check if all nan again        if ds.to_array().isnull().all():            arcpy.AddError('Satellite NetCDF is empty. Please download again.')                        return                         # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Interpolating dataset, if requested...')        arcpy.SetProgressorPosition(10)         # if requested...        if in_interpolate:            try:                # interpolate along time dimension (linear)                ds = tools.perform_interp(ds=ds, method='full')            except Exception as e:                 arcpy.AddError('Could not interpolate satellite NetCDF.')                arcpy.AddMessage(str(e))                return                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Standardising data to invariant targets...')        arcpy.SetProgressorPosition(11)        # check upper quantile        if in_ivt_qlower < 0 or in_ivt_qlower >= 0.5:            arcpy.AddMessage('Lower quantile must be between 0, 0.5. Setting to default.')            in_ivt_qlower = 0.05        # do same for upper quantile        if in_ivt_qupper <= 0.5 or in_ivt_qupper > 1.0:            arcpy.AddMessage('Upper quantile must be between 0.5, 1.0. Setting to default.')            in_ivt_qlower = 0.99         # check if upper <= lower         if in_ivt_qupper <= in_ivt_qlower:            arcpy.AddError('Upper quantile must be > than lower quantile value.')            return        try:            # standardise to targets            ds = gdvspectra.standardise_to_targets(ds,                                                    q_upper=in_ivt_qupper,                                                    q_lower=in_ivt_qlower)        except Exception as e:             arcpy.AddError('Could not standardise satellite data to invariant targets.')            arcpy.AddMessage(str(e))            return                    # final check to see if data exists        if ds['tcg'].isnull().all() or ds['tcb'].isnull().all():            arcpy.AddError('Could not standardise satellite NetCDF.')            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Performing CVA...')        arcpy.SetProgressorPosition(12)                # check baseline and comparison start and end years        if in_base_end_year < in_base_start_year:            arcpy.AddError('Baseline end year must not be < start year.')            return        elif in_comp_end_year < in_comp_start_year:            arcpy.AddError('Comparison end year must not be < start year.')            return        elif in_comp_start_year < in_base_start_year:            arcpy.AddError('Comparison start year must not be < baseline start year.')            return                # check if baseline and comparison years in dataset        years = ds['time.year']        if in_base_start_year not in years or in_base_end_year not in years:            arcpy.AddError('Baseline start and end years not found in dataset.')            return        elif in_comp_start_year not in years or in_comp_end_year not in years:            arcpy.AddError('Comparison start and end years not found in dataset.')            return                    # check magnitude value         if in_tmf < 0 or in_tmf > 100:            arcpy.AddError('CVA threshold magnitude must be between 0 and 100.')            return        try:            # generate cva            ds_cva = gdvspectra.perform_cva(ds=ds,                                            base_times=(in_base_start_year, in_base_end_year),                                            comp_times=(in_comp_start_year, in_comp_end_year),                                            reduce_comp=False,                                            vege_var = 'tcg',                                            soil_var = 'tcb',                                            tmf=in_tmf)        except Exception as e:             arcpy.AddError('Could not perform CVA.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Isolating CVA magnitude quartiles...')        arcpy.SetProgressorPosition(13)                                                        try:            # isolate cva magnitude via angle quartiles            ds_cva = gdvspectra.isolate_cva_quarters(ds=ds_cva,                                                      drop_orig_vars=True)        except Exception as e:             arcpy.AddError('Could not isolate CVA quartiles.')            arcpy.AddMessage(str(e))            return                                                             # check if variables are empty        if ds_cva.to_array().isnull().all():            arcpy.AddError('CVA dataset is empty. Check range of years and magnitude threshold.')                        return                           # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading, checking and applying mask, if requested...')        arcpy.SetProgressorPosition(14)        # if requested...        if in_mask_nc is not None:                    try:                # do quick lazy load of mask netcdf for checking                ds_mask = xr.open_dataset(in_mask_nc)            except Exception as e:                arcpy.AddWarning('Could not quick load input mask NetCDF data.')                arcpy.AddMessage(str(e))                return                    # check xr type, vars, coords, dims, attrs            if not isinstance(ds_mask, xr.Dataset):                arcpy.AddError('Input mask NetCDF must be a xr dataset.')                return            elif len(ds_mask) == 0:                arcpy.AddError('Input mask NetCDF has no data/variables/bands.')                return            elif 'x' not in ds_mask.dims or 'y' not in ds_mask.dims:                arcpy.AddError('Input mask NetCDF must have x, y dimensions.')                return            elif 'x' not in ds_mask.coords or 'y' not in ds_mask.coords:                arcpy.AddError('Input mask NetCDF must have x, y and time coords.')                return            elif 'spatial_ref' not in ds_mask.coords:                arcpy.AddError('Input mask NetCDF must have a spatial_ref coord.')                return            elif len(ds_mask['x']) == 0 or len(ds_mask['y']) == 0:                arcpy.AddError('Input mask NetCDF must have at least one x, y index.')                return                    elif 'time' in ds_mask:                arcpy.AddError('Input mask NetCDF must not have a time dimension.')                return                                elif 'thresh' not in ds_mask:                arcpy.AddError('Input mask NetCDF must have a "thresh" variable. Run GDVSpectra Threshold.')                return            elif ds_mask.attrs == {}:                arcpy.AddError('Input mask NetCDF attributes not found. NetCDF must have attributes.')                return            elif not hasattr(ds_mask, 'crs'):                arcpy.AddError('Input mask NetCDF CRS attribute not found. CRS required.')                return            elif ds_mask.crs != 'EPSG:3577':                arcpy.AddError('Input mask NetCDF CRS is not EPSG:3577. EPSG:3577 required.')                            return             elif not hasattr(ds_mask, 'nodatavals'):                arcpy.AddError('Input mask NetCDF nodatavals attribute not found.')                            return                     # check if variables (should only be thresh) are empty            if ds_mask['thresh'].isnull().all() or (ds_mask['thresh'] == 0).all():                arcpy.AddError('Input mask NetCDF "thresh" variable is empty. Please download again.')                            return                     try:                # now, do proper open of mask netcdf (set nodata to nan)                ds_mask = satfetcher.load_local_nc(nc_path=in_mask_nc,                                                    use_dask=True,                                                    conform_nodata_to=np.nan)                # compute it!                ds_mask = ds_mask.compute()            except Exception as e:                arcpy.AddError('Could not properly load input mask NetCDF data.')                arcpy.AddMessage(str(e))                return                            try:                # check if like and mask datasets overlap                 if not tools.all_xr_intersect([ds_cva, ds_mask]):                    arcpy.AddError('Input datasets do not intersect.')                    arcpy.AddMessage(str(e))                    return                # resample mask dataset to match likelihood                 ds_mask = tools.resample_xr(ds_from=ds_mask,                                             ds_to=ds_cva,                                            resampling='nearest')                # squeeze                ds_mask = ds_mask.squeeze(drop=True)            except Exception as e:                arcpy.AddError('Could not intersect input datasets.')                arcpy.AddMessage(str(e))                return                                # we made it, so mask cva            ds_cva = ds_cva.where(~ds_mask['thresh'].isnull())            # check if variables are empty            if ds_cva.to_array().isnull().all():                arcpy.AddError('Masked CVA dataset is empty. Check range of years and magnitude threshold.')                            return                                                           # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(15)        # append attrbutes on to dataset and bands        ds_cva.attrs = ds_attrs        ds_cva['spatial_ref'].attrs = ds_spatial_ref_attrs        for var in ds_cva:            ds_cva[var].attrs = ds_band_attrs                                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(16)           try:            # export netcdf file            tools.export_xr_as_nc(ds=ds_cva, filename=out_nc)        except Exception as e:            arcpy.AddError('Could not export dataset.')            arcpy.AddMessage(str(e))            return                                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Adding output to map, if requested...')        arcpy.SetProgressorPosition(17)        # if requested...        if in_add_result_to_map:                        try:                # for current project, open current map                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap                                # remove cva layer if already exists                 for layer in m.listLayers():                    if layer.name == 'cva.crf':                        m.removeLayer(layer)                # create output folder using datetime as name                dt = datetime.datetime.now().strftime('%d%m%Y%H%M%S')                out_folder = os.path.join(os.path.dirname(out_nc), 'cva' + '_' + dt)                os.makedirs(out_folder)                                # disable visualise on map temporarily                arcpy.env.addOutputsToMap = False                                # create crf filename and copy it                out_file = os.path.join(out_folder, 'cva.crf')                crf = arcpy.CopyRaster_management(in_raster=out_nc,                                                   out_rasterdataset=out_file)                                                                  # add to map                                  m.addDataFromPath(crf)             except Exception as e:                arcpy.AddWarning('Could not visualise output, aborting visualisation.')                arcpy.AddMessage(str(e))                pass                        try:                # get symbology, update it                layer = m.listLayers('cva.crf')[0]                sym = layer.symbology                                # if layer has stretch coloriser, apply color                if hasattr(sym, 'colorizer'):                    if sym.colorizer.type == 'RasterStretchColorizer':                        # apply percent clip type                        sym.colorizer.stretchType = 'PercentClip'                        sym.colorizer.minPercent = 0.1                        sym.colorizer.maxPercent = 0.1                        # apply color map                        cmap = aprx.listColorRamps('Spectrum By Wavelength-Full Bright')[0]                        sym.colorizer.colorRamp = cmap                        # apply other basic options                        sym.colorizer.invertColorRamp = False                        sym.colorizer.gamma = 1.0                        # update symbology                        layer.symbology = sym            except Exception as e:                arcpy.AddWarning('Could not colorise output, aborting colorisation.')                arcpy.AddMessage(str(e))                pass        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(18)                # close satellite dataset        ds.close()        del ds                # close cva dataset        ds_cva.close()        del ds_cva                # close mask dataset, if exists         if in_mask_nc is not None:            ds_mask.close()            del ds_mask        # notify user        arcpy.AddMessage('Generated CVA successfully.')                          returnclass Phenolopy_Metrics(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = 'Phenolopy Metrics'        self.description = 'Calculate various metrics that describe various. ' \                           'aspects of vegetation phenology from a data cube. ' \                           'Key metrics include Peak of Season (POS), Start and ' \                           'End of Season (SOS, EOS), and various productivity metrics.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """                # input netcdf file        par_raw_nc_path = arcpy.Parameter(                            displayName='Input satellite NetCDF file',                            name='in_nc',                            datatype='DEFile',                            parameterType='Required',                            direction='Input')        par_raw_nc_path.filter.list = ['nc']                # output netcdf file        par_out_nc_path = arcpy.Parameter(                            displayName='Output Phenometrics NetCDF file',                            name='out_nc',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_path.filter.list = ['nc']                # use all dates        par_use_all_dates = arcpy.Parameter(                              displayName='Combine all input dates',                              name='in_use_all_dates',                              datatype='GPBoolean',                              parameterType='Required',                              direction='Input',                              multiValue=False)        par_use_all_dates.value = True                # set specific year        par_specific_years = arcpy.Parameter(                              displayName='Specific year(s) to analyse',                              name='in_specific_years',                              datatype='GPLong',                              parameterType='Optional',                              direction='Input',                              multiValue=True)        par_specific_years.filter.type = 'ValueList'        par_specific_years.filter.list = []        par_specific_years.value = None                # input metrics        par_metrics = arcpy.Parameter(                        displayName='Phenological metrics',                        name='in_metrics',                        datatype='GPString',                        parameterType='Required',                        direction='Input',                        multiValue=True)        metrics = [            'POS: Peak of season',            'VOS: Valley of season',            'BSE: Base of season',            'MOS: Middle of season',            'AOS: Amplitude of season',            'SOS: Start of season',            'EOS: End of season',            'LOS: Length of season',            'ROI: Rate of increase',            'ROD: Rate of decrease',            'SIOS: Short integral of season',            'LIOS: Long integral of season',            'SIOT: Short integral of total',            'LIOT: Long integral of total',            'NOS: Number of seasons'            ]        par_metrics.filter.type = 'ValueList'                par_metrics.filter.list = metrics        remove = [            'MOS: Middle of season',             'BSE: Base of season',             'AOS: Amplitude of season',            'NOS: Number of seasons'            ]        par_metrics.values = [m for m in metrics if m not in remove]        # input vegetation index         par_veg_idx = arcpy.Parameter(                        displayName='Vegetation index',                        name='in_veg_idx',                        datatype='GPString',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_veg_idx.filter.type = 'ValueList'        par_veg_idx.filter.list = [            'NDVI',            'EVI',             'SAVI',            'MSAVI',            'SLAVI',            'MAVI',            'kNDVI'            ]        par_veg_idx.value = 'MAVI'          # input method type        par_method_type = arcpy.Parameter(                            displayName='Season detection method',                            name='in_method_type',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Season Detection',                            multiValue=False)        par_method_type.filter.list = [            'First of slope',            'Mean of slope',            'Seasonal amplitude',            'Absolute amplitude',            'Relative amplitude'            ]        par_method_type.values = 'Seasonal amplitude'                # input amplitude factor (seaamp, relamp)        par_amp_factor = arcpy.Parameter(                           displayName='Amplitude factor',                           name='in_amp_factor',                           datatype='GPDouble',                           parameterType='Optional',                           direction='Input',                           category='Season Detection',                           multiValue=False)        par_amp_factor.filter.type = 'Range'        par_amp_factor.filter.list = [0.0, 1.0]        par_amp_factor.value = 0.75                # input absolute value (absamp)        par_abs_value = arcpy.Parameter(                          displayName='Absolute value',                          name='in_abs_value',                          datatype='GPDouble',                          parameterType='Optional',                          direction='Input',                          category='Season Detection',                          multiValue=False)        par_abs_value.value = 0.3                # input savitsky window length        par_sav_win_length = arcpy.Parameter(                               displayName='Window size',                               name='in_sav_win_length',                               datatype='GPLong',                               parameterType='Required',                               direction='Input',                               category='Smoothing',                               multiValue=False)        par_sav_win_length.filter.type = 'Range'        par_sav_win_length.filter.list = [3, 99]        par_sav_win_length.value = 3                # input polyorder        par_sav_polyorder = arcpy.Parameter(                          displayName='Polyorder',                          name='in_sav_polyorder',                          datatype='GPLong',                          parameterType='Required',                          direction='Input',                          category='Smoothing',                          multiValue=False)        par_sav_polyorder.filter.type = 'Range'        par_sav_polyorder.filter.list = [1, 100]        par_sav_polyorder.value = 1               # input outlier window length        par_out_win_length = arcpy.Parameter(                               displayName='Window size',                               name='in_out_win_length',                               datatype='GPLong',                               parameterType='Required',                               direction='Input',                               category='Outlier Correction',                               multiValue=False)        par_out_win_length.filter.type = 'Range'        par_out_win_length.filter.list = [3, 99]        par_out_win_length.value = 3                # input outlier factor        par_out_factor = arcpy.Parameter(                               displayName='Outlier removal factor',                               name='in_out_factor',                               datatype='GPLong',                               parameterType='Required',                               direction='Input',                               category='Outlier Correction',                               multiValue=False)        par_out_factor.filter.type = 'Range'        par_out_factor.filter.list = [1, 100]        par_out_factor.value = 2                          # fix edge dates        par_fix_edges = arcpy.Parameter(                          displayName='Ignore edge dates',                          name='in_fix_edges',                          datatype='GPBoolean',                          parameterType='Required',                          direction='Input',                          category='Outlier Correction',                          multiValue=False)        par_fix_edges.value = True         # fill empty pixels        par_fill_nans = arcpy.Parameter(                          displayName='Fill erroroneous pixels',                          name='in_fill_nans',                          datatype='GPBoolean',                          parameterType='Required',                          direction='Input',                          category='Outlier Correction',                          multiValue=False)        par_fill_nans.value = True        # input oa fmask         par_fmask_flags = arcpy.Parameter(                            displayName='Include flags',                            name='in_fmask_flags',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Satellite Quality Options',                            multiValue=True)        flags = ['NoData', 'Valid', 'Cloud', 'Shadow', 'Snow', 'Water']        par_fmask_flags.filter.type = 'ValueList'              par_fmask_flags.filter.list = flags        par_fmask_flags.values = ['Valid', 'Snow', 'Water']                # input max cloud cover        par_max_cloud = arcpy.Parameter(                          displayName='Maximum cloud cover',                          name='in_max_cloud',                          datatype='GPDouble',                          parameterType='Optional',                          direction='Input',                          category='Satellite Quality Options',                          multiValue=False)        par_max_cloud.filter.type = 'Range'        par_max_cloud.filter.list = [0.0, 100.0]        par_max_cloud.value = 10.0                # input add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True        # combine parameters        parameters = [            par_raw_nc_path,            par_out_nc_path,            par_use_all_dates,            par_specific_years,            par_metrics,            par_veg_idx,            par_method_type,            par_amp_factor,            par_abs_value,            par_sav_win_length,            par_sav_polyorder,            par_out_win_length,            par_out_factor,            par_fix_edges,            par_fill_nans,            par_fmask_flags,             par_max_cloud,            par_add_result_to_map            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # imports        try:            import numpy as np            import xarray as xr                   except:            arcpy.AddError('Python libraries xarray not installed.')            return                # globals         global PHENOLOPY_METRICS                # unpack global parameter values         curr_file = PHENOLOPY_METRICS.get('in_file')                # if input file added, run        if parameters[0].value is not None:            # if global has no matching file (or first run), reload all            if curr_file != parameters[0].valueAsText:                try:                    ds = xr.open_dataset(parameters[0].valueAsText)                    dts = np.unique(ds['time.year']).tolist()                    ds.close()                except:                    dts = []                # populate years list                parameters[3].filter.list = dts                # select last year                if len(dts) != 0:                    parameters[3].value = dts[-1]                else:                    parameters[3].value = None                      # update global values        PHENOLOPY_METRICS = {'in_file': parameters[0].valueAsText}                # enable year selector based on combine input checkbox        if parameters[2].value is False:            parameters[3].enabled = True        else:            parameters[3].enabled = False        # enable amp factor or abs value when methods selected        if parameters[6].valueAsText in ['Seasonal amplitude', 'Relative amplitude']:            parameters[7].enabled = True            parameters[8].enabled = False        elif parameters[6].valueAsText == 'Absolute amplitude':            parameters[8].enabled = True            parameters[7].enabled = False        else:             parameters[7].enabled = False            parameters[8].enabled = False                return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the Phenolopy Metrics module.        """                # safe imports        import os, sys       # arcgis comes with these        import datetime      # arcgis comes with these        import numpy as np   # arcgis comes with these        import tempfile      # arcgis comes with these        import arcpy         # arcgis comes with these        # risk imports (non-native to arcgis)        try:            import xarray as xr            import dask        except Exception as e:            arcpy.AddError('Python libraries xarray and dask not installed.')            arcpy.AddMessage(str(e))            return                # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                          # module folder            sys.path.append(FOLDER_MODULES)            import phenolopy, cog           except Exception as e:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            arcpy.AddMessage(str(e))            return                    # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        warnings.simplefilter(action='ignore', category=dask.array.core.PerformanceWarning)                # grab parameter values         in_nc = parameters[0].valueAsText                 # raw input satellite netcdf        out_nc = parameters[1].valueAsText                # output phenometrics netcdf        in_use_all_dates = parameters[2].value            # use all dates in nc         in_specific_years = parameters[3].valueAsText     # set specific year         in_metrics = parameters[4].valueAsText            # phenometrics        in_veg_idx = parameters[5].value                  # vege index name        in_method_type = parameters[6].value              # phenolopy method type        in_amp_factor = parameters[7].value               # amplitude factor        in_abs_value = parameters[8].value                # absolute value        in_sav_window_length = parameters[9].value        # savitsky window length         in_sav_polyorder = parameters[10].value           # savitsky polyorder         in_out_window_length = parameters[11].value       # outlier window length         in_out_factor = parameters[12].value              # outlier cutoff user factor        in_fix_edges = parameters[13].value               # fix edge dates        in_fill_nans = parameters[14].value               # fill nans        in_fmask_flags = parameters[15].valueAsText       # fmask flag values        in_max_cloud = parameters[16].value               # max cloud percentage        in_add_result_to_map = parameters[17].value       # add result to map        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning Phenolopy Metrics.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                             min_range=0, max_range=23)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Loading and checking netcdf...')        arcpy.SetProgressorPosition(1)                try:            # do quick lazy load of netcdf for checking            ds = xr.open_dataset(in_nc)        except Exception as e:            arcpy.AddError('Could not quick load input NetCDF data.')            arcpy.AddMessage(str(e))            return                        # check xr type, vars, coords, dims, attrs        if not isinstance(ds, xr.Dataset):            arcpy.AddError('Input NetCDF must be a xr dataset.')            return        elif len(ds) == 0:            arcpy.AddError('Input NetCDF has no data/variables/bands.')            return        elif 'x' not in ds.dims or 'y' not in ds.dims or 'time' not in ds.dims:            arcpy.AddError('Input NetCDF must have x, y and time dimensions.')            return        elif 'x' not in ds.coords or 'y' not in ds.coords or 'time' not in ds.coords:            arcpy.AddError('Input NetCDF must have x, y and time coords.')            return        elif 'spatial_ref' not in ds.coords:            arcpy.AddError('Input NetCDF must have a spatial_ref coord.')            return        elif len(ds['x']) == 0 or len(ds['y']) == 0 or len(ds['time']) == 0:            arcpy.AddError('Input NetCDF must have all at least one x, y and time index.')            return        elif 'oa_fmask' not in ds and 'fmask' not in ds:            arcpy.AddError('Expected cloud mask band not found in NetCDF.')            return        elif not hasattr(ds, 'time.year') or not hasattr(ds, 'time.month'):            arcpy.AddError('Input NetCDF must have time with year and month component.')            return        elif ds.attrs == {}:            arcpy.AddError('NetCDF must have attributes.')            return        elif not hasattr(ds, 'crs'):            arcpy.AddError('NetCDF CRS attribute not found. CRS required.')            return        elif ds.crs != 'EPSG:3577':            arcpy.AddError('NetCDF CRS is not in GDA94 Albers (EPSG:3577).')                        return         elif not hasattr(ds, 'nodatavals'):            arcpy.AddError('NetCDF nodatavals attribute not found.')                        return         # efficient: if all nan, 0 at first var, assume rest same, so abort        if ds[list(ds)[0]].isnull().all() or (ds[list(ds)[0]] == 0).all():            arcpy.AddError('NetCDF has empty variables. Please download again.')                        return         try:            # now, do proper open of netcdf properly (and set nodata to nan)            ds = satfetcher.load_local_nc(nc_path=in_nc,                                           use_dask=True,                                           conform_nodata_to=np.nan)        except Exception as e:            arcpy.AddError('Could not properly load input NetCDF data.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Getting NetCDF attributes...')        arcpy.SetProgressorPosition(2)        # get attributes from dataset        ds_attrs = ds.attrs        ds_band_attrs = ds[list(ds)[0]].attrs        ds_spatial_ref_attrs = ds['spatial_ref'].attrs        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Grouping dates, if required...')        arcpy.SetProgressorPosition(3)                # remove potential datetime duplicates (group by day)        ds = satfetcher.group_by_solar_day(ds)                                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Removing invalid pixels and empty dates...')        arcpy.SetProgressorPosition(4)          # convert fmask as text to numeric code equivalents        in_fmask_flags = [e for e in in_fmask_flags.split(';')]        in_fmask_flags = arc.convert_fmask_codes(in_fmask_flags)        # check if flags selected, if not, select all         if len(in_fmask_flags) == 0:            arcpy.AddWarning('No flags selected, using default.')            in_fmask_flags = [1, 4, 5]        # check numeric flags are valid         for flag in in_fmask_flags:            if flag not in [0, 1, 2, 3, 4, 5]:                arcpy.AddError('Pixel flag not supported.')                return        # check for duplicate flags         u, c = np.unique(in_fmask_flags, return_counts=True)        if len(u[c > 1]) > 0:            arcpy.AddError('Duplicate pixel flags detected.')            return        # get name of mask band        mask_band = arc.get_name_of_mask_band(list(ds))        try:            # remove invalid pixels and empty scenes            ds = cog.remove_fmask_dates(ds=ds,                                         valid_class=in_fmask_flags,                                         max_invalid=in_max_cloud,                                         mask_band=mask_band,                                         nodata_value=np.nan,                                         drop_fmask=True)        except Exception as e:            arcpy.AddError('Could not mask pixels.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Conforming satellite band names...')        arcpy.SetProgressorPosition(5)        try:            # get platform name from attributes, error if no attributes            in_platform = arc.get_platform_from_dea_attrs(ds_attrs)            # conform dea aws band names based on platform            ds = satfetcher.conform_dea_ard_band_names(ds=ds,                                                        platform=in_platform.lower())           except Exception as e:             arcpy.AddError('Could not get platform from attributes.')            arcpy.AddMessage(str(e))            return        # check if all expected bands are in dataset         for band in ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']:            if band not in ds:                arcpy.AddError('NetCDF is missing band: {}. Need all bands.'.format(band))                return                                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Calculating vegetation index...')        arcpy.SetProgressorPosition(6)         # check if veg idx supported         if in_veg_idx.lower() not in ['ndvi', 'evi', 'savi', 'msavi', 'slavi', 'mavi', 'kndvi']:            arcpy.AddError('Vegetation index not supported.')            return         try:            # calculate vegetation index            ds = tools.calculate_indices(ds=ds,                                          index=in_veg_idx.lower(),                                          custom_name='veg_idx',                                          rescale=False,                                          drop=True)            # add band attrs back on            ds['veg_idx'].attrs = ds_band_attrs           except Exception as e:             arcpy.AddError('Could not calculate vegetation index.')            arcpy.AddMessage(str(e))            return                    # check if we sufficient data temporally        if len(ds['time']) == 0:            arcpy.AddError('Insufficient number of dates in data.')            return        elif len(ds['time'].groupby('time.season')) < 3:            arcpy.AddError('Insufficient number of seasons in data.')            return        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Correcting edge dates...')        arcpy.SetProgressorPosition(7)                # check if user years is valid         if in_use_all_dates is None:            arcpy.AddError('Did not specify combine dates parameter.')            return        elif in_use_all_dates is False and in_specific_years is None:            arcpy.AddError('Did not provide a specific year(s).')            return                    # get list of years, else empty list        if in_use_all_dates is False:            in_specific_years = [int(e) for e in in_specific_years.split(';')]        else:            in_specific_years = None        # check specific years for issues, if specific years exist         if in_specific_years is not None:            if datetime.datetime.now().year == max(in_specific_years):                arcpy.AddError('Cannot use current year, insufficient data.')                return            elif 2011 in in_specific_years or 2012 in in_specific_years:                arcpy.AddError('Cannot use years 2011 or 2012, insufficient data.')                return                       # check if requested years in dataset             for year in in_specific_years:                if year not in ds['time.year']:                    arcpy.AddError('Year {} was not found in dataset.'.format(year))                    return        try:            # enforce first/last date is 1jan/31dec for user years (or all)            ds = phenolopy.enforce_edge_dates(ds=ds,                                               years=in_specific_years)        except Exception as e:             arcpy.AddError('Could not correct edge dates.')            arcpy.AddMessage(str(e))            return                                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Subsetting time-series with buffer dates...')        arcpy.SetProgressorPosition(8)        try:            # subset to requested years (or all) with buffer dates (no subset if no years)            ds = phenolopy.subset_via_years_with_buffers(ds=ds,                                                          years=in_specific_years)        except Exception as e:             arcpy.AddError('Could not subset data with buffer dates.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Resampling time-series to equal-spacing...')        arcpy.SetProgressorPosition(9)                try:            # resample to fortnight medians to ensure equal-spacing            ds = phenolopy.resample(ds=ds,                                     interval='SMS')                except Exception as e:             arcpy.AddError('Could not resample to equal-spacing.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Interpolating initial resample...')        arcpy.SetProgressorPosition(10)                try:            # interpolate our initial resample gaps            ds = phenolopy.interpolate(ds=ds)        except Exception as e:             arcpy.AddError('Could not interpolate initial resample.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Removing spike outliers...')        arcpy.SetProgressorPosition(11)            # check window length and factor is valid         if in_out_window_length < 3 or in_out_window_length > 99:            arcpy.AddError('Outlier window size must be between 3 and 99.')            return        elif in_out_factor < 1 or in_out_factor > 99:            arcpy.AddError('Outlier factor must be between 1 and 99.')            return        try:            #  remove outliers from data using requeted method            ds = phenolopy.remove_spikes(ds=ds,                                          user_factor=in_out_factor,                                          win_size=in_out_window_length)        except Exception as e:             arcpy.AddError('Could not remove spike outliers.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Interpolating removed outliers...')        arcpy.SetProgressorPosition(12)                try:            # interpolate our initial resample gaps            ds = phenolopy.interpolate(ds=ds)        except Exception as e:             arcpy.AddError('Could not interpolate removed outliers.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Smoothing time-series...')        arcpy.SetProgressorPosition(13)        # check window length         if in_sav_window_length <= 0 or in_sav_window_length % 2 == 0:            arcpy.AddWarning('Savitsky window length incompatible, using default.')            in_sav_window_length = 3                        # check polyorder         if in_sav_polyorder >= in_sav_window_length:            arcpy.AddWarning('Savitsky polyorder must be < window length, reducing by one.')            in_sav_polyorder = in_sav_window_length - 1        try:            # smooth dataset across time via savitsky            ds = phenolopy.smooth(ds=ds,                                   var='veg_idx',                                   window_length=in_sav_window_length,                                   polyorder=in_sav_polyorder)        except Exception as e:             arcpy.AddError('Could not smooth data.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Subsetting data to specific years, if requested...')        arcpy.SetProgressorPosition(14)        try:            # subset to requested years, if none, returns input dataset            ds = phenolopy.subset_via_years(ds=ds,                                             years=in_specific_years)        except Exception as e:             arcpy.AddError('Could not subset to specific year(s).')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Grouping time-series by dates...')        arcpy.SetProgressorPosition(15)                try:            # group years by m-d (1-1, 1-15, 2-1, 2-15, etc)            ds = phenolopy.group(ds=ds)        except Exception as e:             arcpy.AddError('Could not group by dates.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Resampling high resolution curves...')        arcpy.SetProgressorPosition(16)                try:            # resample to 365 days per pixel for higher accuracy metrics            ds = phenolopy.resample(ds=ds,                                     interval='1D')        except Exception as e:             arcpy.AddError('Could not interpolate high-resolution curves.')            arcpy.AddMessage(str(e))            return                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Interpolating high-resolution curves...')        arcpy.SetProgressorPosition(17)                try:            # interpolate our initial resample gaps            ds = phenolopy.interpolate(ds=ds)        except Exception as e:             arcpy.AddError('Could not interpolate high-resolution curves.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Checking and removing date oversamples...')        arcpy.SetProgressorPosition(18)                try:            # remove potential oversample dates            ds = phenolopy.drop_overshoot_dates(ds=ds,                                                 min_dates=3)        except Exception as e:             arcpy.AddError('Could not remove oversample dates.')            arcpy.AddMessage(str(e))            return        # check if we have 365 days, otherwise warn         if len(ds['time']) != 365:            arcpy.AddWarning('Could not create 365-day time-series, output may have errors.')        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Computing data into memory, please wait...')        arcpy.SetProgressorPosition(19)        try:            # compute!             ds = ds.compute()        except Exception as e:             arcpy.AddError('Could not compute dataset. See messages for details.')            arcpy.AddMessage(str(e))            return            # check if all nan again        if ds.to_array().isnull().all():            arcpy.AddError('NetCDF is empty. Please download again.')                        return         # # # # #        # notify and set on-going progess bar        arcpy.SetProgressor('default', 'Calculating phenology metrics...')        # check if metrics valid         if in_metrics is None:            arcpy.AddError('No metrics were selected.')            return                    # remove single quotes in metric string (due to spaces) and split         in_metrics = in_metrics.lower().replace("'", '').split(';')        in_metrics = [e.split(':')[0] for e in in_metrics]                # convert method to compatible name        in_method_type = in_method_type.lower()        in_method_type = in_method_type.replace(' ', '_')        # check amplitude factors, absolute values         if in_method_type in ['seasonal_amplitude', 'relative_amplitude']:            if in_amp_factor is None:                arcpy.AddError('Must provide an amplitude factor.')                return            elif in_amp_factor < 0 or in_amp_factor > 1:                arcpy.AddError('Amplitude factor must be between 0 and 1.')                return                        elif in_method_type == 'absolute_amplitude':            if in_abs_value is None:                arcpy.AddError('Must provide an absolute value (any value).')                return        # check if fix edge dates and fill pixels set         if in_fix_edges is None:            arcpy.AddError('Must set the ignore edge dates checkbox.')            return        elif in_fill_nans is None:            arcpy.AddError('Must set the fill erroroneous pixels checkbox.')            return        try:            # calculate phenometrics!            ds = phenolopy.get_phenometrics(ds=ds,                                             metrics=in_metrics,                                            method=in_method_type,                                            factor=in_amp_factor,                                             abs_value=in_abs_value,                                            peak_spacing=12,                                            fix_edges=in_fix_edges,                                            fill_nan=in_fill_nans)        except Exception as e:             arcpy.AddError('Could not calculate phenometrics.')            arcpy.AddMessage(str(e))            return        # check if any data was returned        if ds.to_array().isnull().all():            arcpy.AddError('Metric output contains no values.')            return               # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(20)                # append attrbutes on to dataset and bands        ds.attrs = ds_attrs        ds['spatial_ref'].attrs = ds_spatial_ref_attrs        for var in ds:            ds[var].attrs = ds_band_attrs                                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(21)           try:            # export netcdf file            tools.export_xr_as_nc(ds=ds, filename=out_nc)        except Exception as e:            arcpy.AddError('Could not export dataset.')            arcpy.AddMessage(str(e))            return                                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Adding output to map, if requested...')        arcpy.SetProgressorPosition(22)                # if requested...        if in_add_result_to_map:            try:                # open current map                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap                # remove existing ensemble layers if exist                for layer in m.listLayers():                    if layer.isGroupLayer and layer.name == 'metrics':                        m.removeLayer(layer)                # setup a group layer via template                grp_lyr = arcpy.mp.LayerFile(GRP_LYR_FILE)                grp = m.addLayer(grp_lyr)[0]                grp.name = 'metrics'                        # create output folder using datetime as name                dt = datetime.datetime.now().strftime('%d%m%Y%H%M%S')                out_folder = os.path.join(os.path.dirname(out_nc), 'metrics' + '_' + dt)                os.makedirs(out_folder)                        # disable visualise on map temporarily                arcpy.env.addOutputsToMap = False                # iter each var and export a seperate tif                tif_list = []                for var in ds:                                        # create temp netcdf for one var (prevents 2.9 bug)                    with tempfile.NamedTemporaryFile() as tmp:                        tmp_nc = '{}_{}.nc'.format(tmp.name, var)                        ds[[var]].to_netcdf(tmp_nc)                            # build in-memory crf for temp netcdf                    crf = arcpy.md.MakeMultidimensionalRasterLayer(in_multidimensional_raster=tmp_nc,                                                                    out_multidimensional_raster_layer=var)                                        # export temp tif                    tmp_tif = os.path.join(out_folder, '{}.tif'.format(var))                    tif = arcpy.management.CopyRaster(in_raster=crf,                                                       out_rasterdataset=tmp_tif)                                                                          # add temp tif to map and get as layer                    m.addDataFromPath(tif)                    layer = m.listLayers('{}.tif'.format(var))[0]                                        # hide layer once added                    #layer.visible = False                                        # add layer to group and then remove outside layer                    m.addLayerToGroup(grp, layer, 'BOTTOM')                    m.removeLayer(layer)                                       # success, add store current layer for symbology below                    tif_list.append('{}.tif'.format(var))                                    except Exception as e:                arcpy.AddWarning('Could not visualise output, aborting visualisation.')                arcpy.AddMessage(str(e))                pass                            try:                       # iter tif layer names and update symbology                for tif in tif_list:                    layer = m.listLayers(tif)[0]                    sym = layer.symbology                                                            # if layer has stretch coloriser, apply color                    if hasattr(sym, 'colorizer'):                        if sym.colorizer.type == 'RasterStretchColorizer':                                                    # apply percent clip type                            sym.colorizer.stretchType = 'PercentClip'                                                    # colorize depending on metric                             if 'roi' in tif or 'rod' in tif:                                sym.colorizer.minPercent = 1.0                                sym.colorizer.maxPercent = 1.0                                cmap = aprx.listColorRamps('Inferno')[0]                                                                                       elif 'aos' in tif or 'los' in tif:                                sym.colorizer.minPercent = 0.5                                sym.colorizer.maxPercent = 0.5                                cmap = aprx.listColorRamps('Spectrum By Wavelength-Full Bright')[0]                              elif 'nos' in tif:                                sym.colorizer.stretchType = 'MinimumMaximum'                                cmap = aprx.listColorRamps('Spectrum By Wavelength-Full Bright')[0]                              elif 'times' in tif:                                sym.colorizer.minPercent = 0.25                                sym.colorizer.maxPercent = 0.25                                cmap = aprx.listColorRamps('Temperature')[0]                            elif 'values' in tif:                                sym.colorizer.minPercent = 1.0                                sym.colorizer.maxPercent = 1.0                                cmap = aprx.listColorRamps('Precipitation')[0]                                                            # apply color map                            sym.colorizer.colorRamp = cmap                            # apply other basic options                            sym.colorizer.invertColorRamp = False                            sym.colorizer.gamma = 1.0                            # update symbology                            layer.symbology = sym                                                        # show layer                             #layer.visible = True                                        except Exception as e:                arcpy.AddWarning('Could not colorise output, aborting colorisation.')                arcpy.AddMessage(str(e))                pass        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(23)                # close and del dataset        ds.close()        del ds        # notify user        arcpy.AddMessage('Generated Phenometrics successfully.')        returnclass Nicher_SDM(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = 'Nicher SDM'        self.description = 'Generate a species distribution model (SDM) using ' \                           'a shapefile of known species occurrence field points ' \                           'and two or more digital elevation model (DEM)-derived ' \                           'topographic variables representing potential habitat.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """                # input continous tif(s)        par_in_continuous_tifs = arcpy.Parameter(                                  displayName='Input GeoTiff(s) of continuous variables',                                  name='in_continuous_tifs',                                  datatype='GPRasterLayer',                                  parameterType='Optional',                                  direction='Input',                                  multiValue=True)         # input categorical tif(s)        par_in_categorical_tifs = arcpy.Parameter(                                    displayName='Input GeoTiff(s) of categorical variables',                                    name='in_categorical_tifs',                                    datatype='GPRasterLayer',                                    parameterType='Optional',                                    direction='Input',                                    multiValue=True)                # output netcdf file        par_out_nc_path = arcpy.Parameter(                            displayName='Output Nicher NetCDF file',                            name='out_nc',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_path.filter.list = ['nc']        # occurrence points        par_occurrence_feat = arcpy.Parameter(                                displayName='Occurrence point feature',                                name='in_occurrence_feat',                                datatype='GPFeatureLayer',                                parameterType='Required',                                direction='Input',                                multiValue=False)        par_occurrence_feat.filter.list = ['Point']                # number of pseudoabsences        par_num_absence = arcpy.Parameter(                            displayName='Number of pseudoabsence points',                            name='in_num_pseudos',                            datatype='GPLong',                            parameterType='Required',                            direction='Input',                            multiValue=False)        par_num_absence.filter.type = 'Range'        par_num_absence.filter.list = [1, 9999999]        par_num_absence.value = 1000                # exclusion buffer        par_exclusion_buffer = arcpy.Parameter(                                 displayName='Exclusion buffer (in metres)',                                 name='in_exclusion_buffer',                                 datatype='GPLong',                                 parameterType='Required',                                 direction='Input',                                 multiValue=False)        par_exclusion_buffer.filter.type = 'Range'        par_exclusion_buffer.filter.list = [1, 9999999]        par_exclusion_buffer.value = 500                # equalise pseudoabsence        par_equalise_absence = arcpy.Parameter(                                 displayName='Equalise number of pseudoabsence points',                                 name='in_equalise_pseudos',                                 datatype='GPBoolean',                                 parameterType='Required',                                 direction='Input',                                 multiValue=False)        par_equalise_absence.value = False                  # input test ratio        par_test_ratio = arcpy.Parameter(                           displayName='Proportion of data for testing',                           name='in_test_ratio',                           datatype='GPDouble',                           parameterType='Required',                           direction='Input',                           multiValue=False)        par_test_ratio.filter.type = 'Range'        par_test_ratio.filter.list = [0, 1]        par_test_ratio.value = 0.1                     # input resample        par_resample = arcpy.Parameter(                         displayName='Resample resolution',                         name='in_resample',                         datatype='GPString',                         parameterType='Required',                         direction='Input',                         multiValue=False)        resample_to = ['Highest Resolution', 'Lowest Resolution']        par_resample.filter.type = 'ValueList'              par_resample.filter.list = resample_to        par_resample.value = 'Lowest Resolution'                  # number of model estimators        par_num_estimators = arcpy.Parameter(                               displayName='Number of model estimators',                               name='in_num_estimators',                               datatype='GPLong',                               parameterType='Required',                               direction='Input',                               category='Model Parameters',                               multiValue=False)        par_num_estimators.filter.type = 'Range'        par_num_estimators.filter.list = [1, 9999999]        par_num_estimators.value = 100                # model criterion        par_criterion = arcpy.Parameter(                          displayName='Criterion',                          name='in_criterion',                          datatype='GPString',                          parameterType='Required',                          direction='Input',                          category='Model Parameters',                          multiValue=False)        par_criterion.filter.type = 'ValueList'              par_criterion.filter.list = ['Gini', 'Entropy']        par_criterion.value = 'Gini'                # max depth        par_max_depth = arcpy.Parameter(                          displayName='Maximum tree depth',                          name='in_max_depth',                          datatype='GPLong',                          parameterType='Optional',                          direction='Input',                          category='Model Parameters',                          multiValue=False)        par_max_depth.filter.type = 'Range'        par_max_depth.filter.list = [1, 9999999]        par_max_depth.value = None                # max_features        par_max_features = arcpy.Parameter(                             displayName='Maximum features',                             name='in_max_features',                             datatype='GPString',                             parameterType='Required',                             direction='Input',                             category='Model Parameters',                             multiValue=False)        par_max_features.filter.type = 'ValueList'              par_max_features.filter.list = ['Auto', 'Log2']        par_max_features.value = 'Auto'                        # boostrap        par_boostrap = arcpy.Parameter(                             displayName='Boostrap',                             name='in_boostrap',                             datatype='GPBoolean',                             parameterType='Required',                             direction='Input',                             category='Model Parameters',                             multiValue=False)        par_boostrap.value = True               # input add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True                # combine parameters        parameters = [            par_in_continuous_tifs,            par_in_categorical_tifs,            par_out_nc_path,            par_occurrence_feat,            par_num_absence,            par_exclusion_buffer,            par_equalise_absence,            par_test_ratio,            par_resample,            par_num_estimators,            par_criterion,            par_max_depth,            par_max_features,            par_boostrap,            par_add_result_to_map            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the Nicher SDM module.        """                # safe imports        import os, sys                                # arcgis comes with these        import datetime                               # arcgis comes with these        import numpy as np                            # arcgis comes with these        import pandas as pd                           # arcgis comes with these        import arcpy                                  # arcgis comes with these        # risk imports (non-native to arcgis)        try:            import xarray as xr            import dask        except Exception as e:            arcpy.AddError('Python libraries xarray and dask not installed.')            arcpy.AddMessage(str(e))            return                # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                          # module folder            sys.path.append(FOLDER_MODULES)            import nicher          except Exception as e:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            arcpy.AddMessage(str(e))            return                    # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        warnings.simplefilter(action='ignore', category=dask.array.core.PerformanceWarning)        # grab parameter values         in_cont_tifs = parameters[0].valueAsText          # continous tifs        in_cate_tifs = parameters[1].valueAsText          # categorical tifs        out_nc = parameters[2].valueAsText                # output netcdf        in_occurrence_feat = parameters[3]                # occurrence point shapefile           in_num_absence = parameters[4].value              # num absences         in_exclusion_buffer = parameters[5].value         # exclusion buffer        in_equalise_absence = parameters[6].value         # equalise absences        in_test_ratio = parameters[7].value               # test ratio        in_resample = parameters[8].value                 # resample        in_num_estimator = parameters[9].value            # number of estimators        in_criterion = parameters[10].value               # criterion type        in_max_depth = parameters[11].value               # max tree depth        in_max_features = parameters[12].value            # maximum features        in_bootstrap = parameters[13].value               # boostrap        in_add_result_to_map = parameters[14].value       # add result to map        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning Nicher SDM.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                             min_range=0, max_range=14)                                                                                            # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Preparing GeoTiffs...')        arcpy.SetProgressorPosition(1)                # check if at least on layer provided         if in_cont_tifs is None and in_cate_tifs is None:            arcpy.AddError('Must provide at least one GeoTiff.')            return                    # convert continuous tifs to list if exists         cont_tifs = []        if in_cont_tifs is not None:            for tif in [t.replace("'", '') for t in in_cont_tifs.split(';')]:                desc = arcpy.Describe(tif)                cont_tifs.append(os.path.join(desc.path, desc.name))        # convert categorical tifs to list if exists         cate_tifs = []        if in_cate_tifs is not None:            for tif in [t.replace("'", '') for t in in_cate_tifs.split(';')]:                desc = arcpy.Describe(tif)                cate_tifs.append(os.path.join(desc.path, desc.name))        # ensure all tifs are unique         _, c = np.unique(cont_tifs + cate_tifs, return_counts=True)        if len(np.unique(c)) > 1:            arcpy.AddError('Duplicate input GeoTiffs provided.')            return                # ensure we have at least two variables         if len(cont_tifs + cate_tifs) < 2:            arcpy.AddError('At least two variables are required.')            return                # ensure all files are tifs         for tif in cont_tifs + cate_tifs:            if not tif.endswith('.tif'):                arcpy.AddError('Only GeoTiff raster type supported.')                return                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Loading and checking GeoTiffs...')        arcpy.SetProgressorPosition(2)                # iterate layers for check        ds_list = []        for tif in cont_tifs + cate_tifs:                    try:                ds = xr.open_rasterio(tif)                ds = ds.to_dataset(dim='band')                              except Exception as e:                arcpy.AddError('Could not quick load input GeoTiff {}.'.format(tif))                arcpy.AddMessage(str(e))                return                            # check xr type, vars, coords, dims, attrs            if not isinstance(ds, xr.Dataset):                arcpy.AddError('Input GeoTiff must be an xr dataset.')                return            elif not tif.endswith('.tif'):                arcpy.AddError('File is not a GeoTiff.')                return            elif len(ds) == 0:                arcpy.AddError('Input GeoTiff has no data/variables/bands.')                return            elif len(ds) != 1:                arcpy.AddError('Input GeoTiff has multiple bands.')                return            elif 'x' not in list(ds.coords) or 'y' not in list(ds.coords):                arcpy.AddError('Input GeoTiff must have x, y coords.')                return            elif 'x' not in list(ds.dims) or 'y' not in list(ds.dims):                arcpy.AddError('Input GeoTiff must have x, y dimensions.')                return            elif len(ds['x']) == 0 or len(ds['y']) == 0:                arcpy.AddError('Input GeoTiff must have at least one x, y index.')                return            elif ds.attrs == {}:                arcpy.AddError('GeoTiff attributes not found. GeoTiff must have attributes.')                return            elif not hasattr(ds, 'crs'):                arcpy.AddError('GeoTiff CRS attribute not found. CRS required.')                return            elif '3577' not in ds.crs:                arcpy.AddError('GeoTiff CRS is not EPSG:3577. EPSG:3577 required.')                            return            elif not hasattr(ds, 'nodatavals'):                arcpy.AddError('GeoTiff nodatavals attribute not found.')                            return            try:                # do proper load with dask, set nodata to nan                ds = satfetcher.load_local_rasters(rast_path_list=tif,                                                    use_dask=True,                                                    conform_nodata_to=np.nan)                ds = tools.manual_create_xr_attrs(ds=ds)            except Exception as e:                arcpy.AddError('Could not properly load input GeoTiff {}.'.format(tif))                arcpy.AddMessage(str(e))                return            # check if xr is all nan            if ds.to_array().isnull().all():                arcpy.AddError('Input GeoTiff is completely null.')                            return                            # append to dataset list            ds_list.append(ds)        # check xr list isnt empty         if len(ds_list) == 0:            arcpy.AddError('No GeoTiffs were successfully loaded.')                        return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Conforming GeoTiffs via resampling...')        arcpy.SetProgressorPosition(3)                # check extents overlap        if not tools.all_xr_intersect(ds_list):            arcpy.AddError('Not all input layers intersect.')                        return         # check resample        if in_resample not in ['Lowest Resolution', 'Highest Resolution']:            arcpy.AddError('Resample type not supported.')            return        try:            # select target resolution dataset            ds_target = tools.get_target_res_xr(ds_list,                                                 in_resample)        except Exception as e:            arcpy.AddError('Could not get target GeoTiff resolution.')            arcpy.AddMessage(str(e))            return        # check target xr captured        if ds_target is None:            arcpy.AddError('Could not obtain optimal GeoTiff resolution.')                        return            try:            # resample all datasets to target dataset            for idx in range(len(ds_list)):                ds_list[idx] = tools.resample_xr(ds_from=ds_list[idx],                                                  ds_to=ds_target,                                                 resampling='nearest')                                # squeeze to be safe!                ds_list[idx] = ds_list[idx].squeeze(drop=True)        except Exception as e:            arcpy.AddError('Could not resample GeoTiffs.')            arcpy.AddMessage(str(e))            return                     # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Combining GeoTiffs together...')        arcpy.SetProgressorPosition(4)        try:            # merge vars into one dataset, fix attrs            ds = xr.merge(ds_list)            ds = tools.manual_create_xr_attrs(ds=ds)        except Exception as e:            arcpy.AddError('Could not combine GeoTiffs.')            arcpy.AddMessage(str(e))            return         # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Reducing data to smallest GeoTiff extent...')        arcpy.SetProgressorPosition(5)        try:            # ensure bounding box is fixed to smallest mbr            ds = tools.remove_nan_xr_bounds(ds=ds)        except Exception as e:            arcpy.AddError('Could not reduce to smallest GeoTiff extent.')            arcpy.AddMessage(str(e))            return         # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Computing GeoTiffs into memory, please wait...')        arcpy.SetProgressorPosition(6)                try:            # compute!             ds = ds.compute()        except Exception as e:             arcpy.AddError('Could not compute GeoTiffs. See messages for details.')            arcpy.AddMessage(str(e))            return                    # check if all nan again        if ds.to_array().isnull().all():            arcpy.AddError('GeoTiff data is empty. Please check GeoTiffs.')                        return         # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Preparing occurrence points...')        arcpy.SetProgressorPosition(7)          # check if shapefile provided         if in_occurrence_feat.value is None:            arcpy.AddError('No occurrence point shapefile provided.')            return                    # ensure dataset has a required nodatavals attribute        ds.attrs.update({'nodatavals': np.nan})                try:            # get full path to feature            desc = arcpy.Describe(in_occurrence_feat)            in_occurrence_feat = os.path.join(desc.path, desc.name)                        # read shapefile (arcpy), convert to x, y dataframe, check validity            df_pres = arc.read_shp_for_nicher(in_occurrence_feat)                        # extract values for presence points, keep x, y            df_pres = tools.extract_xr_values(ds=ds,                                               coords=df_pres,                                               keep_xy=True,                                               res_factor=3)                                                          # remove any nodata (nan) values            df_pres = tools.remove_nodata_records(df_records=df_pres,                                                  nodata_value=np.nan)          except Exception as e:            arcpy.AddError('Could not prepare occurrence points, see messages for details.')            arcpy.AddMessage(str(e))            return                    # ensure we have presence points remaining         if len(df_pres) == 0:            arcpy.AddError('No occurrence points remaining after extraction.')                        return         # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Preparing pseudoabsence points...')        arcpy.SetProgressorPosition(8)          # check exclusion buffer size and num pseudoabsence        if in_exclusion_buffer < 1:            arcpy.AddError('Exclusion buffer not between 1 and 1000000 metres.')            return        elif in_num_absence < 1:            arcpy.AddError('Number of psuedoabsences not between 1 and 10000.')            return                    # prepare random sample size based on selection        num_to_sample = in_num_absence        if in_equalise_absence is True:            num_to_sample = len(df_pres)        try:                        # generate absences pixels and presence buffer (arcpy)            df_abse = arc.generate_absences_for_nicher(df_pres=df_pres,                                                        ds=ds,                                                       buff_m=in_exclusion_buffer)             # randomly sample pseudoabsence points (these are all non-nan)            df_abse = nicher.random_sample_absences(df_abse=df_abse,                                                     num_to_sample=num_to_sample)                                                                # extract values for absence points, keep x, y            df_abse = tools.extract_xr_values(ds=ds,                                               coords=df_abse,                                               keep_xy=True,                                               res_factor=3)                                                          # remove any nodata (nan) values            df_abse = tools.remove_nodata_records(df_records=df_abse,                                                  nodata_value=np.nan)        except Exception as e:            arcpy.AddError('Could not generate psuedoabsence points, see messages for details.')            arcpy.AddMessage(str(e))            return         # ensure we have absence points         if len(df_abse) == 0:            arcpy.AddError('No pseudoabsence points remain.')            return                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Combining presence and absence points...')        arcpy.SetProgressorPosition(9)                         # get list of all continuous var names        cont_vars = []        for c in cont_tifs:            fp = os.path.splitext(c)[0]            cont_vars.append(os.path.basename(fp))                    # get list of all categorical var names        cate_vars = []        for c in cate_tifs:            fp = os.path.splitext(c)[0]            cate_vars.append(os.path.basename(fp))                try:            # combine pres/abse data and add "pres_abse" column            df_pres_abse = nicher.combine_pres_abse_records(df_presence=df_pres,                                                             df_absence=df_abse)                        # drop all useless vars             keep_vars = ['pres_abse'] + cont_vars + cate_vars            for col in df_pres_abse.columns:                if col not in keep_vars:                    df_pres_abse = df_pres_abse.drop(columns=[col], errors='ignore')        except Exception as e:            arcpy.AddError('Could not combine presence and absence data.')            arcpy.AddMessage(str(e))            return                     # check we have columns and data        if len(df_pres_abse) == 0 or len(df_pres_abse.columns) == 0:            arcpy.AddError('No columns and/or data exists after processing occurence points.')            return                                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Calculating exploratory statistics...')        arcpy.SetProgressorPosition(10)         # if we have continuous vars, proceed...        if len(cont_vars) != 0:            try:                # make temporary df and drop categoricals, ignore if non                df_tmp = df_pres_abse.copy(deep=True)                df_tmp = df_tmp.drop(cate_vars, errors='ignore')                # calculate pearson correlation as text and display                result = nicher.generate_correlation_matrix(df_records=df_tmp)                arcpy.AddMessage(result)                                # calculate variance impact factor as text and display                result = nicher.generate_vif_scores(df_records=df_tmp)                  arcpy.AddMessage(result)            except Exception as e:                arcpy.AddError('Could not calculate exploratory statistics.')                arcpy.AddMessage(str(e))                return                        # # # # #        # notify and set on-going progess bar        arcpy.SetProgressor('default', 'Performing species distribution modelling...')                # check parameters are valid        if in_num_estimator < 1:            arcpy.AddError('Number of model estimators not between 1 and 10000.')            return        elif in_criterion not in ['Gini', 'Entropy']:            arcpy.AddError('Criterion must be Gini or Entropy.')            return        elif in_max_depth is not None and in_max_depth < 1:            arcpy.AddError('Maximum depth must be empty or > 0.')            return        elif in_max_features not in ['Auto', 'Log2']:            arcpy.AddError('Maximum features must be Auto or Log2.')            return        elif in_bootstrap not in [True, False]:            arcpy.AddError('Boostrap must be either True or False.')            return        # prepare options         options = {            'n_estimators': in_num_estimator,            'criterion': in_criterion.lower(),            'max_depth': in_max_depth,            'max_features': in_max_features.lower(),            'bootstrap': in_bootstrap        }                # check test ratio         if in_test_ratio <= 0 or in_test_ratio >= 1:            arcpy.AddError('Testing ratio must be between 0 and 1.')            return        try:                            # generate sdm dataset and capture accuracy measures (result)            ds_sdm, result = nicher.perform_sdm(ds=ds,                                                 df=df_pres_abse,                                                test_ratio=in_test_ratio,                                                options=options)            # display accuracy results            arcpy.AddMessage(result)        except Exception as e:            arcpy.AddError('Could not generate model, see messages for details.')            arcpy.AddMessage(str(e))            return                     # check sdm dataset if all nan        if ds_sdm.to_array().isnull().all():            arcpy.AddError('SDM dataset result is empty.')                        return                                             # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(11)                try:            # manually create attrs for dataset (geotiffs lacking)             ds_sdm = tools.manual_create_xr_attrs(ds_sdm)            ds_sdm.attrs.update({'nodatavals': np.nan})                   except Exception as e:            arcpy.AddError('Could not append attributes onto dataset.')            arcpy.AddMessage(str(e))            return                                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(12)           try:            # export netcdf file            tools.export_xr_as_nc(ds=ds_sdm, filename=out_nc)        except Exception as e:             arcpy.AddError('Could not export dataset.')            arcpy.AddMessage(str(e))            return                        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Adding output to map, if requested...')        arcpy.SetProgressorPosition(13)        # if requested...        if in_add_result_to_map:            try:                # open current map                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap                                # remove existing sdm layer if exist                for layer in m.listLayers():                    if layer.name == 'sdm.crf':                        m.removeLayer(layer)                                        # create output folder using datetime as name                dt = datetime.datetime.now().strftime('%d%m%Y%H%M%S')                out_folder = os.path.join(os.path.dirname(out_nc), 'sdm' + '_' + dt)                os.makedirs(out_folder)                                # disable visualise on map temporarily                arcpy.env.addOutputsToMap = False                                # create crf filename and copy it                out_file = os.path.join(out_folder, 'sdm.crf')                crf = arcpy.CopyRaster_management(in_raster=out_nc,                                                   out_rasterdataset=out_file)                # add to map                                  m.addDataFromPath(crf)                           except Exception as e:                arcpy.AddWarning('Could not visualise output, aborting visualisation.')                arcpy.AddMessage(str(e))                pass                            try:                # get symbology, update it                layer = m.listLayers('sdm.crf')[0]                sym = layer.symbology                # if layer has stretch coloriser, apply color                if hasattr(sym, 'colorizer'):                    if sym.colorizer.type == 'RasterStretchColorizer':                        # apply percent clip type                        sym.colorizer.stretchType = 'PercentClip'                        sym.colorizer.minPercent = 0.1                        sym.colorizer.maxPercent = 0.1                        # apply color map                        cmap = aprx.listColorRamps('Spectrum By Wavelength-Full Bright')[0]                        sym.colorizer.colorRamp = cmap                        # apply other basic options                        sym.colorizer.invertColorRamp = False                        sym.colorizer.gamma = 1.0                        # update symbology                        layer.symbology = sym            except Exception as e:                arcpy.AddWarning('Could not colorise output, aborting colorisation.')                arcpy.AddMessage(str(e))                pass                                                        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(14)                # close and del satellite dataset        ds.close()        del ds                # close and del satellite dataset        ds_sdm.close()        del ds_sdm        # notify user        arcpy.AddMessage('Generated Nicher SDM successfully.')                returnclass Nicher_Masker(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = "Nicher Masker"        self.description = "Use another raster layer to mask out areas " \                           "from SDM outputs."        self.canRunInBackground = False    def getParameterInfo(self):        """        Set various ArcGIS Pro UI controls. Data validation        is enforced via ArcGIS Pro API.        """                # input sdm netcdf file        par_sdm_nc_path = arcpy.Parameter(                            displayName='Input SDM NetCDF file',                            name='in_sdm_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Input')        par_sdm_nc_path.filter.list = ['nc']                # input mask tif or netcdf file        par_mask_file_path = arcpy.Parameter(                            displayName='Input mask NetCDF or GeoTIFF file',                            name='in_mask_file_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Input')        par_mask_file_path.filter.list = ['tif', 'nc']                # output netcdf file        par_out_nc_path = arcpy.Parameter(                            displayName='Output masked SDM Netcdf file',                            name='out_masked_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_path.filter.list = ['nc']                # input minimum value        par_min_value = arcpy.Parameter(                          displayName='Minimum value',                          name='in_min_value',                          datatype='GPDouble',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_min_value.value = 2        # input max value        par_max_value = arcpy.Parameter(                          displayName='Maximum value',                          name='in_max_value',                          datatype='GPDouble',                          parameterType='Optional',                          direction='Input',                          multiValue=False)                # input replacement value        par_replacement_value = arcpy.Parameter(                                  displayName='Replacement value',                                  name='in_replacement_value',                                  datatype='GPDouble',                                  parameterType='Required',                                  direction='Input',                                  multiValue=False)        par_replacement_value.value = 0                parameters = [            par_sdm_nc_path,            par_mask_file_path,            par_out_nc_path,            par_min_value,            par_max_value,            par_replacement_value        ]        return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the Nicher Mask module.        """                # safe imports        import os, sys                         # arcgis comes with these        import datetime                        # arcgis comes with this        import numpy as np                     # arcgis comes with this        # risk imports (non-native to arcgis)        try:            import xarray as xr                # not in arcgis        except:            arcpy.AddError('Python library Xarray is not installed.')            return                # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                          # module folder            sys.path.append(FOLDER_MODULES)            import nicher, cog                      except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return                    # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        # grab parameter values         in_sdm_nc_path = parameters[0].valueAsText     # input sdm netcdf        in_mask_file_path = parameters[1].valueAsText  # input mask geotiff / netcdf        out_nc_path = parameters[2].valueAsText        # output masked sdm netcdf        in_min_value = parameters[3].value             # input min value        in_max_value = parameters[4].value             # input max value        in_replacement_value = parameters[5].value     # input replacement value                        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning Nicher Species Distribution Model Masker.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                             min_range=0, max_range=8)                                    # get mask file type               mask_filetype = 'tif' if in_mask_file_path.endswith('.tif') else 'nc'                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Reading SDM NetCDF...')        arcpy.SetProgressorPosition(1)                # load netcdf. set nodata to nan to mimic dea odc        ds_sdm = satfetcher.load_local_nc(nc_path=in_sdm_nc_path,                                           use_dask=True,                                           conform_nodata_to=np.nan)                                              # check netcdf if it has bands, get attributes for ds and a band               if len(ds_sdm.data_vars) == 0:            arcpy.AddError('No bands/variables detected in input NetCDF.')            return        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Getting Dataset attributes...')        arcpy.SetProgressorPosition(2)                # get attributes from dataset        ds_attrs = ds_sdm.attrs        ds_band_attrs = ds_sdm[list(ds_sdm.data_vars)[0]].attrs        ds_spatial_ref_attrs = ds_sdm['spatial_ref'].attrs                           # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Reading SDM Mask NetCDF/GeoTIFF...')        arcpy.SetProgressorPosition(2)                        # open input depeneding on file type        if mask_filetype == 'tif':            ds_mask = satfetcher.load_local_rasters(rast_path_list=in_mask_file_path,                                                     use_dask=True,                                                     conform_nodata_to=np.nan)        else:            ds_mask = satfetcher.load_local_nc(nc_path=in_mask_file_path,                                                use_dask=True,                                                conform_nodata_to=np.nan)                # check netcdf if it has bands, get attributes for ds and a band        if isinstance(ds_mask, (xr.DataArray)):            ds_mask = ds_mask.to_dataset(dim='variable')        elif len(ds_mask.data_vars) == 0:            arcpy.AddError('No bands/variables detected in input mask file.')            return                    # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Clipping and resampling SDM Mask to match SDM NetCDF...')        arcpy.SetProgressorPosition(3)           # clip and resample the mask to extent and size of sdm netcdf        ds_mask = tools.clip_xr_to_xr(ds_a=ds_mask, ds_b=ds_sdm, inplace=True)        ds_mask = tools.resample_xr(ds_from=ds_mask, ds_to=ds_sdm, resampling='nearest')        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Computing SDM and SDM Mask datasets into memory...')        arcpy.SetProgressorPosition(4)          # compute into memory        ds_sdm = ds_sdm.compute()        ds_mask = ds_mask.compute()                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Preparing SDM Mask...')        arcpy.SetProgressorPosition(5)                       # prepare mask        if in_max_value is not None:            ds_mask = xr.where((ds_mask > in_min_value) &                                (ds_mask < in_max_value), True, False)        else:            ds_mask = xr.where(ds_mask > in_min_value, True, False)                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Applying mask to SDM data...')        arcpy.SetProgressorPosition(6)                # if mask is still a dataset, convert to array        if isinstance(ds_mask, xr.Dataset):            ds_mask = ds_mask.to_array().squeeze(drop=True)                                # mask out lowest value and set to replacement value        ds_sdm = ds_sdm.where(ds_mask, in_replacement_value)                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(7)                # append attrbutes on to dataset and bands        ds_sdm.attrs = ds_attrs        ds_sdm['spatial_ref'].attrs = ds_spatial_ref_attrs        for var in list(ds_sdm.data_vars):            ds_sdm[var].attrs = ds_band_attrs                        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Exporting NetCDF...')        arcpy.SetProgressorPosition(8)                        # export netcdf file        tools.export_xr_as_nc(ds=ds_sdm, filename=out_nc_path)                        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(9)                # close and del dataset        ds_sdm.close()        ds_mask.close()        del ds_sdm, ds_mask        # notify user        arcpy.AddMessage('Generated SDM Mask successfully.')                returnclass VegFrax_Fractional_Cover(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = "VegFrax Fractional Cover"        self.description = "Extrapolate small areas of high resolution " \                           "classifed imagery across larger areas of lower " \                           "resolution imagery, such as Landsat or Sentinel."        self.canRunInBackground = False    def getParameterInfo(self):        """        Set various ArcGIS Pro UI controls. Data validation        is enforced via ArcGIS Pro API.        """                # input low res satellite netcdf/tif file        par_low_res_path = arcpy.Parameter(                             displayName='Input low-resolution Satellite data NetCDF',                             name='in_low_res_path',                             datatype='DEFile',                             parameterType='Required',                             direction='Input')        par_low_res_path.filter.list = ['nc']        # input high res satellite tif file        par_high_res_path = arcpy.Parameter(                              displayName='Input classified high-resolution GeoTIF',                              name='in_high_res_path',                              datatype='DEFile',                              parameterType='Required',                              direction='Input')        par_high_res_path.filter.list = ['tif']                        # output netcdf file        par_out_nc_path = arcpy.Parameter(                            displayName='Output Fractional Cover Netcdf',                            name='out_vegfrax_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_path.filter.list = ['nc']                      # input start year for low res netcdf/file        par_from_date = arcpy.Parameter(                                 displayName='Subset date of low resolution data from',                                 name='in_from_date',                                 datatype='GPDate',                                 parameterType='Required',                                 direction='Input',                                 multiValue=False)        par_from_date.values = '2015/01/01'                # input end year for low res netcdf/file        par_date_to = arcpy.Parameter(                        displayName='Subset date of low resolution data to',                        name='in_to_date',                        datatype='GPDate',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_date_to.values = '2020/12/31'                # input aggregator        par_aggregator = arcpy.Parameter(                                    displayName='Aggregator',                                    name='in_aggregator',                                    datatype='GPString',                                    parameterType='Required',                                    direction='Input',                                    multiValue=False)        par_aggregator.filter.type = 'ValueList'        par_aggregator.filter.list = [            'Mean',            'Median'            ]        par_aggregator.value = 'Median'                 # input classes list from high res netcdf/tif         par_classes = arcpy.Parameter(                        displayName='Set fractonal classes',                        name='in_classes',                        datatype='GPString',                        parameterType='Required',                        direction='Input',                        multiValue=True,                        enabled=False)        classes = ['No Classes']        par_classes.filter.type = 'ValueList'              par_classes.filter.list = classes        par_classes.values = ['Class: NoClass']                # input merge selected classes        par_merge_classes = arcpy.Parameter(                              displayName='Merge selected classes',                              name='in_merge_classes',                              datatype='GPBoolean',                              parameterType='Required',                              direction='Input',                              multiValue=False,                              enabled=False)        par_merge_classes.value = False                # input number of samples        par_num_samples = arcpy.Parameter(                            displayName='Number of random samples per class',                            name='in_num_samples',                            datatype='GPLong',                            parameterType='Required',                            direction='Input',                            category='VegFrax Options',                            multiValue=False)        par_num_samples.filter.type = 'Range'        par_num_samples.filter.list = [10, 10000]        par_num_samples.value = 200                # input number of model estimators        par_num_estimators = arcpy.Parameter(                               displayName='Number of model estimators',                               name='in_num_estimators',                               datatype='GPLong',                               parameterType='Required',                               direction='Input',                               category='VegFrax Options',                               multiValue=False)        par_num_estimators.filter.type = 'Range'        par_num_estimators.filter.list = [5, 1000]        par_num_estimators.value = 100                # input number of model validations        par_num_validations = arcpy.Parameter(                                displayName='Number of model validations',                                name='in_num_validations',                                datatype='GPLong',                                parameterType='Required',                                direction='Input',                                category='VegFrax Options',                                multiValue=False)        par_num_validations.filter.type = 'Range'        par_num_validations.filter.list = [1, 100]        par_num_validations.value = 10                # input oa fmask         par_fmask_flags = arcpy.Parameter(                            displayName='Include flags',                            name='in_fmask_flags',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Satellite Quality Options',                            multiValue=True)        flags = ['NoData', 'Valid', 'Cloud', 'Shadow', 'Snow', 'Water']        par_fmask_flags.filter.type = 'ValueList'              par_fmask_flags.filter.list = flags        par_fmask_flags.values = ['Valid', 'Snow', 'Water']                # input max cloud cover        par_max_cloud = arcpy.Parameter(                          displayName='Maximum cloud cover',                          name='in_max_cloud',                          datatype='GPDouble',                          parameterType='Optional',                          direction='Input',                          category='Satellite Quality Options',                          multiValue=False)        par_max_cloud.filter.type = 'Range'        par_max_cloud.filter.list = [0.0, 100.0]        par_max_cloud.value = 10.0                # input add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True                # combine parameters        parameters = [            par_low_res_path,            par_high_res_path,            par_out_nc_path,            par_from_date,            par_date_to,            par_aggregator,            par_classes,            par_merge_classes,            par_num_samples,            par_num_estimators,            par_num_validations,            par_fmask_flags,            par_max_cloud,            par_add_result_to_map            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # risk imports (non-native to arcgis)        try:            import numpy as np   # this is in arcgis, but ok            import xarray as xr  # not in arcgis            import rasterio      # not in arcgis        except:            arcpy.AddError('Python library Xarray is not installed.')            return                    # update ui datetimes input ds altered - only do on first input change        if parameters[0].altered and not parameters[0].hasBeenValidated:            try:                # get nc path, load nc                 nc_path = parameters[0].valueAsText                ds = xr.open_dataset(nc_path)                                # convert array to dataset if array detected                 if isinstance(ds, xr.DataArray):                    ds = ds.to_dataset(name='variable')                      # check if got time dim, if so, pluck dts                if 'time' in list(ds.dims):                    start_dt = ds['time'].isel(time=0).dt.strftime('%Y-%m-%d').values                    end_dt = ds['time'].isel(time=-1).dt.strftime('%Y-%m-%d').values                # update date controls                parameters[3].value = str(start_dt)                parameters[4].value = str(end_dt)            except:                arcpy.AddError('Could not open low resolution input NetCDF.')                return                        # modify ui classes selector list if high res altered        if parameters[1].altered and not parameters[1].hasBeenValidated:            try:                # open provided high res tif                 tif_path = parameters[1].valueAsText                ds = xr.open_rasterio(tif_path)                                                   # check bands, if single, create classes list (strings)                if len(ds) != 1:                    arcpy.AddError('High resolution raster can not be multiband.')                    return                                # check if nodata value embedded in xr                if not hasattr(ds, 'nodatavals'):                    arcpy.AddError('Dataset does not have nodata value attribute.')                    return                elif ds.nodatavals == 'unknown':                    arcpy.AddError('Dataset nodata value is unknown.')                    return                                    # get all unique classes in dataset and remove nodata                classes = np.unique(ds)                classes = classes[classes != ds.nodatavals]                                # check if we got something                if len(classes) <= 0:                    arcpy.AddError('No classes detected in dataset.')                    return                                    # finally, load all non-nodata classes into ui                text_classes = []                for c in classes:                    text_classes.append('Class: {}'.format(c))                                        # do best to sort alphabetically                text_classes.sort()                # update classes ui control with new classes and enable                parameters[6].enabled = True                parameters[6].filter.list = text_classes                parameters[6].values = text_classes                            except:                arcpy.AddError('Could not open high resolution input GeoTIFF.')                return                  # update ui merge selected classes when classes selector is enabled        if parameters[6].enabled:            parameters[7].enabled = True  # enable merge classes option        else:            parameters[7].enabled = False  # disable merge classes option                      return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the VegFrax Fractional Cover module.        """                # safe imports        import os, sys                           # arcgis comes with these        import datetime                          # arcgis comes with this        import numpy as np                       # arcgis comes with this        import pandas as pd                      # arcgis comes with this        import tempfile                          # arcgis comes with this        from io import StringIO                  # arcgis comes with this        from contextlib import redirect_stdout   # arcgis comes with this        # risk imports (non-native to arcgis)        try:            import xarray as xr                  # not in arcgis        except:            arcpy.AddError('Python library Xarray is not installed.')            return                # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools                          # module folder            sys.path.append(FOLDER_MODULES)            import vegfrax, cog                      except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return                    # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        # grab parameter values         in_low_res_nc = parameters[0].valueAsText         # raw input low res satellite netcdf        in_high_res_tif = parameters[1].valueAsText       # raw input high res satellite tif        out_nc = parameters[2].valueAsText                # output vegfrax netcdf        in_from_date = parameters[3].value                # start date of aggregate        in_to_date = parameters[4].value                  # end date of aggregate        in_aggregator = parameters[5].value               # aggregator        in_classes = parameters[6].valueAsText            # selected classes        in_merge_classes = parameters[7].value            # merge selected classes               in_num_samples = parameters[8].value              # number of samples        in_num_estimators = parameters[9].value           # number of model estimators        in_num_validations = parameters[10].value         # number of model validations        in_fmask_flags = parameters[11].valueAsText       # fmask flag values        in_max_cloud = parameters[12].value               # max cloud percentage        in_add_result_to_map = parameters[13].value       # add result to map        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning VegFrax Fractional Cover.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                             min_range=0, max_range=20)        # convert datetime strings to numpy datetime64        in_from_date = arc.datetime_to_numpy(in_from_date)        in_to_date = arc.datetime_to_numpy(in_to_date)               # convert arcgis multi-value format to list of values and notify               in_classes = arc.prepare_vegfrax_classes(in_classes)                # convert fmask flags as text to numeric code equivalents        in_fmask_flags = [e for e in in_fmask_flags.split(';')]        in_fmask_flags = arc.convert_fmask_codes(in_fmask_flags)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Loading and checking low resolution netcdf...')        arcpy.SetProgressorPosition(1)                # load low res netcdf. set nodata to nan to mimic dea odc        ds_low = satfetcher.load_local_nc(nc_path=in_low_res_nc,                                           use_dask=True,                                           conform_nodata_to=np.nan)                                              # check netcdf if it has bands, get attributes for ds and a band        if len(ds_low.data_vars) == 0:            arcpy.AddError('Input NetDF must be a xr dataset.')            return        elif len(ds_low.data_vars) == 0:            arcpy.AddError('Input NetDF has no data/variables/bands.')            return                                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Getting Dataset attributes...')        arcpy.SetProgressorPosition(2)                        # get attributes from dataset        ds_low_attrs = ds_low.attrs        ds_low_band_attrs = ds_low[list(ds_low.data_vars)[0]].attrs        ds_low_spatial_ref_attrs = ds_low['spatial_ref'].attrs                        # check if expected band name exists (landsat/sentinel differences)        ds_low_mask_band = arc.get_name_of_mask_band(list(ds_low.data_vars))        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Removing invalid pixels and dates from low res NetCDF...')        arcpy.SetProgressorPosition(3)                # group duplicate times if exist        ds_low = satfetcher.group_dupe_times(ds_low)                # remove invalid pixels and empty scenes        ds_low = cog.remove_fmask_dates(ds=ds_low,                                         valid_class=in_fmask_flags,                                         max_invalid=in_max_cloud,                                         mask_band=ds_low_mask_band,                                         nodata_value=np.nan,                                         drop_fmask=True)                                                     # conform and prepare low res dataset         ds_low = vegfrax.prepare_raw_xr(ds_low,                                         dtype='float32',                                         conform_nodata_to=-999)                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Calculating tasselled cap...')        arcpy.SetProgressorPosition(4)                # get platform name from attributes, error if no attributes        in_platform = arc.get_platform_from_dea_attrs(ds_low_attrs)                # conform dea aws band names based on platform        ds_low = satfetcher.conform_dea_ard_band_names(ds=ds_low,                                                        platform=in_platform.lower())         # calculate tasselled cap index         ds_low = tools.calculate_indices(ds=ds_low,                                          index=['tcg', 'tcb', 'tcw'],                                          custom_name=None,                                          rescale=False,                                          drop=True)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Subsetting data to requested time range...')        arcpy.SetProgressorPosition(5)                        # restrict date range        ds_low = ds_low.where((ds_low['time'] >= in_from_date) &                               (ds_low['time'] <= in_to_date), drop=True)                                      # check if any data exists after subset         if len(ds_low['time']) == 0:            arcpy.AddError('Time range used to subset data returned nothing.')            return                    # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Aggregating low res data and computing into memory...')        arcpy.SetProgressorPosition(5)                # aggregate and compute        if in_aggregator.lower() == 'mean':            ds_low = ds_low.mean('time', keep_attrs=True).compute()        elif in_aggregator.lower() == 'median':            ds_low = ds_low.median('time', keep_attrs=True).compute()                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to low resolution dataset...')        arcpy.SetProgressorPosition(6)                        # append attrbutes on to dataset and bands        ds_low.attrs = ds_low_attrs        ds_low['spatial_ref'].attrs = ds_low_spatial_ref_attrs        for var in list(ds_low.data_vars):            ds_low[var].attrs = ds_low_band_attrs                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Loading and checking high res GeoTIFF...')        arcpy.SetProgressorPosition(7)                # load raster as an xarray dataset and set nodata to -128        ds_high = satfetcher.load_local_rasters(rast_path_list=in_high_res_tif,                                                 use_dask=True,                                                 conform_nodata_to=-128)                # check dataset has one band, get attributes for ds (i.e., band)        if isinstance(ds_high, xr.DataArray):            ds_high = ds_high.to_dataset(dim='variable')        elif len(ds_high) != 1:            arcpy.AddError('More than one band in high resolution input GeoTIFF.')            return                # do basic preparations (dtype, rename, checks)        ds_high = vegfrax.prepare_classified_xr(ds=ds_high)                 # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Clipping high resolution data lowest extent...')        arcpy.SetProgressorPosition(8)                # subset high to low extent        ds_high = tools.clip_xr_to_xr(ds_a=ds_high, ds_b=ds_low)                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Reclassifying all non-selected classes to 0...')        arcpy.SetProgressorPosition(9)                # add 0 if missing. chekc this, could be problematic        if 0 not in in_classes:            in_classes.append(0)                                      # reclassify all other classes to 0 (and leave nodata as is)        ds_high = vegfrax.reclassify_xr(ds=ds_high,                                         req_class=in_classes,                                        merge_classes=in_merge_classes,                                        inplace=True)                # get new list of current classes        in_classes = vegfrax.get_xr_classes(ds_high)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Computing high resolution data into memory...')        arcpy.SetProgressorPosition(10)                # load into memory now - we have values to modify!        ds_high = ds_high.compute()                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Generating random samples in overlap areas...')        arcpy.SetProgressorPosition(11)                # generate random samples within area overlap between raw and classified rasters        df_samples = vegfrax.generate_strat_random_samples(ds_raw=ds_low,                                                           ds_class=ds_high,                                                            req_class=in_classes,                                                           num_samples=in_num_samples)                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Extracting values from low res data...')        arcpy.SetProgressorPosition(12)                # extract pixel values from raw, low resolution rasters at each point        df_extract = tools.extract_xr_values(ds=ds_low,                                              coords=df_samples,                                              keep_xy=True)        # remove any points containing a nodata value        df_extract_clean = tools.remove_nodata_records(df_extract,                                                        nodata_value=ds_low.nodatavals)                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Building focal windows...')        arcpy.SetProgressorPosition(13)                # generate focal windows and extract pixels from class raster        df_windows = vegfrax.create_frequency_windows(ds_raw=ds_low,                                                       ds_class=ds_high,                                                       df_records=df_extract_clean)                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Converting window values to frequencies...')        arcpy.SetProgressorPosition(14)                        # transform raw focal window pixel class counts into frequencies        df_freqs = vegfrax.convert_window_counts_to_freqs(df_windows=df_windows,                                                           nodata_value=ds_high.nodatavals)                               # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Selecting requested classes for analysis...')        arcpy.SetProgressorPosition(15)                # convert classes to text. all classes in request list will be used        text_classes = [str(c) for c in in_classes]        # prepare data for analysis - prepare classes, nulls, normalise frequencies        df_data = vegfrax.prepare_freqs_for_analysis(ds_raw=ds_low,                                                      ds_class=ds_high,                                                      df_freqs=df_freqs,                                                      override_classes=text_classes)                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Performing Fractional Cover Analysis...')        arcpy.SetProgressorPosition(16)                # perform fca and show results in arcgis        f = StringIO()        with redirect_stdout(f):            ds_vegfrax = vegfrax.perform_fca(ds_raw=ds_low,                                              ds_class=ds_high,                                              df_data=df_data,                                              df_extract_clean=df_extract_clean,                                              n_estimators=in_num_estimators,                                             n_validations=in_num_validations)            arcpy.AddMessage(f.getvalue())                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(17)                # append top-level attributes        ds_vegfrax.attrs = ds_low_attrs                # append spatial ref back on        if 'spatial_ref' not in list(ds_vegfrax.coords):            crs = tools.get_xr_crs(ds_low)            ds_vegfrax = ds_vegfrax.assign_coords({'spatial_ref': crs})        ds_vegfrax['spatial_ref'].attrs = ds_low_spatial_ref_attrs                # add band-level attributes back on        for var in list(ds_vegfrax.data_vars):            ds_vegfrax[var].attrs = ds_low_band_attrs        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(18)                   # export netcdf file        tools.export_xr_as_nc(ds=ds_vegfrax, filename=out_nc)                                  # # # # #        # add multi-dim raster to current map        if in_add_result_to_map:                        # notify and increment progess bar            arcpy.SetProgressorLabel('Adding fractional cover classes to current ArcGIS map...')            arcpy.SetProgressorPosition(19)                                    # create output folder with dt            dt = datetime.datetime.now().strftime("%d%m%Y%H%M%S")            out_folder = os.path.join(os.path.dirname(out_nc), 'vegfrax' + '_' + dt)            os.makedirs(out_folder)                        try:                # try to get current map, fail if does not exist                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap	                                # enable auto-add to map                #arcpy.env.addOutputToMap = False                                # setup a group layer via template                grp_lyr = arcpy.mp.LayerFile(GRP_LYR_FILE)                grp = m.addLayer(grp_lyr)[0]                grp.name = 'vegfrax'                # loop each var and export a seperate crf                for var in list(ds_vegfrax.data_vars):                                    # create temporary netcdfil for one var (prevents 2.9 bug)                    with tempfile.NamedTemporaryFile() as tmp:                        tmp_nc = '{}_{}.nc'.format(tmp.name, var)                        ds_vegfrax[var].to_dataset().to_netcdf(tmp_nc)                                # build in-memory crf for temp netcdf                    out_crf = os.path.join(out_folder, 'temp_{}.crf'.format(var))                    lyr = arcpy.md.MakeMultidimensionalRasterLayer(in_multidimensional_raster=tmp_nc,                                                                    out_multidimensional_raster_layer=out_crf)                    # export final tif                    out_tif = os.path.join(out_folder, '{}.tif'.format(var))                    arcpy.management.CopyRaster(in_raster=lyr, out_rasterdataset=out_tif)                    # add to current map                    m.addDataFromPath(out_tif)                                        # apply symbology to layer                    sym = arc.apply_cmap(aprx=aprx,                                          lyr_name='{}.tif'.format(var),                                         cmap_name='Spectrum By Wavelength-Full Bright',                                         cutoff_pct=0.0)                    # rename lyr, add to group, remove second instance                        m.addLayerToGroup(grp, sym)                    m.removeLayer(sym)            except:                arcpy.AddWarning('Could not visualise output.')        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(20)                # close and del dataset        ds_low.close()        ds_high.close()        ds_vegfrax.close()        del ds_low, ds_high, ds_vegfrax        # notify user        arcpy.AddMessage('Generated fractional covers successfully.')        return# TODO CHANGE BUILD XR ATTRS WITH manual_create_xr_attrs... also... do we really wanna do that before we check crs??? See nicher sdm!# SEE SDM FOR NICER CODE EXAMPLEclass Ensemble_Sigmoider(object):    def __init__(self):        """        Initialise tool.        """                self.label = "Ensemble Sigmoider"        self.description = "Rescale raw NetCDF or GeoTiffs to 0-1 using advanced sigmoids."        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """        # input netcdf or geotiff file        par_in_file = arcpy.Parameter(                        displayName='Input evidence layer (NetCDF or GeoTiff',                        name='in_file',                        datatype='DEFile',                        parameterType='Required',                        direction='Input')        par_in_file.filter.list = ['nc', 'tif']                # output netcdf file        par_out_file = arcpy.Parameter(                         displayName='Output rescaled layer (NetCDF)',                         name='out_file',                         datatype='DEFile',                         parameterType='Required',                         direction='Output')        par_out_file.filter.list = ['nc']                # input variables        par_in_var = arcpy.Parameter(                       displayName='Variable',                       name='in_var',                       datatype='GPString',                       parameterType='Required',                       direction='Input')        par_in_var.filter.type = 'ValueList'              par_in_var.filter.list = []                    # input type        par_in_type = arcpy.Parameter(                        displayName='Membership type',                        name='in_type',                        datatype='GPString',                        parameterType='Required',                        direction='Input')        par_in_type.filter.type = 'ValueList'              par_in_type.filter.list = ['Increasing', 'Decreasing', 'Symmetric']          par_in_type.value = 'Increasing'                # input minimum (low inflection)        par_in_min = arcpy.Parameter(                       displayName='Low inflection point',                       name='in_minimum',                       datatype='GPDouble',                       parameterType='Required',                       direction='Input')                # input maximum (high inflection)        par_in_max = arcpy.Parameter(                       displayName='High inflection point',                       name='in_maximum',                       datatype='GPDouble',                       parameterType='Required',                       direction='Input')        # input add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True        params = [            par_in_file,             par_out_file,            par_in_var,            par_in_type,            par_in_min,            par_in_max,            par_add_result_to_map            ]        return params    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # imports        try:            import xarray as xr            import rasterio        except:            arcpy.AddError('Could not import Xarray or rasterio.')            return                    # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import satfetcher        except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return                    # globals        global ENSEMBLE_SIGMOIDS        # unpack global parameter values         curr_file = ENSEMBLE_SIGMOIDS.get('in_file')        curr_var = ENSEMBLE_SIGMOIDS.get('in_var')                # if input file added, run        if parameters[0].value is not None:                        # if global has no matching file (or first run), reload all            if curr_file != parameters[0].valueAsText:                if parameters[0].valueAsText.endswith('.nc'):                    try:                        ds = xr.open_dataset(parameters[0].valueAsText)                        data_vars = [var for var in ds]                        ds.close()                    except:                        data_vars = []                                elif parameters[0].valueAsText.endswith('.tif'):                    try:                        da = xr.open_rasterio(parameters[0].valueAsText)                        data_vars = ['{}'.format(var) for var in da['band'].values]                        da.close()                    except:                        data_vars = []                # populate var list with new vars                parameters[2].filter.list = data_vars                                # set var and min, max, mid to no selections                parameters[2].value = None                parameters[4].value = None                parameters[5].value = None                           # calc min, max, mid if var changed            elif curr_var != parameters[2].valueAsText:                new_var = parameters[2].valueAsText                new_type = parameters[3].value                                if parameters[0].valueAsText.endswith('.nc'):                    try:                        ds = xr.open_dataset(parameters[0].valueAsText)                        da = ds[new_var]                                                if hasattr(ds, 'nodatavals'):                            da = da.where(da != ds.nodatavals)                                                mins = round(float(da.min()), 3)                        maxs = round(float(da.max()), 3)                                                ds.close()                    except:                        mins, maxs = None, None                                elif parameters[0].valueAsText.endswith('.tif'):                    try:                        ds = xr.open_rasterio(parameters[0].valueAsText)                        da = ds.sel(band=int(new_var))                                                if hasattr(ds, 'nodatavals') and len(ds.nodatavals) == 1:                            da = da.where(da != ds.nodatavals)                        mins = round(float(da.min()), 3)                        maxs = round(float(da.max()), 3)                            ds.close()                    except:                        mins, maxs = None, None                # set min, max                parameters[4].value = mins                parameters[5].value = maxs                    # update global values        ENSEMBLE_SIGMOIDS = {            'in_file': parameters[0].valueAsText,            'in_var': parameters[2].value,        }        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""               return    def execute(self, parameters, messages):        """        Executes the Ensemble Sigmoider module.        """                # safe imports        import os             # arcgis comes with these        import datetime       # arcgis comes with these        # risky imports (not native to arcgis)        try:            import numpy as np            import xarray as xr        except:            arcpy.AddError('Python libraries xarray and dask not installed.')            return        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools            # module folder            sys.path.append(FOLDER_MODULES)            import canopy        except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return        # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        # grab parameter values         in_file = parameters[0].valueAsText          # input netcdf or geotiff        out_nc = parameters[1].valueAsText           # output netcdf        in_var = parameters[2].value                 # input variable        in_type = parameters[3].value                # input membership type        in_min = parameters[4].value                 # input minimum        in_max = parameters[5].value                 # input maximum        in_add_result_to_map = parameters[6].value   # input add result to map        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning Ensemble Sigmoider.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=7)        # prepare and check file extension        in_ext = os.path.splitext(in_file)[1]        if in_ext not in ['.nc', '.tif']:            arcpy.AddError('File type not supported.')            return        # check type        if in_type not in ['Increasing', 'Decreasing', 'Symmetric']:            arcpy.AddError('Membership type not supported.')            return                    # check values         if in_min is None or in_max is None:            arcpy.AddError('Low and high inflection points must not be empty.')            return        elif in_max <= in_min:            arcpy.AddError('High inflection point can not be <= low inflection.')            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading and checking input data...')        arcpy.SetProgressorPosition(1)        # do quick load depending on input type        try:            if in_ext == '.nc':                ds = xr.open_dataset(in_file)            else:                ds = xr.open_rasterio(in_file)                ds = ds.to_dataset(dim='band')                ds = tools.build_xr_attributes(ds)           except:            arcpy.AddError('Could not quick load input file.')            return        # check xr type, vars, coords, dims, attrs        if not isinstance(ds, xr.Dataset):            arcpy.AddError('Input NetCDF must be a xr dataset.')            return        elif 'time' in ds:            arcpy.AddError('Input NetCDF time dimension must not exist.')            return        elif len(ds) == 0:            arcpy.AddError('Input NetCDF has no data/variables/bands.')            return        elif in_ext == '.tif' and len(ds) > 1:            arcpy.AddError('Input GeoTiff must have only one band.')            return        elif 'x' not in list(ds.coords) or 'y' not in list(ds.coords):            arcpy.AddError('Input NetCDF must have x, y coords.')            return        elif 'spatial_ref' not in list(ds.coords):            arcpy.AddError('Input NetCDF must have a spatial_ref coord.')            return        elif 'x' not in list(ds.dims) or 'y' not in list(ds.dims):            arcpy.AddError('Input NetCDF must have x, y dimensions.')            return        elif len(ds['x']) == 0 or len(ds['y']) == 0:            arcpy.AddError('Input NetCDF must have at least one x, y index.')            return        elif ds.attrs == {}:            arcpy.AddError('NetCDF attributes not found. NetCDF must have attributes.')            return        elif not hasattr(ds, 'crs'):            arcpy.AddError('NetCDF CRS attribute not found. CRS required.')            return        elif ds.crs != 'EPSG:3577':            arcpy.AddError('NetCDF CRS is not EPSG:3577. EPSG:3577 required.')                        return        elif not hasattr(ds, 'nodatavals'):            arcpy.AddError('NetCDF nodatavals attribute not found.')                        return        # check if xr is all nan/0 via centroid pixel timeseries (saves full load)        if ds.to_array().isnull().all():            arcpy.AddError('Input data is completely null.')                        return         try:            # now, do proper open of file, set nodata to nan            if in_ext == '.nc':                ds = satfetcher.load_local_nc(nc_path=in_file,                                               use_dask=False,                                               conform_nodata_to=np.nan)            else:                ds = satfetcher.load_local_rasters(rast_path_list=in_file,                                                    use_dask=False,                                                    conform_nodata_to=np.nan)                ds = tools.build_xr_attributes(ds)         except:            arcpy.AddError('Could not properly load input file.')            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Getting file attributes...')        arcpy.SetProgressorPosition(2)        # get attributes from dataset        ds_attrs = ds.attrs        ds_band_attrs = ds[list(ds.data_vars)[0]].attrs        ds_spatial_ref_attrs = ds['spatial_ref'].attrs        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Subsetting data to specified variable...')        arcpy.SetProgressorPosition(3)        # check if requested var in netcdf (tif always has one)        if in_ext == '.nc' and in_var not in ds:            arcpy.AddError('Requested variable not found in NetCDF.')            return        # subset dataset if netcdf (tif always have one), set both as array        ds = ds[in_var] if in_ext == '.nc' else ds.to_array()        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Applying sigmoidal function to data...')        arcpy.SetProgressorPosition(3)                # get nan mask        ds_mask = xr.where(~ds.isnull(), True, False)        try:            # apply sigmoidal depending on user selection            if in_type == 'Increasing':                ds = canopy.inc_sigmoid(ds, a=in_min, b=in_max)            elif in_type == 'Decreasing':                ds = canopy.dec_sigmoid(ds, c=in_min, d=in_max)            elif in_type == 'Symmetric':                ds = canopy.bell_sigmoid(ds, a=in_min, bc=in_max, d=in_min)        except:            arcpy.AddError('Could not perform signoidal.')            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Cleaning up output dataset...')        arcpy.SetProgressorPosition(4)        # apply nan mask to be safe        ds = ds.where(ds_mask)        # name single variable to sigmoid        ds = ds.to_dataset(name='sigmoid').squeeze(drop=True)        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(5)        # append attrbutes on to dataset and bands        ds.attrs = ds_attrs        ds['spatial_ref'].attrs = ds_spatial_ref_attrs        for var in list(ds.data_vars):            ds[var].attrs = ds_band_attrs                    # now that we have forced nan as nodata, update attribute         ds.attrs.update({'nodatavals': np.nan})        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(6)           # export netcdf file        tools.export_xr_as_nc(ds=ds, filename=out_nc)        # # # # #        # add to map if requested        if in_add_result_to_map:            # notify and increment progress bar            arcpy.SetProgressorLabel('Adding sigmoidal to map...')            arcpy.SetProgressorPosition(8)            try:                # open current map                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap                                # remove sigmoidal layer if exists                for layer in m.listLayers():                    if layer.name == 'sigmoid.crf':                        m.removeLayer(layer)                # create output folder using datetime as name                dt = datetime.datetime.now().strftime('%d%m%Y%H%M%S')                out_folder = os.path.join(os.path.dirname(out_nc), 'sigmoid' + '_' + dt)                os.makedirs(out_folder)                # disable visualise on map temporarily                arcpy.env.addOutputsToMap = False                # create crf filename and copy it                out_file = os.path.join(out_folder, 'sigmoid.crf')                crf = arcpy.CopyRaster_management(in_raster=out_nc,                                                   out_rasterdataset=out_file)                # add to map                                  m.addDataFromPath(crf)               except:                arcpy.AddWarning('Could not visualise output. Aborting visualisation.')                            try:                # get symbology, update it                layer = m.listLayers('sigmoid.crf')[0]                sym = layer.symbology                                # if layer has stretch coloriser, apply color                if hasattr(sym, 'colorizer'):                    if sym.colorizer.type == 'RasterStretchColorizer':                        # apply percent clip type                        sym.colorizer.stretchType = 'PercentClip'                        sym.colorizer.minPercent = 0.01                        sym.colorizer.maxPercent = 0.99                        # apply color map                        cmap = aprx.listColorRamps('Spectrum By Wavelength-Full Bright')[0]                        sym.colorizer.colorRamp = cmap                        # apply other basic options                        sym.colorizer.invertColorRamp = False                        sym.colorizer.gamma = 1.0                        # update symbology                        layer.symbology = sym            except:                arcpy.AddWarning('Could not visualise layer GeoTiffs.')        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(7)        # close main dataset        ds.close()        del ds                # close mask dataset        ds_mask.close()        del ds_mask        # notify user        arcpy.AddMessage('Generated Sigmoidal successfully.')        return# check CHANGE BUILD XR ATTRS WITH manual_create_xr_attrs... also... do we really wanna do that before we check crs??? See nicher sdm!# SEE SDM FOR NICER CODE EXAMPLEclass Ensemble_Model(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = 'Ensemble Model'        self.description = 'Combine two or more evidence layers into ' \                           'belief/disbelief/plausability/confidence outputs .' \                           'The benefit of this is areas of certainty and ' \                           'uncertainty can be derived, providing a better ' \                           'understanding of where the model can be trusted.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set various ArcGIS Pro UI controls. Data validation        is enforced via ArcGIS Pro API.        """                # input input sigmoidals        par_layers = arcpy.Parameter(                       displayName='Input fuzzy evidence layers',                       name='in_layers',                       datatype='GPValueTable',                       parameterType='Required',                       direction='Input',                       multiValue=False)        par_layers.columns = [            ['DEFile', 'NetCDF File'],             ['GPString', 'Evidence Type']            ]        par_layers.filters[0].list = ['nc']        par_layers.filters[1].type = 'ValueList'        par_layers.filters[1].list = ['Belief', 'Disbelief']                # output netcdf file        par_out_nc = arcpy.Parameter(                       displayName='Output ensemble model (NetCDF)',                       name='out_nc',                       datatype='DEFile',                       parameterType='Required',                       direction='Output')        par_out_nc.filter.list = ['nc']                # input resample        par_resample = arcpy.Parameter(                         displayName='Resample resolution',                         name='in_resample',                         datatype='GPString',                         parameterType='Required',                         direction='Input',                         #category='Ensemble Options',                         multiValue=False)        resample_to = ['Highest Resolution', 'Lowest Resolution']        par_resample.filter.type = 'ValueList'              par_resample.filter.list = resample_to        par_resample.value = 'Lowest Resolution'                # smoothing window        par_in_win_size = arcpy.Parameter(                          displayName='Smoothing window size',                          name='in_win_size',                          datatype='GPLong',                          parameterType='Optional',                          direction='Input',                          multiValue=False)        par_in_win_size.filter.type = 'Range'        par_in_win_size.filter.list = [3, 99]        par_in_win_size.value = None                # input add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True                params = [            par_layers,            par_out_nc,            par_resample,            par_in_win_size,            par_add_result_to_map]        return params    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the Ensemble Model module.        """        # safe imports        import os             # arcgis comes with these        import datetime       # arcgis comes with these        import tempfile       # arcgis comes with these        # risky imports (not native to arcgis)        try:            import numpy as np            import xarray as xr        except:            arcpy.AddError('Python libraries xarray and dask not installed.')            return        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools            # module folder            sys.path.append(FOLDER_MODULES)            import canopy, ensemble        except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return        # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        # grab parameter values         in_layers = parameters[0].value              # input layers (value array)        out_nc = parameters[1].valueAsText           # output netcdf        in_resample = parameters[2].value            # resample resolution        in_win_size = parameters[3].value            # smoothing window size        in_add_result_to_map = parameters[4].value   # add result to map        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning Ensemble Modelling.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=9)        # check layers isnt empty        if len(in_layers) == 0:            arcpy.AddError('No fuzzy layers provided.')            return        # get all layers types and check        layer_types = [layer[1] for layer in in_layers]        if 'Belief' not in layer_types:            arcpy.AddError('Layers must have at least one Belief layer.')            return        elif 'Disbelief' not in layer_types:            arcpy.AddError('Layers must have at least one Disbelief layer.')            return        elif len(np.unique(layer_types)) != 2:            arcpy.AddError('Layers must have only contain Belief and Disbelief types.')            return                    # check resample        if in_resample not in ['Lowest Resolution', 'Highest Resolution']:            arcpy.AddWarning('Resample type not supported, setting to default.')            in_resample = 'Lowest Resolution'                    # check smooth window size (we support none, for no smoothing)        if in_win_size is not None:            if not isinstance(in_win_size, int):                arcpy.AddWarning('Smoothing window size must be integer, setting to default.')                in_win_size = 3            elif in_win_size < 3:                arcpy.AddWarning('Smoothing window size must be 3 or above, setting to default.')                in_win_size = 3        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Checking and loading input datasets...')        arcpy.SetProgressorPosition(1)        # iterate layers for check        ds_list = []        for layer in in_layers:                    try:                ds = xr.open_dataset(layer[0].value)                            except:                arcpy.AddError('Could not load input NetCDF {}.'.format(layer[0].value))                return                            # check xr type, vars, coords, dims, attrs            if not isinstance(ds, xr.Dataset):                arcpy.AddError('Input NetCDF must be a xr dataset.')                return            elif 'sigmoid' not in ds:                arcpy.AddError('Input NetCDF does not contain a sigmoid variable.')                return            elif 'time' in ds and ds['time'] > 1:                arcpy.AddError('Input NetCDF must not have a time dimension.')                return            elif len(ds.data_vars) == 0:                arcpy.AddError('Input NetCDF has no data/variables/bands.')                return            elif 'x' not in list(ds.coords) or 'y' not in list(ds.coords):                arcpy.AddError('Input NetCDF must have x, y coords.')                return            elif 'spatial_ref' not in list(ds.coords):                arcpy.AddError('Input NetCDF must have a spatial_ref coord.')                return            elif 'x' not in list(ds.dims) or 'y' not in list(ds.dims):                arcpy.AddError('Input NetCDF must have x, y dimensions.')                return            elif len(ds['x']) == 0 or len(ds['y']) == 0:                arcpy.AddError('Input NetCDF must have at least one x, y index.')                return            elif ds.attrs == {}:                arcpy.AddError('NetCDF attributes not found. NetCDF must have attributes.')                return            elif not hasattr(ds, 'crs'):                arcpy.AddError('NetCDF CRS attribute not found. CRS required.')                return            elif ds.crs != 'EPSG:3577':                arcpy.AddError('NetCDF CRS is not EPSG:3577. EPSG:3577 required.')                            return            elif not hasattr(ds, 'nodatavals'):                arcpy.AddError('NetCDF nodatavals attribute not found.')                            return                            # check if xr is all nan            if ds.to_array().isnull().all():                arcpy.AddError('Input data is completely null.')                            return            try:                # do proper load with dask, set nodata to nan                ds = satfetcher.load_local_nc(nc_path=layer[0].value,                                               use_dask=True,                                               conform_nodata_to=np.nan)            except:                arcpy.AddError('Could not properly load input file.')                return                            # add belief type to ds, will be striped later            ds.attrs.update({'evi_type': layer[1]})                            # add to dataset list            ds_list.append(ds)        # check at least one belief and disbelief layer captured        if len(ds_list) == 0:            arcpy.AddError('Insufficient processed datasets.')                        return                                            # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Checking dataset extents intersect...')        arcpy.SetProgressorPosition(2)                    # check extents overlap        if not tools.all_xr_intersect(ds_list):            arcpy.AddError('Not all input GeoTiffs intersect.')                        return                                             # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Conforming datasets via resampling...')        arcpy.SetProgressorPosition(3)        # select target resolution dataset        ds_target = tools.get_target_res_xr(ds_list, in_resample)        # check if somethign returned        if ds_target is None:            arcpy.AddError('Could not obtain optimal resolution dataset.')                        return            try:            # resample all datasets to target dataset            for idx in range(len(ds_list)):                ds_list[idx] = tools.resample_xr(ds_from=ds_list[idx],                                                  ds_to=ds_target,                                                 resampling='nearest')                                                                 # squeeze                 ds_list[idx] = ds_list[idx].squeeze(drop=True)        except:            arcpy.AddError('Could not resmaple datasets.')            return         # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Smoothing datasets, if requested...')        arcpy.SetProgressorPosition(4)        # smooth each dataset, if requested        if in_win_size is not None:            try:                # smooth each dataset, eject if none (error)                for idx in range(len(ds_list)):                    ds_list[idx] = ensemble.smooth_xr_dataset(ds_list[idx], in_win_size)                                        if ds_list[idx] is None:                        arcpy.AddError('Could not smooth datasets.')                        return            except:                arcpy.AddError('Could not smooth datasets.')                return                                    # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading datasets into memory, please wait...')        arcpy.SetProgressorPosition(5)        # load each dataset        for ds in ds_list:             ds.load()        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Splitting datasets into belief and disbelief...')        arcpy.SetProgressorPosition(5)        # split datasets via evidence type attribute        beliefs = [ds for ds in ds_list if ds.evi_type == 'Belief']        disbeliefs = [ds for ds in ds_list if ds.evi_type == 'Disbelief']                      # check something was returned for each        if len(beliefs) == 0 or len(disbeliefs) == 0:            arcpy.AddError('Could not split datasets into belief and disbelief.')            return                    # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Performing ensemble modelling...')        arcpy.SetProgressorPosition(6)        try:            # perfom ensemble modelling            ds = ensemble.perform_modelling(belief=beliefs,                                            disbelief=disbeliefs)                        # create and add attributes to dataset            ds = tools.manual_create_xr_attrs(ds)                        # add nodatavals attribute, as we used nan             ds.attrs.update({'nodatavals': np.nan})                    except:            arcpy.AddError('Could not perform ensemble modelling.')            return        # check if dataset exists        if ds is None:            arcpy.AddError('No result was produced from ensemble modelling.')            return                                            # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(7)        # export netcdf file        tools.export_xr_as_nc(ds=ds, filename=out_nc)        # # # # #        # add multi-dim raster to current map        if in_add_result_to_map:            # notify and increment progess bar            arcpy.SetProgressorLabel('Adding outputs to current map...')            arcpy.SetProgressorPosition(8)            # create output folder with dt            dt = datetime.datetime.now().strftime("%d%m%Y%H%M%S")            out_folder = os.path.join(os.path.dirname(out_nc), 'ensemble' + '_' + dt)            os.makedirs(out_folder)            try:                # open current map                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap                                # remove existing ensemble layers if exist                for layer in m.listLayers():                    if layer.isGroupLayer and layer.name == 'ensemble':                        m.removeLayer(layer)                # setup a group layer via template                grp_lyr = arcpy.mp.LayerFile(GRP_LYR_FILE)                grp = m.addLayer(grp_lyr)[0]                grp.name = 'ensemble'                                # disable visual add to map                arcpy.env.addOutputsToMap = False                                        # loop each var and export a seperate crf                tif_list = []                for var in ds:                                                              # create temporary netcdfil for one var (prevents 2.9 bug)                    with tempfile.NamedTemporaryFile() as tmp:                        tmp_nc = '{}_{}.nc'.format(tmp.name, var)                        ds[[var]].to_netcdf(tmp_nc)                    # build in-memory crf for temp netcdf                    crf = arcpy.md.MakeMultidimensionalRasterLayer(in_multidimensional_raster=tmp_nc,                                                                    out_multidimensional_raster_layer=var)                                        # export temp tif                    tmp_tif = os.path.join(out_folder, '{}.tif'.format(var))                    arcpy.management.CopyRaster(in_raster=crf, out_rasterdataset=tmp_tif)                                        # add temp tif to map abd get as layer                    m.addDataFromPath(tmp_tif)                    layer = m.listLayers('{}.tif'.format(var))[0]                    # add layer to group and then remove outside layer                    m.addLayerToGroup(grp, layer, 'BOTTOM')                    m.removeLayer(layer)                                         # success, add store current layer for symbology below                    tif_list.append('{}.tif'.format(var))            except:                arcpy.AddWarning('Could not create layer GeoTiffs.')                            try:                       # iter tif layer names, get symbology, update it                for tif in tif_list:                    layer = m.listLayers(tif)[0]                    sym = layer.symbology                    # if layer has stretch coloriser, apply color                    if hasattr(sym, 'colorizer'):                        if sym.colorizer.type == 'RasterStretchColorizer':                            # apply percent clip type                            sym.colorizer.stretchType = 'PercentClip'                            sym.colorizer.minPercent = 0                            sym.colorizer.maxPercent = 1                            # apply color map                            cmap = aprx.listColorRamps('Spectrum By Wavelength-Full Bright')[0]                            sym.colorizer.colorRamp = cmap                            # apply other basic options                            sym.colorizer.invertColorRamp = False                            sym.colorizer.gamma = 1.0                            # update symbology                            layer.symbology = sym            except:                arcpy.AddWarning('Could not visualise layer GeoTiffs.')        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(9)                    # close ensemble dataset         ds.close()        del ds        # close all xr datasets in input lists        for ds in ds_list + beliefs + disbeliefs:            ds.close()        # notify user        arcpy.AddMessage('Performed Ensemble Modelling successfully.')        return# check CHANGE BUILD XR ATTRS WITH manual_create_xr_attrs... also... do we really wanna do that before we check crs??? See nicher sdm!# SEE SDM FOR NICER CODE EXAMPLEclass Ensemble_Masker(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = 'Ensemble Masker'        self.description = "Use another NetCDF or GeoTiff layer to mask " \                           "out areas from previously generated Ensemble modelling " \                           "outputs."        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """        # input netcdf or geotiff file        par_in_file = arcpy.Parameter(                        displayName='Input ensemble model (NetCDF)',                        name='in_file',                        datatype='DEFile',                        parameterType='Required',                        direction='Input')        par_in_file.filter.list = ['nc']                # output netcdf file        par_out_file = arcpy.Parameter(                         displayName='Output masked ensemble model (NetCDF)',                         name='out_file',                         datatype='DEFile',                         parameterType='Required',                         direction='Output')        par_out_file.filter.list = ['nc']                # input netcdf or geotiff file        par_in_mask_file = arcpy.Parameter(                             displayName='Input mask layer (NetCDF or GeoTiff)',                             name='in_mask',                             datatype='DEFile',                             parameterType='Required',                             direction='Input')        par_in_mask_file.filter.list = ['nc', 'tif']                # input variable        par_in_var = arcpy.Parameter(                       displayName='Mask variable',                       name='in_var',                       datatype='GPString',                       parameterType='Required',                       direction='Input')        par_in_var.filter.type = 'ValueList'              par_in_var.filter.list = []                    # input mask type        par_in_type = arcpy.Parameter(                        displayName='Mask type',                        name='in_type',                        datatype='GPString',                        parameterType='Required',                        direction='Input')        par_in_type.filter.type = 'ValueList'              par_in_type.filter.list = ['Binary', 'Range']          par_in_type.value = 'Binary'                # input binary mask value        par_in_bin = arcpy.Parameter(                             displayName='Mask out value',                             name='in_binary',                             datatype='GPDouble',                             parameterType='Optional',                             direction='Input')        # input range minimum        par_range_min = arcpy.Parameter(                                displayName='Minimum mask out value',                                name='in_range_min',                                datatype='GPDouble',                                parameterType='Optional',                                direction='Input')                                        # input range maximum        par_range_max = arcpy.Parameter(                                displayName='Maximum mask out value',                                name='in_range_max',                                datatype='GPDouble',                                parameterType='Optional',                                direction='Input')        # input add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True        params = [            par_in_file,             par_out_file,            par_in_mask_file,            par_in_var,            par_in_type,            par_in_bin,            par_range_min,            par_range_max,            par_add_result_to_map            ]        return params    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # imports        try:            import xarray as xr            import rasterio        except:            arcpy.AddError('Could not import Xarray or rasterio.')            return                    # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import satfetcher        except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return                    # globals        global ENSEMBLE_MASKER        # unpack global parameter values         curr_file = ENSEMBLE_MASKER.get('in_file')        curr_var = ENSEMBLE_MASKER.get('in_var')                # if input file added, run        if parameters[2].value is not None:                    # if global has no matching file (or first run), reload all            if curr_file != parameters[2].valueAsText:                if parameters[2].valueAsText.endswith('.nc'):                    try:                        ds = xr.open_dataset(parameters[2].valueAsText)                        data_vars = [var for var in ds]                        ds.close()                    except:                        data_vars = []                                elif parameters[2].valueAsText.endswith('.tif'):                    try:                        da = xr.open_rasterio(parameters[2].valueAsText)                        data_vars = ['{}'.format(var) for var in da['band'].values]                        da.close()                    except:                        data_vars = []                                          # populate var list with new vars                parameters[3].filter.list = data_vars                                # set var and bin, min, max to no selections                parameters[3].value = None                parameters[5].value = None                parameters[6].value = None                parameters[7].value = None              # calc min, max if var changed            elif curr_var != parameters[3].valueAsText:                new_var = parameters[3].valueAsText                new_type = parameters[4].value                if parameters[2].valueAsText.endswith('.nc'):                    try:                        ds = xr.open_dataset(parameters[2].valueAsText)                        da = ds[new_var]                                                if hasattr(ds, 'nodatavals'):                            da = da.where(da != ds.nodatavals)                                                mins = round(float(da.min()), 3)                        maxs = round(float(da.max()), 3)                                                ds.close()                    except:                        mins, maxs = None, None                                elif parameters[2].valueAsText.endswith('.tif'):                    try:                        ds = xr.open_rasterio(parameters[2].valueAsText)                        da = ds.sel(band=int(new_var))                                                if hasattr(ds, 'nodatavals') and len(ds.nodatavals) == 1:                            da = da.where(da != ds.nodatavals)                        mins = round(float(da.min()), 3)                        maxs = round(float(da.max()), 3)                            ds.close()                    except:                        mins, maxs = None, None                # set range min, max                parameters[6].value = mins                parameters[7].value = maxs                        # update global values        ENSEMBLE_MASKER = {            'in_file': parameters[2].valueAsText,            'in_var': parameters[3].value,        }        # enable binary or range parameters based on drop down        if parameters[4].value == 'Binary':            parameters[5].enabled = True             parameters[6].enabled = False             parameters[7].enabled = False         else:            parameters[5].enabled = False             parameters[6].enabled = True             parameters[7].enabled = True                 return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the Ensemble Masker module.        """        # safe imports        import os             # arcgis comes with these        import datetime       # arcgis comes with these        import tempfile       # arcgis comes with these        # risky imports (not native to arcgis)        try:            import numpy as np            import xarray as xr        except:            arcpy.AddError('Python libraries xarray and dask not installed.')            return        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools            # module folder            sys.path.append(FOLDER_MODULES)            import ensemble        except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return        # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        # grab parameter values         in_file = parameters[0].valueAsText          # input netcdf        out_nc = parameters[1].valueAsText           # output netcdf        in_mask_file = parameters[2].valueAsText     # input mask nc or tif        in_var = parameters[3].value                 # variable        in_type = parameters[4].value                # mask type        in_bin = parameters[5].value                 # binary value        in_range_min = parameters[6].value           # range minimum        in_range_max = parameters[7].value           # range maximum        in_add_result_to_map = parameters[8].value   # add result to map        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning Ensemble Masker.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=7)        # check variable        if in_var is None:            arcpy.AddError('No variable selected.')            return        # check type        if in_type not in ['Binary', 'Range']:            arcpy.AddError('Mask type not supported.')            return                    # check binary value        if in_type == 'Binary' and in_bin is None:            arcpy.AddError('Must provide a mask value when using binary type.')            return                    # check range values        if in_type == 'Range':            if in_range_min is None or in_range_max is None:                arcpy.AddError('Must provide a min and max value when using range type.')                return            elif in_range_max <= in_range_min:                arcpy.AddError('Range maximum can not be <= minimum.')                return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Checking input data...')        arcpy.SetProgressorPosition(1)        # check input ensemble and mask files        for file in [in_file, in_mask_file]:                        # get and check extension            in_ext = os.path.splitext(file)[1]            if in_ext not in ['.nc', '.tif']:                arcpy.AddError('File type not supported.')                return            try:                # do quick load depending on input type                if in_ext == '.nc':                    ds = xr.open_dataset(file)                else:                    ds = xr.open_rasterio(file)                    ds = ds.to_dataset(dim='band')                    ds = tools.build_xr_attributes(ds)               except:                arcpy.AddError('Could not quick load input file.')                return            # check xr type, vars, coords, dims, attrs            if not isinstance(ds, xr.Dataset):                arcpy.AddError('Input NetCDF must be a xr dataset.')                return            elif 'time' in ds:                arcpy.AddError('Input NetCDF must not have a time dimension.')                return            elif len(ds.data_vars) == 0:                arcpy.AddError('Input NetCDF has no data/variables/bands.')                return            elif in_ext == '.tif' and len(ds) > 1:                arcpy.AddError('Input GeoTiff must have only one band.')                return            elif 'x' not in list(ds.coords) or 'y' not in list(ds.coords):                arcpy.AddError('Input NetCDF must have x, y coords.')                return            elif 'spatial_ref' not in list(ds.coords):                arcpy.AddError('Input NetCDF must have a spatial_ref coord.')                return            elif 'x' not in list(ds.dims) or 'y' not in list(ds.dims):                arcpy.AddError('Input NetCDF must have x, y dimensions.')                return            elif len(ds['x']) == 0 or len(ds['y']) == 0:                arcpy.AddError('Input NetCDF must have at least one x, y index.')                return            elif ds.attrs == {}:                arcpy.AddError('NetCDF attributes not found. NetCDF must have attributes.')                return            elif not hasattr(ds, 'crs'):                arcpy.AddError('NetCDF CRS attribute not found. CRS required.')                return            elif ds.crs != 'EPSG:3577':                arcpy.AddError('NetCDF CRS is not EPSG:3577. EPSG:3577 required.')                            return            elif not hasattr(ds, 'nodatavals'):                arcpy.AddError('NetCDF nodatavals attribute not found.')                            return            # check if xr is all nan/0 via centroid pixel timeseries (saves full load)            if ds.to_array().isnull().all():                arcpy.AddError('Input data is completely null.')                            return                                         # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading input ensemble data...')        arcpy.SetProgressorPosition(2)        try:            # now, do proper load on ensemble netcdf, set nodata to nan            ds_ensemble = satfetcher.load_local_nc(nc_path=in_file,                                                    use_dask=False,                                                    conform_nodata_to=np.nan)        except:            arcpy.AddError('Could not properly load input ensemble NetCDF.')            return                    # check if required variables exist        for var in ds_ensemble:            if var not in ['belief', 'disbelief', 'plausability', 'interval']:                arcpy.AddError('Ensemble NetCDF is invalid does not have required variables.')                return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading input mask data...')        arcpy.SetProgressorPosition(3)        try:            # get mask file extension            in_mask_ext = os.path.splitext(in_mask_file)[1]                           # load mask file depending on file, set nodata to nan            if in_mask_ext == '.nc':                ds_mask = satfetcher.load_local_nc(nc_path=in_mask_file,                                                    use_dask=False,                                                    conform_nodata_to=np.nan)            else:                ds_mask = satfetcher.load_local_rasters(rast_path_list=in_mask_file,                                                         use_dask=False,                                                         conform_nodata_to=np.nan)                ds_mask = tools.build_xr_attributes(ds_mask)         except:            arcpy.AddError('Could not properly load input mask file.')            return                                            # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Getting ensemble file attributes...')        arcpy.SetProgressorPosition(4)        # get attributes from dataset        ds_attrs = ds_ensemble.attrs        ds_band_attrs = ds_ensemble[list(ds_ensemble.data_vars)[0]].attrs        ds_spatial_ref_attrs = ds_ensemble['spatial_ref'].attrs                    # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Conforming mask resolution via resampling...')        arcpy.SetProgressorPosition(6)                # check extents overlap        if not tools.all_xr_intersect([ds_ensemble, ds_mask]):            arcpy.AddError('Not all input layers intersect.')                        return         try:            # resample mask to ensemble dataset, if same, no change            ds_mask = tools.resample_xr(ds_from=ds_mask,                                         ds_to=ds_ensemble,                                        resampling='nearest')        except:            arcpy.AddError('Could not resamaple datasets.')            return         # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading datasets into memory, please wait...')        arcpy.SetProgressorPosition(7)        # load each dataset        ds_ensemble.load()        ds_mask.load()        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Preparing mask values...')        arcpy.SetProgressorPosition(8)               try:            # prepare mask depending on user choice            if in_type == 'Binary':                ds_mask = xr.where(ds_mask != in_bin, True, False)            else:                ds_mask = xr.where((ds_mask < in_range_min) |                                    (ds_mask > in_range_max), True, False)        except:            arcpy.AddError('Could not properly prepare mask dataset.')            return                                            # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Subsetting mask dataset to specified variable...')        arcpy.SetProgressorPosition(9)        # get mask file extension        in_mask_ext = os.path.splitext(in_mask_file)[1]                  # check if requested var in netcdf (tif always has one)        if in_mask_ext == '.nc' and in_var not in ds_mask:            arcpy.AddError('Requested variable not found in NetCDF.')            return        # subset dataset if netcdf (tif always have one), set both as array        ds_mask = ds_mask[in_var] if in_mask_ext == '.nc' else ds_mask.to_array()                                            # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Masking ensemble dataset via mask...')        arcpy.SetProgressorPosition(10)        # mask out any values under mask to nan and squeeze        ds_ensemble = ds_ensemble.where(ds_mask).squeeze(drop=True)        # check if any values exist in ensemble dataset        if ds_ensemble.to_array().isnull().all() == True:            arcpy.AddError('Ensemble dataset has no values after mask, check mask.')            return                                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(11)        # append attrbutes on to dataset and bands        ds_ensemble.attrs = ds_attrs        ds_ensemble['spatial_ref'].attrs = ds_spatial_ref_attrs        for var in ds_ensemble:            ds_ensemble[var].attrs = ds_band_attrs                    # ensure nodata is nan, update attribute         ds_ensemble.attrs.update({'nodatavals': np.nan})                                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(12)           # export netcdf file        tools.export_xr_as_nc(ds=ds_ensemble, filename=out_nc)        # # # # #        # add multi-dim raster to current map        if in_add_result_to_map:            # notify and increment progess bar            arcpy.SetProgressorLabel('Adding outputs to current map...')            arcpy.SetProgressorPosition(13)            # create output folder with dt            dt = datetime.datetime.now().strftime("%d%m%Y%H%M%S")            out_folder = os.path.join(os.path.dirname(out_nc), 'ensemble_mask' + '_' + dt)            os.makedirs(out_folder)            try:                # open current map                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap                # remove existing ensemble layers if exist                for layer in m.listLayers():                    if layer.isGroupLayer and layer.name == 'ensemble_mask':                        m.removeLayer(layer)                # setup a group layer via template                grp_lyr = arcpy.mp.LayerFile(GRP_LYR_FILE)                grp = m.addLayer(grp_lyr)[0]                grp.name = 'ensemble_mask'                # disable visual add to map                arcpy.env.addOutputsToMap = False                # loop each var and export a seperate crf                tif_list = []                for var in ds_ensemble:                          # create temporary netcdfil for one var (prevents 2.9 bug)                    with tempfile.NamedTemporaryFile() as tmp:                        tmp_nc = '{}_{}.nc'.format(tmp.name, var)                        ds_ensemble[[var]].to_netcdf(tmp_nc)                    # build in-memory crf for temp netcdf                    crf = arcpy.md.MakeMultidimensionalRasterLayer(in_multidimensional_raster=tmp_nc,                                                                    out_multidimensional_raster_layer=var)                    # export temp tif                    tmp_tif = os.path.join(out_folder, '{}_mask.tif'.format(var))                    arcpy.management.CopyRaster(in_raster=crf, out_rasterdataset=tmp_tif)                    # add temp tif to map abd get as layer                    m.addDataFromPath(tmp_tif)                    layer = m.listLayers('{}_mask.tif'.format(var))[0]                    # add layer to group and then remove outside layer                    m.addLayerToGroup(grp, layer, 'BOTTOM')                    m.removeLayer(layer)                     # success, add store current layer for symbology below                    tif_list.append('{}_mask.tif'.format(var))            except:                arcpy.AddWarning('Could not create layer GeoTiffs.')            try:                       # iter tif layer names, get symbology, update it                for tif in tif_list:                    layer = m.listLayers(tif)[0]                    sym = layer.symbology                    # if layer has stretch coloriser, apply color                    if hasattr(sym, 'colorizer'):                        if sym.colorizer.type == 'RasterStretchColorizer':                            # apply percent clip type                            sym.colorizer.stretchType = 'PercentClip'                            sym.colorizer.minPercent = 0                            sym.colorizer.maxPercent = 1                            # apply color map                            cmap = aprx.listColorRamps('Spectrum By Wavelength-Full Bright')[0]                            sym.colorizer.colorRamp = cmap                            # apply other basic options                            sym.colorizer.invertColorRamp = False                            sym.colorizer.gamma = 1.0                            # update symbology                            layer.symbology = sym            except:                arcpy.AddWarning('Could not visualise layer GeoTiffs.')                                        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(9)        # close ensemble dataset         ds_ensemble.close()        del ds_ensemble        # close mask dataset        ds_mask.close()        del ds_mask        # notify user        arcpy.AddMessage('Performed Ensemble Masking successfully.')        returnclass NRT_Create_Project(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = 'NRT Create Project'        self.description = 'Create a new project geodatabase to hold monitoring ' \                           'areas.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """                # output gdb folder        par_out_folder = arcpy.Parameter(                           displayName='Output Project Folder',                           name='out_folder',                           datatype='DEFolder',                           parameterType='Required',                           direction='Input')                                     # combine parameters        parameters = [par_out_folder]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the NRT Create Project module.        """                # safe imports        import os            # arcgis comes with these        import arcpy         # arcgis comes with these                        # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc                    # module folder            sys.path.append(FOLDER_MODULES)            import nrt        except Exception as e:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            arcpy.AddMessage(str(e))            return        # grab parameter values         out_folder = parameters[0].valueAsText      # output gdb folder path        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning NRT Create Project.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=8)                                                                                            # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Checking input parameters...')        arcpy.SetProgressorPosition(1)                                    # check inputs are not none and are strings        if out_folder is None:            arcpy.AddError('No project folder provided.')            return        elif not isinstance(out_folder, str):            arcpy.AddError('Project Folder is not a string.')            return        # check if monitoring area gdb already exists         gdb_path = os.path.join(out_folder, 'monitoring_areas.gdb')        if os.path.exists(gdb_path):            arcpy.AddError('Project folder already exists, provide a different folder.')            return                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Creating new project geodatabase...')        arcpy.SetProgressorPosition(2)        try:            # build project geodatbase            out_filepath = arcpy.management.CreateFileGDB(out_folder_path=out_folder,                                                           out_name='monitoring_areas.gdb')        except Exception as e:            arcpy.AddError('Could not create file geodatabase.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Generating database feature class...')        arcpy.SetProgressorPosition(3)        # temporarily disable auto-visual of outputs        arcpy.env.addOutputsToMap = False        try:            # create feature class and aus albers spatial ref sys            srs = arcpy.SpatialReference(3577)            out_feat = arcpy.management.CreateFeatureclass(out_path=out_filepath,                                                            out_name='monitoring_areas',                                                            geometry_type='POLYGON',                                                           spatial_reference=srs)        except Exception as e:            arcpy.AddError('Could not create featureclass.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Generating database domains...')        arcpy.SetProgressorPosition(4)        try:            # create platform domain            arcpy.management.CreateDomain(in_workspace=out_filepath,                                           domain_name='dom_platforms',                                           domain_description='Platform name (Landsat or Sentinel)',                                          field_type='TEXT',                                           domain_type='CODED')                                                      # generate coded values to platform domain            dom_values = {'Landsat': 'Landsat', 'Sentinel': 'Sentinel'}            for dom_value in dom_values:                arcpy.management.AddCodedValueToDomain(in_workspace=out_filepath,                                                        domain_name='dom_platforms',                                                        code=dom_value,                                                        code_description=dom_values.get(dom_value))            # create year domain            arcpy.management.CreateDomain(in_workspace=out_filepath,                                           domain_name='dom_years',                                           domain_description='Training years (1980 - 2050)',                                          field_type='LONG',                                           domain_type='RANGE')            # generate range values to year domain            arcpy.management.SetValueForRangeDomain(in_workspace=out_filepath,                                                     domain_name='dom_years',                                                     min_value=1980,                                                     max_value=2050)                                                                # create index domain            arcpy.management.CreateDomain(in_workspace=out_filepath,                                           domain_name='dom_indices',                                           domain_description='Vegetation index name',                                          field_type='TEXT',                                           domain_type='CODED')            # generate coded values to index domain            dom_values = {'NDVI': 'NDVI', 'MAVI': 'MAVI', 'kNDVI': 'kNDVI'}            for dom_value in dom_values:                arcpy.management.AddCodedValueToDomain(in_workspace=out_filepath,                                                        domain_name='dom_indices',                                                        code=dom_value,                                                        code_description=dom_values.get(dom_value))            # create persistence domain            arcpy.management.CreateDomain(in_workspace=out_filepath,                                           domain_name='dom_persistence',                                           domain_description='Vegetation persistence (0.001 - 9.999)',                                          field_type='FLOAT',                                           domain_type='RANGE')            # generate range values to persistence domain            arcpy.management.SetValueForRangeDomain(in_workspace=out_filepath,                                                     domain_name='dom_persistence',                                                     min_value=0.001,                                                     max_value=9.999)                                                                # create rule 1 min consequtives domain            arcpy.management.CreateDomain(in_workspace=out_filepath,                                           domain_name='dom_rule_1_consequtives',                                           domain_description='Rule 1 Consequtives (0 - 999)',                                          field_type='LONG',                                           domain_type='RANGE')            # generate range values to consequtives domain            arcpy.management.SetValueForRangeDomain(in_workspace=out_filepath,                                                     domain_name='dom_rule_1_consequtives',                                                     min_value=0,                                                     max_value=999)                                                                # create rule 2 min zone domain            arcpy.management.CreateDomain(in_workspace=out_filepath,                                           domain_name='dom_rule_2_min_zone',                                           domain_description='Rule 2 Minimum Zone (1 - 11)',                                          field_type='LONG',                                           domain_type='RANGE')            # generate range values for min zone domain            arcpy.management.SetValueForRangeDomain(in_workspace=out_filepath,                                                     domain_name='dom_rule_2_min_zone',                                                     min_value=1,                                                     max_value=11)            # create rule 3 num zones domain            arcpy.management.CreateDomain(in_workspace=out_filepath,                                           domain_name='dom_rule_3_num_zones',                                           domain_description='Rule 3 Number of Zones (1 - 11)',                                          field_type='LONG',                                           domain_type='RANGE')            # generate range values to num zones domain            arcpy.management.SetValueForRangeDomain(in_workspace=out_filepath,                                                     domain_name='dom_rule_3_num_zones',                                                     min_value=1,                                                     max_value=11)            # create ruleset domain            arcpy.management.CreateDomain(in_workspace=out_filepath,                                           domain_name='dom_ruleset',                                           domain_description='Various rulesets',                                          field_type='TEXT',                                           domain_type='CODED')            # generate coded values to ruleset domain               dom_values = {                '1':     '1 only',                '2':     '2 only',                '3':     '3 only',                '1&2':   '1 and 2',                '1&3':   '1 and 3',                '2&3':   '2 and 3',                '1|2':   '1 or 2',                '1|3':   '1 or 3',                '2|3':   '2 or 3',                '1&2&3': '1 and 2 and 3',                '1|2&3': '1 or 2 and 3',                '1&2|3': '1 and 2 or 3',                '1|2|3': '1 or 2 or 3'                }                  for dom_value in dom_values:                arcpy.management.AddCodedValueToDomain(in_workspace=out_filepath,                                                        domain_name='dom_ruleset',                                                        code=dom_value,                                                        code_description=dom_values.get(dom_value))                                                        # create alert method domain            arcpy.management.CreateDomain(in_workspace=out_filepath,                                           domain_name='dom_alert_method',                                           domain_description='Alert method',                                          field_type='TEXT',                                           domain_type='CODED')            # generate coded values to alert method domain             dom_values = {'static': 'Static', 'dynamic': 'Dynamic'}            for dom_value in dom_values:                arcpy.management.AddCodedValueToDomain(in_workspace=out_filepath,                                                        domain_name='dom_alert_method',                                                        code=dom_value,                                                        code_description=dom_values.get(dom_value))                    # create alert direction domain            arcpy.management.CreateDomain(in_workspace=out_filepath,                                           domain_name='dom_alert_direction',                                           domain_description='Alert directions',                                          field_type='TEXT',                                           domain_type='CODED')            # generate coded values to boolean domain            dom_values = {                'inc_any':      'Incline only (any)',                 'dec_any':      'Decline only (any)',                 'inc_pos':      'Incline only (+ zones only)',                 'dec_neg':      'Decline only (- zones only)',                 'both_any':     'Incline or Decline (any)',                'both_pos_neg': 'Incline or Decline (+/- zones only)',                }            for dom_value in dom_values:                arcpy.management.AddCodedValueToDomain(in_workspace=out_filepath,                                                        domain_name='dom_alert_direction',                                                        code=dom_value,                                                        code_description=dom_values.get(dom_value))            # create boolean domain            arcpy.management.CreateDomain(in_workspace=out_filepath,                                           domain_name='dom_boolean',                                           domain_description='Boolean (Yes or No)',                                          field_type='TEXT',                                           domain_type='CODED')            # generate coded values to boolean domain            dom_values = {'Yes': 'Yes', 'No': 'No'}            for dom_value in dom_values:                arcpy.management.AddCodedValueToDomain(in_workspace=out_filepath,                                                        domain_name='dom_boolean',                                                        code=dom_value,                                                        code_description=dom_values.get(dom_value))        except Exception as e:            arcpy.AddError('Could not create domains.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Generating database fields...')        arcpy.SetProgressorPosition(5)        try:            # add area id field to featureclass               arcpy.management.AddField(in_table=out_feat,                                       field_name='area_id',                                       field_type='TEXT',                                       field_alias='Area ID',                                      field_length=200,                                      field_is_required='REQUIRED')            # add platforms field to featureclass               arcpy.management.AddField(in_table=out_feat,                                       field_name='platform',                                       field_type='TEXT',                                       field_alias='Platform',                                      field_length=20,                                      field_is_required='REQUIRED',                                      field_domain='dom_platforms')                # add s_year field to featureclass               arcpy.management.AddField(in_table=out_feat,                                       field_name='s_year',                                       field_type='LONG',                                       field_alias='Start Year of Training Period',                                      field_is_required='REQUIRED',                                      field_domain='dom_years')            # add e_year field to featureclass               arcpy.management.AddField(in_table=out_feat,                                       field_name='e_year',                                       field_type='LONG',                                       field_alias='End Year of Training Period',                                      field_is_required='REQUIRED',                                      field_domain='dom_years')            # add index field to featureclass               arcpy.management.AddField(in_table=out_feat,                                       field_name='index',                                       field_type='TEXT',                                       field_alias='Vegetation Index',                                      field_length=20,                                      field_is_required='REQUIRED',                                      field_domain='dom_indices')            # add persistence field to featureclass               arcpy.management.AddField(in_table=out_feat,                                       field_name='persistence',                                       field_type='FLOAT',                                       field_alias='Vegetation Persistence',                                      field_is_required='REQUIRED',                                      field_domain='dom_persistence')            # add rule 1 min consequtives field to featureclass               arcpy.management.AddField(in_table=out_feat,                                       field_name='rule_1_min_conseqs',                                       field_type='LONG',                                       field_alias='Rule 1 Minimum Consequtives',                                      field_is_required='REQUIRED',                                      field_domain='dom_rule_1_consequtives')            # add include plateaus field to featureclass               arcpy.management.AddField(in_table=out_feat,                                       field_name='rule_1_inc_plateaus',                                       field_type='TEXT',                                       field_alias='Rule 1 Include Pleateaus',                                      field_length=20,                                      field_is_required='REQUIRED',                                      field_domain='dom_boolean')            # add rule 2 min stdv field to featureclass               arcpy.management.AddField(in_table=out_feat,                                       field_name='rule_2_min_zone',                                       field_type='LONG',                                       field_alias='Rule 2 Minimum Zone',                                      field_is_required='REQUIRED',                                      field_domain='dom_rule_2_min_zone')            # add rule 3 num zones field to featureclass               arcpy.management.AddField(in_table=out_feat,                                       field_name='rule_3_num_zones',                                       field_type='LONG',                                       field_alias='Rule 3 Number of Zones',                                      field_is_required='REQUIRED',                                      field_domain='dom_rule_3_num_zones')                                          # add ruleset field to featureclass               arcpy.management.AddField(in_table=out_feat,                                       field_name='ruleset',                                       field_type='TEXT',                                       field_alias='Ruleset',                                      field_length=20,                                      field_is_required='REQUIRED',                                      field_domain='dom_ruleset')                  # add alert field to featureclass               arcpy.management.AddField(in_table=out_feat,                                       field_name='alert',                                       field_type='TEXT',                                       field_alias='Alert via Email',                                      field_is_required='REQUIRED',                                      field_domain='dom_boolean')               # add method field to featureclass               arcpy.management.AddField(in_table=out_feat,                                       field_name='method',                                       field_type='TEXT',                                       field_alias='Alert via Method',                                      field_length=20,                                      field_is_required='REQUIRED',                                      field_domain='dom_alert_method')               # add alert direction field to featureclass               arcpy.management.AddField(in_table=out_feat,                                       field_name='alert_direction',                                       field_type='TEXT',                                       field_alias='Change Direction for Alert',                                      field_is_required='REQUIRED',                                      field_domain='dom_alert_direction')            # add email field to featureclass               arcpy.management.AddField(in_table=out_feat,                                       field_name='email',                                       field_type='TEXT',                                       field_alias='Email of User',                                      field_is_required='REQUIRED')            # add ignore field to featureclass               arcpy.management.AddField(in_table=out_feat,                                       field_name='ignore',                                       field_type='TEXT',                                       field_alias='Ignore When Run',                                      field_is_required='REQUIRED',                                      field_domain='dom_boolean')                                                 # add zone_color field to featureclass               arcpy.management.AddField(in_table=out_feat,                                       field_name='color',                                       field_type='LONG',                                       field_alias='Color Code',                                      field_is_required='REQUIRED')              # add global_id field to featureclass               arcpy.management.AddField(in_table=out_feat,                                       field_name='global_id',                                       field_type='TEXT',                                       field_alias='Global ID',                                      field_length=200,                                      field_is_required='REQUIRED')        except Exception as e:            arcpy.AddError('Could not create fields.')            arcpy.AddMessage(str(e))            return                        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Generating database defaults...')        arcpy.SetProgressorPosition(6)        try:            # set default platform            arcpy.management.AssignDefaultToField(in_table=out_feat,                                                   field_name='platform',                                                  default_value='Landsat')               # set default index            arcpy.management.AssignDefaultToField(in_table=out_feat,                                                   field_name='index',                                                  default_value='MAVI')              # set default persistence            arcpy.management.AssignDefaultToField(in_table=out_feat,                                                   field_name='persistence',                                                  default_value=0.5)            # set default min conseqs            arcpy.management.AssignDefaultToField(in_table=out_feat,                                                   field_name='rule_1_min_conseqs',                                                  default_value=3)            # set default inc plateaus            arcpy.management.AssignDefaultToField(in_table=out_feat,                                                   field_name='rule_1_inc_plateaus',                                                  default_value='No')            # set default min zone            arcpy.management.AssignDefaultToField(in_table=out_feat,                                                   field_name='rule_2_min_zone',                                                  default_value=2)            # set default num zones            arcpy.management.AssignDefaultToField(in_table=out_feat,                                                   field_name='rule_3_num_zones',                                                  default_value=2)            # set default ruleset            arcpy.management.AssignDefaultToField(in_table=out_feat,                                                   field_name='ruleset',                                                  default_value='1&2|3')            # set default alert            arcpy.management.AssignDefaultToField(in_table=out_feat,                                                   field_name='alert',                                                  default_value='No')                       # set default alert method            arcpy.management.AssignDefaultToField(in_table=out_feat,                                                   field_name='method',                                                  default_value='static')                      # set default alert direction            arcpy.management.AssignDefaultToField(in_table=out_feat,                                                   field_name='alert_direction',                                                  default_value='dec_any')               # set default ignore            arcpy.management.AssignDefaultToField(in_table=out_feat,                                                   field_name='ignore',                                                  default_value='No')                                                            # set default color            arcpy.management.AssignDefaultToField(in_table=out_feat,                                                   field_name='color',                                                  default_value=0)         except Exception as e:            arcpy.AddError('Could not assign default values.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Adding output to map...')        arcpy.SetProgressorPosition(7)        try:                       # for current project, open current map            aprx = arcpy.mp.ArcGISProject('CURRENT')            m = aprx.activeMap            m.addDataFromPath(out_feat)                except Exception as e:            arcpy.AddWarning('Could not visualise output, aborting visualisation.')            arcpy.AddMessage(str(e))            pass                    try:            # update all monitoring area features symbology            for layer in m.listLayers('monitoring_areas'):                arc.apply_monitoring_area_symbology(layer)                except Exception as e:            arcpy.AddWarning('Could not colorise output, aborting colorisation.')            arcpy.AddMessage(str(e))            pass                                    # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(8)        # finish up        arcpy.AddMessage('Created new NRT Project.')        returnclass NRT_Create_Monitoring_Areas(object):    def __init__(self):        """        Initialise tool.        """                # set tool name, description, options        self.label = 'NRT Create Monitoring Areas'        self.description = 'Create new monitoring area boundaries and set ' \                           'monitoring options.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """                # set input feature class        par_in_feat = arcpy.Parameter(                        displayName='Input monitoring area feature',                        name='in_feat',                        datatype='GPFeatureLayer',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_in_feat.filter.list = ['Polygon']                # set input feature class        par_in_new_feat = arcpy.Parameter(                            displayName='Draw new monitoring area',                            name='in_new_feat',                            datatype='GPFeatureRecordSetLayer',                            parameterType='Required',                            direction='Input',                            multiValue=False)        par_in_new_feat.filter.list = ['Polygon']        par_in_new_feat.enabled = False                # set monitoring area        par_in_set_area = arcpy.Parameter(                            displayName='Unique identifier',                            name='in_area_id',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            multiValue=False)        par_in_set_area.enabled = False                # platform        par_in_platform = arcpy.Parameter(                            displayName='Satellite platform',                            name='in_platform',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            multiValue=False)        par_in_platform.filter.type = 'ValueList'        par_in_platform.filter.list = ['Landsat', 'Sentinel']        par_in_platform.value = 'Landsat'        par_in_platform.enabled = False                # start year        par_in_s_year = arcpy.Parameter(                          displayName='Pre-impact start year',                          name='in_s_year',                          datatype='GPLong',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_in_s_year.filter.type = 'Range'        par_in_s_year.filter.list = [1980, 2050]        par_in_s_year.enabled = False                # end year        par_in_e_year = arcpy.Parameter(                          displayName='Pre-impact end year',                          name='in_e_year',                          datatype='GPLong',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_in_e_year.filter.type = 'Range'        par_in_e_year.filter.list = [1980, 2050]        par_in_e_year.enabled = False        # vegetation index        par_in_veg_idx = arcpy.Parameter(                           displayName='Vegetation index',                           name='in_veg_idx',                           datatype='GPString',                           parameterType='Required',                           direction='Input',                           multiValue=False)        par_in_veg_idx.filter.type = 'ValueList'        par_in_veg_idx.filter.list = ['NDVI', 'MAVI', 'kNDVI']        par_in_veg_idx.value = 'MAVI'        par_in_veg_idx.enabled = False                # persistence        par_in_persistence = arcpy.Parameter(                               displayName='Vegetation persistence',                               name='in_persistence',                               datatype='GPDouble',                               parameterType='Required',                               direction='Input',                               multiValue=False)        par_in_persistence.filter.type = 'Range'        par_in_persistence.filter.list = [0.001, 9.999]        par_in_persistence.value = 0.5        par_in_persistence.enabled = False               # rule 1 min conseqs        par_in_min_conseqs = arcpy.Parameter(                               displayName='Rule 1: Minimum consequtives',                               name='in_min_conseqs',                               datatype='GPLong',                               parameterType='Required',                               direction='Input',                               category='Rules',                               multiValue=False)        par_in_min_conseqs.filter.type = 'Range'        par_in_min_conseqs.filter.list = [0, 99]        par_in_min_conseqs.value = 3        par_in_min_conseqs.enabled = False                # rule 1 include plateaus        par_in_inc_plateaus = arcpy.Parameter(                                displayName='Rule 1: Include plateaus',                                name='in_inc_plateaus',                                datatype='GPString',                                parameterType='Required',                                direction='Input',                                category='Rules',                                multiValue=False)        par_in_inc_plateaus.filter.type = 'ValueList'        par_in_inc_plateaus.filter.list = ['Yes', 'No']        par_in_inc_plateaus.value = 'No'        par_in_inc_plateaus.enabled = False                # rule 2 minimum zone        par_in_min_stdv = arcpy.Parameter(                            displayName='Rule 2: Minimum Zone',                            name='in_min_zone',                            datatype='GPLong',                            parameterType='Required',                            category='Rules',                            direction='Input',                            multiValue=False)        par_in_min_stdv.filter.type = 'Range'        par_in_min_stdv.filter.list = [0, 99]        par_in_min_stdv.value = 2        par_in_min_stdv.enabled = False                  # rule 3 number of zones        par_in_num_zones = arcpy.Parameter(                             displayName='Rule 3: Number of zones',                             name='in_num_zones',                             datatype='GPLong',                             parameterType='Required',                             category='Rules',                             direction='Input',                             multiValue=False)        par_in_num_zones.filter.type = 'Range'        par_in_num_zones.filter.list = [0, 99]        par_in_num_zones.value = 2        par_in_num_zones.enabled = False                       # ruleset        par_in_ruleset = arcpy.Parameter(                           displayName='Ruleset',                           name='in_ruleset',                           datatype='GPString',                           parameterType='Required',                           direction='Input',                           category='Rules',                           multiValue=False)        par_in_ruleset.filter.type = 'ValueList'        ruleset = [            '1 Only',            '2 Only',            '3 Only',            '1 and 2',            '1 and 3',            '2 and 3',            '1 or 2',            '1 or 3',            '2 or 3',            '1 and 2 and 3',            '1 or 2 and 3',            '1 and 2 or 3',            '1 or 2 or 3'            ]         par_in_ruleset.filter.list = ruleset        par_in_ruleset.value = '1 and 2 or 3'        par_in_ruleset.enabled = False               # alert user         par_in_alert_user = arcpy.Parameter(                              displayName='Set alert user',                              name='in_alert_user',                              datatype='GPString',                              parameterType='Required',                              direction='Input',                              category='Alerts and Email',                              multiValue=False)        par_in_alert_user.filter.type = 'ValueList'        par_in_alert_user.filter.list = ['Yes', 'No']        par_in_alert_user.value = 'No'        par_in_alert_user.enabled = False             # alert method        par_in_alert_method = arcpy.Parameter(                           displayName='Set alert method',                           name='in_alert_method',                           datatype='GPString',                           parameterType='Required',                           direction='Input',                           category='Alerts and Email',                           multiValue=False)        par_in_alert_method.filter.type = 'ValueList'        par_in_alert_method.filter.list = ['Static', 'Dynamic']        par_in_alert_method.value = 'Static'        par_in_alert_method.enabled = False                        # alert direction         par_in_alert_direction = arcpy.Parameter(                                   displayName='Direction of change for alert',                                   name='in_alert_direction',                                   datatype='GPString',                                   parameterType='Required',                                   direction='Input',                                   category='Alerts and Email',                                   multiValue=False)        directions = [            'Incline only (any)',             'Decline only (any)',             'Incline only (+ zones only)',             'Decline only (- zones only)',             'Incline or Decline (any)',            'Incline or Decline (+/- zones only)'            ]        par_in_alert_direction.filter.type = 'ValueList'        par_in_alert_direction.filter.list = directions        par_in_alert_direction.value = 'Decline only (any)'        par_in_alert_direction.enabled = False            # email         par_in_email = arcpy.Parameter(                         displayName='Set user email address',                         name='in_email',                         datatype='GPString',                         parameterType='Optional',                         direction='Input',                         category='Alerts and Email',                         multiValue=False)        par_in_email.enabled = False           # ignore         par_in_ignore = arcpy.Parameter(                          displayName='Ignore during monitoring',                          name='in_ignore_user',                          datatype='GPString',                          parameterType='Required',                          direction='Input',                          category='Other',                          multiValue=False)        par_in_ignore.filter.type = 'ValueList'        par_in_ignore.filter.list = ['Yes', 'No']        par_in_ignore.value = 'No'        par_in_ignore.enabled = False          # combine parameters        parameters = [            par_in_feat,            par_in_new_feat,            par_in_set_area,            par_in_platform,            par_in_s_year,            par_in_e_year,            par_in_veg_idx,            par_in_persistence,            par_in_min_conseqs,            par_in_inc_plateaus,            par_in_min_stdv,            par_in_num_zones,            par_in_ruleset,            par_in_alert_user,            par_in_alert_method,            par_in_alert_direction,            par_in_email,            par_in_ignore            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # set required fields         required_fields = [            'area_id',             'platform',             's_year',             'e_year',             'index',            'persistence',            'rule_1_min_conseqs',            'rule_1_inc_plateaus',            'rule_2_min_zone',             'rule_3_num_zones',            'ruleset',            'alert',            'method',            'alert_direction',            'email',            'ignore',            'color',            'global_id'            ]         # globals         global NRT_CREATE_AREA        # unpack global parameter values         curr_feat = NRT_CREATE_AREA.get('in_feat')        # check existing feature input        if parameters[0].valueAsText != curr_feat:                        try:                # load column names                in_feat = parameters[0].valueAsText                                cols = [f.name for f in arcpy.ListFields(in_feat)]            except:                cols = []            # if invalid fields, abort            if all(f in cols for f in required_fields):                for i in range(1, 18):                    parameters[i].enabled = True            else:                for i in range(1, 18):                    parameters[i].enabled = False        # update global values        NRT_CREATE_AREA = {'in_feat': parameters[0].valueAsText}        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the NRT Create Monitoring Areas tool.        """                # safe imports        import os        import uuid     # arcgis comes with these        import arcpy    # arcgis comes with these                # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc                    # module folder            sys.path.append(FOLDER_MODULES)            import nrt        except Exception as e:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            arcpy.AddMessage(str(e))            return                # grab parameter values         in_exist_feat = parameters[0]                     # input monitoring areas feature        in_new_feat = parameters[1]                       # input new monitoring areas feature        in_area_id = parameters[2].value                  # input monitoring area id        in_platform = parameters[3].value                 # input platform        in_s_year = parameters[4].value                   # input start year        in_e_year = parameters[5].value                   # input end year        in_veg_idx = parameters[6].value                  # input vegetation index        in_persistence = parameters[7].value              # input persistence        in_rule_1_min_conseqs = parameters[8].value       # input rule 1 min conseqs        in_rule_1_inc_plateaus = parameters[9].value      # input rule 1 include plateaus        in_rule_2_min_zone = parameters[10].value         # input rule 2 min stdvs        in_rule_3_num_zones = parameters[11].value        # input rule 3 num zones         in_ruleset = parameters[12].value                 # input rulesets        in_alert_user = parameters[13].value              # input alert user         in_alert_method = parameters[14].value            # input alert method         in_alert_direction = parameters[15].value         # input alert direction        in_email = parameters[16].value                   # input email        in_ignore = parameters[17].value                  # input ignore                                # # # # #        # notify user and set up progress bar         arcpy.AddMessage('Beginning NRT Create Monitoring Areas.')        arcpy.SetProgressor(type='step',                            message='Preparing parameters...',                            min_range=0, max_range=9)                # # # # #        # notify user and increment progress bar         arcpy.SetProgressorLabel('Validating existing feature...')         arcpy.SetProgressorPosition(1)         # get full path to existing monitoring areas        exist_feat_desc = arcpy.Describe(parameters[0].value)        in_exist_feat = os.path.join(exist_feat_desc.path, exist_feat_desc.name)        # check if input is valid        if not nrt.validate_monitoring_areas(in_exist_feat):            arcpy.AddError('Existing monitoring feature is incompatible.')            return                                    # # # # #        # notify user and increment progress bar         arcpy.SetProgressorLabel('Validating new feature...')         arcpy.SetProgressorPosition(2)                 # get path to new layer         new_feat_desc = arcpy.Describe(in_new_feat)        in_new_feat = os.path.join(new_feat_desc.path, new_feat_desc.name)        # check if new feature is valid based on name        if 'NRT Create Monitoring Areas Draw' not in parameters[1].valueAsText:            arcpy.AddError('New monitoring feature is incompatible.')            return        try:            # check if only one new feature drawn            with arcpy.da.SearchCursor(in_new_feat, field_names=['SHAPE@']) as cursor:                row_count = len([row for row in cursor])                except Exception as e:            arcpy.AddError('Could not count new feature records.')            arcpy.AddMessage(str(e))            return                    # check if row count is one        if row_count != 1:            arcpy.AddError('Only one new monitoring area can be added at a time.')            return        try:            # fetch a list of all existing feature ids            with arcpy.da.SearchCursor(in_exist_feat, field_names=['area_id']) as cursor:                existing_area_ids = [row[0] for row in cursor]                except Exception as e:            arcpy.AddError('Could not count new feature records.')            arcpy.AddMessage(str(e))            return           # check if existing id already exists        if in_area_id in existing_area_ids:            arcpy.AddError('New area identifier {} already used.'.format(in_area_id))            return        # # # # #        # notify user and increment progress bar         arcpy.SetProgressorLabel('Preparing new feature geometry...')         arcpy.SetProgressorPosition(3)                  try:            # get geometry of new feature in wgs84            with arcpy.da.SearchCursor(in_new_feat, field_names=['SHAPE@']) as cursor:                row = cursor.next()                srs = arcpy.SpatialReference(4326)                poly = arcpy.Polygon(arcpy.Array(row[0]), srs)                except Exception as e:            arcpy.AddError('Could not prepare new feature geometry.')            arcpy.AddMessage(str(e))                return        # # # # #        # notify user and increment progress bar         arcpy.SetProgressorLabel('Inserting new feature into existing feature...')         arcpy.SetProgressorPosition(4)                 # set required fields         data = {            'area_id':             in_area_id,             'platform':            in_platform,             's_year':              in_s_year,             'e_year':              in_e_year,             'index':               in_veg_idx,            'persistence':         in_persistence,            'rule_1_min_conseqs':  in_rule_1_min_conseqs,            'rule_1_inc_plateaus': in_rule_1_inc_plateaus,            'rule_2_min_zone':     in_rule_2_min_zone,             'rule_3_num_zones':    in_rule_3_num_zones,            'ruleset':             in_ruleset,            'alert':               in_alert_user,            'method':              in_alert_method,            'alert_direction':     in_alert_direction,            'email':               in_email,            'ignore':              in_ignore,            'color':               0,                 # default color            'global_id':           uuid.uuid4().hex,  # generate guid            'SHAPE@':              poly            }        # validate new area before insertion        if not nrt.validate_monitoring_area(tuple(data.values())[:-3]):            arcpy.AddError('New monitoring area is incompatible.')            return                try:            # insert new area into existing feature            with arcpy.da.InsertCursor(in_exist_feat, field_names=list(data.keys())) as cursor:                inserted = cursor.insertRow(list(data.values()))                except Exception as e:            arcpy.AddError('Could not insert new area into existing features.')            arcpy.AddMessage(str(e))            return                                # # # # #        # notify user and increment progress bar         arcpy.SetProgressorLabel('Refreshing spatial index...')         arcpy.SetProgressorPosition(5)                        try:            # recalculate spatial index             arcpy.AddSpatialIndex_management(in_exist_feat)                except Exception as e:            arcpy.AddWarning('Could not refresh spatial index, skipping..')            arcpy.AddMessage(str(e))            pass                                   # # # # #        # notify user and increment progress bar         arcpy.SetProgressorLabel('Removing features from map...')         arcpy.SetProgressorPosition(6)                 try:                       # for current project, open current map            aprx = arcpy.mp.ArcGISProject('CURRENT')            m = aprx.activeMap                        # remove all layers associated with monitoring areas            for layer in m.listLayers():                if layer.name == 'monitoring_areas':                    m.removeLayer(layer)                elif 'NRT Create Monitoring Areas Draw' in layer.name:                    m.removeLayer(layer)                except Exception as e:            arcpy.AddWarning('Could not remove new feature from map, skipping.')            arcpy.AddMessage(str(e))            pass        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Adding output to map...')        arcpy.SetProgressorPosition(7)        try:                       # for current project, open current map, re-add area feature            aprx = arcpy.mp.ArcGISProject('CURRENT')            m = aprx.activeMap            m.addDataFromPath(in_exist_feat)                        # update all monitoring area features symbology            for layer in m.listLayers('monitoring_areas'):                arc.apply_monitoring_area_symbology(layer)                    except Exception as e:            arcpy.AddWarning('Could not visualise output, aborting visualisation.')            arcpy.AddMessage(str(e))            pass        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(8)        # notify user        arcpy.AddMessage('Created new NRT Monitoring Area.')                returnclass NRT_Modify_Monitoring_Areas(object):    def __init__(self):        """        Initialise tool.        """                # set input feature class        self.label = 'NRT Modify Monitoring Areas'        self.description = 'Modify existing NRT monitoring area features.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set up UI parameters / controls.        """                # set input feature class        par_in_feat = arcpy.Parameter(                        displayName='Input Monitoring Area Featureclass',                        name='in_feat',                        datatype='GPFeatureLayer',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_in_feat.filter.list = ['Polygon']                # set monitoring area        par_in_set_area = arcpy.Parameter(                            displayName='Select the monitoring area to modify',                            name='in_set_area',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            multiValue=False)        par_in_set_area.filter.type = 'ValueList'        par_in_set_area.enabled = False                # platform        par_in_platform = arcpy.Parameter(                            displayName='Satellite platform',                            name='in_platform',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            multiValue=False)        par_in_platform.filter.type = 'ValueList'        par_in_platform.filter.list = ['Landsat', 'Sentinel']        par_in_platform.value = 'Landsat'        par_in_platform.enabled = False                # start year        par_in_s_year = arcpy.Parameter(                          displayName='Pre-impact start year',                          name='in_s_year',                          datatype='GPLong',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_in_s_year.filter.type = 'Range'        par_in_s_year.filter.list = [1980, 2050]        par_in_s_year.enabled = False                # end year        par_in_e_year = arcpy.Parameter(                          displayName='Pre-impact end year',                          name='in_e_year',                          datatype='GPLong',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_in_e_year.filter.type = 'Range'        par_in_e_year.filter.list = [1980, 2050]        par_in_e_year.enabled = False        # vegetation index        par_in_veg_idx = arcpy.Parameter(                           displayName='Vegetation index',                           name='in_veg_idx',                           datatype='GPString',                           parameterType='Required',                           direction='Input',                           multiValue=False)        par_in_veg_idx.filter.type = 'ValueList'        par_in_veg_idx.filter.list = ['NDVI', 'MAVI', 'kNDVI']        par_in_veg_idx.value = 'MAVI'        par_in_veg_idx.enabled = False                # persistence        par_in_persistence = arcpy.Parameter(                               displayName='Vegetation persistence',                               name='in_persistence',                               datatype='GPDouble',                               parameterType='Required',                               direction='Input',                               multiValue=False)        par_in_persistence.filter.type = 'Range'        par_in_persistence.filter.list = [0.001, 9.999]        par_in_persistence.value = 0.5        par_in_persistence.enabled = False               # rule 1 min conseqs        par_in_min_conseqs = arcpy.Parameter(                               displayName='Rule 1: Minimum consequtives',                               name='in_min_conseqs',                               datatype='GPLong',                               parameterType='Required',                               direction='Input',                               category='Rules',                               multiValue=False)        par_in_min_conseqs.filter.type = 'Range'        par_in_min_conseqs.filter.list = [0, 99]        par_in_min_conseqs.value = 3        par_in_min_conseqs.enabled = False                # rule 1 include plateaus        par_in_inc_plateaus = arcpy.Parameter(                                displayName='Rule 1: Include plateaus',                                name='in_inc_plateaus',                                datatype='GPString',                                parameterType='Required',                                direction='Input',                                category='Rules',                                multiValue=False)        par_in_inc_plateaus.filter.type = 'ValueList'        par_in_inc_plateaus.filter.list = ['Yes', 'No']        par_in_inc_plateaus.value = 'No'        par_in_inc_plateaus.enabled = False                # rule 2 minimum zone        par_in_min_zone = arcpy.Parameter(                            displayName='Rule 2: Minimum Zone',                            name='in_min_zone',                            datatype='GPLong',                            parameterType='Required',                            category='Rules',                            direction='Input',                            multiValue=False)        par_in_min_zone.filter.type = 'Range'        par_in_min_zone.filter.list = [0, 99]        par_in_min_zone.value = 1        par_in_min_zone.enabled = False                  # rule 3 number of zones        par_in_num_zones = arcpy.Parameter(                             displayName='Rule 3: Number of zones',                             name='in_num_zones',                             datatype='GPLong',                             parameterType='Required',                             category='Rules',                             direction='Input',                             multiValue=False)        par_in_num_zones.filter.type = 'Range'        par_in_num_zones.filter.list = [0, 99]        par_in_num_zones.value = 2        par_in_num_zones.enabled = False                       # ruleset        par_in_ruleset = arcpy.Parameter(                           displayName='Ruleset',                           name='in_ruleset',                           datatype='GPString',                           parameterType='Required',                           direction='Input',                           category='Rules',                           multiValue=False)        par_in_ruleset.filter.type = 'ValueList'        ruleset = [            '1 Only',            '2 Only',            '3 Only',            '1 and 2',            '1 and 3',            '2 and 3',            '1 or 2',            '1 or 3',            '2 or 3',            '1 and 2 and 3',            '1 or 2 and 3',            '1 and 2 or 3',            '1 or 2 or 3'            ]         par_in_ruleset.filter.list = ruleset        par_in_ruleset.value = '1 and 2 or 3'        par_in_ruleset.enabled = False               # alert user         par_in_alert_user = arcpy.Parameter(                              displayName='Set alert user',                              name='in_alert_user',                              datatype='GPString',                              parameterType='Required',                              direction='Input',                              category='Alerts and Email',                              multiValue=False)        par_in_alert_user.filter.type = 'ValueList'        par_in_alert_user.filter.list = ['Yes', 'No']        par_in_alert_user.value = 'No'        par_in_alert_user.enabled = False             # alert method        par_in_alert_method = arcpy.Parameter(                           displayName='Set alert method',                           name='in_alert_method',                           datatype='GPString',                           parameterType='Required',                           direction='Input',                           category='Alerts and Email',                           multiValue=False)        par_in_alert_method.filter.type = 'ValueList'        par_in_alert_method.filter.list = ['Static', 'Dynamic']        par_in_alert_method.value = 'Static'        par_in_alert_method.enabled = False                        # alert direction         par_in_alert_direction = arcpy.Parameter(                                   displayName='Direction of change for alert',                                   name='in_alert_direction',                                   datatype='GPString',                                   parameterType='Required',                                   direction='Input',                                   category='Alerts and Email',                                   multiValue=False)        directions = [            'Incline only (any)',             'Decline only (any)',             'Incline only (+ zones only)',             'Decline only (- zones only)',             'Incline or Decline (any)',            'Incline or Decline (+/- zones only)'            ]        par_in_alert_direction.filter.type = 'ValueList'        par_in_alert_direction.filter.list = directions        par_in_alert_direction.value = 'Decline only (any)'        par_in_alert_direction.enabled = False            # email         par_in_email = arcpy.Parameter(                         displayName='Set user email address',                         name='in_email',                         datatype='GPString',                         parameterType='Optional',                         direction='Input',                         category='Alerts and Email',                         multiValue=False)        par_in_email.enabled = False           # ignore         par_in_ignore = arcpy.Parameter(                          displayName='Ignore during monitoring',                          name='in_ignore_user',                          datatype='GPString',                          parameterType='Required',                          direction='Input',                          category='Other',                          multiValue=False)        par_in_ignore.filter.type = 'ValueList'        par_in_ignore.filter.list = ['Yes', 'No']        par_in_ignore.value = 'No'        par_in_ignore.enabled = False          # combine parameters        parameters = [            par_in_feat,            par_in_set_area,            par_in_platform,            par_in_s_year,            par_in_e_year,            par_in_veg_idx,            par_in_persistence,            par_in_min_conseqs,            par_in_inc_plateaus,            par_in_min_zone,            par_in_num_zones,            par_in_ruleset,            par_in_alert_user,            par_in_alert_method,            par_in_alert_direction,            par_in_email,            par_in_ignore        ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when         controls are changed on ArcGIS Pro panel.        """                # set required fields         required_fields = [            'area_id',             'platform',             's_year',             'e_year',             'index',            'persistence',            'rule_1_min_conseqs',            'rule_1_inc_plateaus',            'rule_2_min_zone',             'rule_3_num_zones',            'ruleset',            'alert',            'method',            'alert_direction',            'email',            'ignore',            'color',            'global_id'            ]                 # globals         global NRT_MODIFY_AREA                 # unpack global parameter values        curr_feat = NRT_MODIFY_AREA.get('in_feat')        curr_area_id = NRT_MODIFY_AREA.get('in_area_id')           # if not first run or no change to area id, skip        if curr_area_id is not None and curr_area_id == parameters[1].value:            return                 # check feature input        if parameters[0].value is not None:            try:                # load column names                in_feat = parameters[0].valueAsText                                cols = [f.name for f in arcpy.ListFields(in_feat)]            except:                cols = []                    # if valid fields, proceed to get all rows            if all(f in cols for f in required_fields):                try:                    with arcpy.da.SearchCursor(in_feat, field_names=required_fields) as cursor:                        rows = [rec for rec in cursor]                except:                    return                # if first time, get first row values, else user selected                row = None                if parameters[1].value is None:                    row = rows[0]                else:                    for row in rows:                        if row[0] == parameters[1].value:                            break                                   # enable, populate and set parameters                if row is not None:                    parameters[1].enabled = True                    parameters[1].filter.list = [rec[0] for rec in rows]                    parameters[1].value = row[0]                                        # use loop for rest, they're all the same                     for i in range(1, 16):                        parameters[i + 1].enabled = True                        parameters[i + 1].value = row[i]                    # update global                    NRT_MODIFY_AREA = {'in_area_id': parameters[1].value}        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the NRT Modify Monitoring Areas tool.        """                # safe imports        import os        import arcpy     # arcgis comes with these                # risky imports        try:            import xarray as xr        except Exception as e:            arcpy.AddError('Could not import xarray module.')            arcpy.AddMessage(str(e))            return          # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc                        # module folder            sys.path.append(FOLDER_MODULES)            import nrt        except Exception as e:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            arcpy.AddMessage(str(e))            return                         # grab parameter values         in_feat = parameters[0]                           # input monitoring areas feature        in_area_id = parameters[1].value                  # input monitoring area id        in_platform = parameters[2].value                 # input platform        in_s_year = parameters[3].value                   # input start year        in_e_year = parameters[4].value                   # input end year        in_veg_idx = parameters[5].value                  # input vegetation index        in_persistence = parameters[6].value              # input persistence        in_rule_1_min_conseqs = parameters[7].value       # input rule 1 min conseqs        in_rule_1_inc_plateaus = parameters[8].value      # input rule 1 include plateaus        in_rule_2_min_zone = parameters[9].value          # input rule 2 min stdvs        in_rule_3_num_zones = parameters[10].value        # input rule 3 num zones         in_ruleset = parameters[11].value                 # input rulesets        in_alert_user = parameters[12].value              # input alert user         in_alert_method = parameters[13].value            # input alert method         in_alert_direction = parameters[14].value         # input alert direction        in_email = parameters[15].value                   # input email        in_ignore = parameters[16].value                  # input ignore                        # # # # #        # notify user and set up progress bar         arcpy.AddMessage('Beginning NRT Modify Monitoring Areas.')        arcpy.SetProgressor(type='step',                            message='Preparing parameters...',                            min_range=0, max_range=7)                                # # # # #        # notify user and increment progress bar         arcpy.SetProgressorLabel('Validating existing feature...')         arcpy.SetProgressorPosition(1)         # get full path to existing monitoring areas        exist_feat_desc = arcpy.Describe(parameters[0].value)        in_exist_feat = os.path.join(exist_feat_desc.path, exist_feat_desc.name)        # check if input is valid        if not nrt.validate_monitoring_areas(in_exist_feat):            arcpy.AddError('Existing monitoring feature is incompatible.')            return                      # # # # #        # notify user and increment progress bar         arcpy.SetProgressorLabel('Validating new information...')         arcpy.SetProgressorPosition(2)         # set required fields         data = {            'area_id':             in_area_id,             'platform':            in_platform,             's_year':              in_s_year,             'e_year':              in_e_year,             'index':               in_veg_idx,            'persistence':         in_persistence,            'rule_1_min_conseqs':  in_rule_1_min_conseqs,            'rule_1_inc_plateaus': in_rule_1_inc_plateaus,            'rule_2_min_zone':     in_rule_2_min_zone,             'rule_3_num_zones':    in_rule_3_num_zones,            'ruleset':             in_ruleset,            'alert':               in_alert_user,            'method':              in_alert_method,            'alert_direction':     in_alert_direction,            'email':               in_email,            'ignore':              in_ignore,            }        # validate new area before insertion        if not nrt.validate_monitoring_area(tuple(data.values())):            arcpy.AddError('Modified monitoring area information is incompatible.')            return        # # # # #        # notify user and increment progress bar         arcpy.SetProgressorLabel('Updating feature...')         arcpy.SetProgressorPosition(3)         try:            # open feature and update it at current id            with arcpy.da.UpdateCursor(in_exist_feat, field_names=list(data.keys())) as cursor:                for row in cursor:                                        # update for current id                    if row[0] == in_area_id:                        row[1] = data.get('platform')                        row[2] = data.get('s_year')                        row[3] = data.get('e_year')                        row[4] = data.get('index')                        row[5] = data.get('persistence')                        row[6] = data.get('rule_1_min_conseqs')                        row[7] = data.get('rule_1_inc_plateaus')                        row[8] = data.get('rule_2_min_zone')                        row[9] = data.get('rule_3_num_zones')                        row[10] = data.get('ruleset')                        row[11] = data.get('alert')                        row[12] = data.get('method')                        row[13] = data.get('alert_direction')                        row[14] = data.get('email')                        row[15] = data.get('ignore')                        # update row                        cursor.updateRow(row)        except Exception as e:            arcpy.AddError('Could not update existing feature.')            arcpy.AddMessage(str(e))            return        # # # # #        # notify user and increment progress bar         arcpy.SetProgressorLabel('Refreshing spatial index...')         arcpy.SetProgressorPosition(4)                try:            # recalculate spatial index             arcpy.AddSpatialIndex_management(in_exist_feat)        except Exception as e:            arcpy.AddWarning('Could not refresh spatial index, skipping..')            arcpy.AddMessage(str(e))            pass           # # # # #        # notify user and increment progress bar         arcpy.SetProgressorLabel('Removing features from map...')         arcpy.SetProgressorPosition(5)         try:                       # for current project, open current map            aprx = arcpy.mp.ArcGISProject('CURRENT')            m = aprx.activeMap            # remove all layers associated with monitoring areas            for layer in m.listLayers():                if layer.name == 'monitoring_areas':                    m.removeLayer(layer)        except Exception as e:            arcpy.AddWarning('Could not remove new feature from map, skipping.')            arcpy.AddMessage(str(e))            pass        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Adding output to map...')        arcpy.SetProgressorPosition(6)        try:                       # for current project, open current map, re-add area feature            aprx = arcpy.mp.ArcGISProject('CURRENT')            m = aprx.activeMap            m.addDataFromPath(in_exist_feat)            # update all monitoring area features symbology            for layer in m.listLayers('monitoring_areas'):                arc.apply_monitoring_area_symbology(layer)        except Exception as e:            arcpy.AddWarning('Could not visualise output, aborting visualisation.')            arcpy.AddMessage(str(e))            pass        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(7)        # notify user        arcpy.AddMessage('Modified existing NRT Monitoring Area.')                returnclass NRT_Delete_Monitoring_Areas(object):    def __init__(self):        """        Initialise tool.        """        self.label = 'NRT Delete Monitoring Areas'        self.description = 'Delete existing NRT monitoring area features.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set various ArcGIS Pro UI controls. Data validation        is enforced via ArcGIS Pro API.        """                # set input feature class        par_in_feat = arcpy.Parameter(                        displayName='Input Monitoring Area Featureclass',                        name='in_feat',                        datatype='GPFeatureLayer',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_in_feat.filter.list = ['Polygon']                # set monitoring area        par_in_set_area = arcpy.Parameter(                            displayName='Select monitoring area to delete',                            name='in_area_id',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            multiValue=False)        par_in_set_area.enabled = False                # combine parameters        parameters = [            par_in_feat,            par_in_set_area        ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when         controls are changed on ArcGIS Pro panel.        """                 # set required fields         required_fields = [            'area_id',             'platform',             's_year',             'e_year',             'index',            'persistence',            'rule_1_min_conseqs',            'rule_1_inc_plateaus',            'rule_2_min_zone',             'rule_3_num_zones',            'ruleset',            'alert',            'method',            'alert_direction',            'email',            'ignore',            'color',            'global_id'            ]                 # globals         global NRT_DELETE_AREA        # unpack global parameter values         curr_feat = NRT_DELETE_AREA.get('in_feat')                # check existing feature input        if parameters[0].valueAsText != curr_feat:            try:                # load column names                in_feat = parameters[0].valueAsText                                cols = [f.name for f in arcpy.ListFields(in_feat)]            except:                cols = []            # if invalid fields, abort            if all(f in cols for f in required_fields):                try:                    # get all rows as list of tuples with specific field order                    with arcpy.da.SearchCursor(in_feat, field_names=required_fields) as cursor:                        rows = [rec for rec in cursor]                except:                    return                                    # if first time, get first row values, else user selected                row = None                if parameters[1].value is None:                    row = rows[0]                else:                    for row in rows:                        if row[0] == parameters[1].value:                            break                            # if row exists, enable + populate controls with first row values                 if row is not None:                    parameters[1].enabled = True                    parameters[1].filter.list = [rec[0] for rec in rows]                    parameters[1].value = row[0]                                    else:                    parameters[1].enabled = True                    parameters[1].filter.list = []                    parameters[1].value = None        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the NRT Delete Monitoring Areas tool.        """                # safe imports        import os        import arcpy                # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc                        # module folder            sys.path.append(FOLDER_MODULES)            import nrt        except Exception as e:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            arcpy.AddMessage(str(e))            return                   # grab parameter values         in_exist_feat = parameters[0]                     # input monitoring areas feature        in_area_id = parameters[1].value                  # input monitoring area id                        # # # # #        # notify user and set up progress bar         arcpy.AddMessage('Beginning NRT Delete Monitoring Areas.')        arcpy.SetProgressor(type='step',                            message='Preparing parameters...',                            min_range=0, max_range=6)                                        # # # # #        # notify user and increment progress bar         arcpy.SetProgressorLabel('Validating existing feature...')         arcpy.SetProgressorPosition(1)         # get full path to existing monitoring areas        exist_feat_desc = arcpy.Describe(parameters[0].value)        in_exist_feat = os.path.join(exist_feat_desc.path, exist_feat_desc.name)        # check if input is valid        if not nrt.validate_monitoring_areas(in_exist_feat):            arcpy.AddError('Existing monitoring feature is incompatible.')            return          # # # # #        # notify user and increment progress bar         arcpy.SetProgressorLabel('Deleting monitoring area feature...')         arcpy.SetProgressorPosition(2)                        try:            # delete feature at current id            with arcpy.da.UpdateCursor(in_exist_feat, field_names=['area_id']) as cursor:                for row in cursor:                    if row[0] == in_area_id:                        cursor.deleteRow()        except Exception as e:            arcpy.AddError('Could not delete existing feature.')            arcpy.AddMessage(str(e))            return                                            # # # # #        # notify user and increment progress bar         arcpy.SetProgressorLabel('Refreshing spatial index...')         arcpy.SetProgressorPosition(3)                try:            # recalculate spatial index             arcpy.AddSpatialIndex_management(in_exist_feat)        except Exception as e:            arcpy.AddWarning('Could not refresh spatial index, skipping.')            arcpy.AddMessage(str(e))            pass          # # # # #        # notify user and increment progress bar         arcpy.SetProgressorLabel('Removing features from map...')         arcpy.SetProgressorPosition(4)         try:                       # for current project, open current map            aprx = arcpy.mp.ArcGISProject('CURRENT')            m = aprx.activeMap            # remove all layers associated with monitoring areas            for layer in m.listLayers():                if layer.name == 'monitoring_areas':                    m.removeLayer(layer)        except Exception as e:            arcpy.AddWarning('Could not remove new feature from map, skipping.')            arcpy.AddMessage(str(e))            pass        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Adding output to map...')        arcpy.SetProgressorPosition(5)        try:                       # for current project, open current map, re-add area feature            aprx = arcpy.mp.ArcGISProject('CURRENT')            m = aprx.activeMap            m.addDataFromPath(in_exist_feat)            # update all monitoring area features symbology            for layer in m.listLayers('monitoring_areas'):                arc.apply_monitoring_area_symbology(layer)        except Exception as e:            arcpy.AddWarning('Could not visualise output, aborting visualisation.')            arcpy.AddMessage(str(e))            pass                                # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(6)        # notify user        arcpy.AddMessage('Deleted existing NRT Monitoring Area.')                return# meta, fields, more chartsclass NRT_Build_Graphs(object):    def __init__(self):        """Define the tool (tool name is the name of the class)."""        self.label = "NRT Build Graphs"        self.description = "Helper tool to build html graphs for UI popups."        self.canRunInBackground = True    def getParameterInfo(self):        """Define parameter definitions"""                # input change cube path        par_cube_path = arcpy.Parameter(                          displayName='Filepath to a single change cube',                          name='in_cube_path',                          datatype='GPString',                          parameterType='Required',                          direction='Input',                          multiValue=False)        par_cube_path.value = ''                # input monitoring area parameters        par_area_params = arcpy.Parameter(                            displayName='String of monitoring areas parameters.',                            name='in_area_params',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            multiValue=False)        par_area_params.value = ''                params = [par_cube_path, par_area_params]        return params    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the NRT Create Monitoring Areas tool.        """                # safe imports        import os        import arcpy                # risky imports        try:            import xarray as xr        except:            arcpy.AddMessage('<h3>Could not generate graph information.</h3>')            return                 # set up non-chart html code         overview_template = """        <html>            <head>                <style type="text/css">                    h4 {                        margin-top: 0px;                        margin-bottom: 0px;                    }                    p {                        margin-top: 0px;                        margin-bottom: 0px;                    }                </style>            </head>            <body>                <center>                    <h3>Overview of Selected Area</h3>                </center>                <h4>Area identifier</h4>                <p>//data.OvrAreaId</p>                <br />                <h4>Satellite platform</h4>                <p>//data.OvrPlatform</p>                <br />                <h4>Starting year of pre-impact period</h4>                <p>//data.OvrSYear</p>                <br />                <h4>Ending year of pre-impact period</h4>                <p>//data.OvrEYear</p>                <br />                <h4>Vegetation index</h4>                <p>//data.OvrIndex</p>                <br />                <h4>Model Persistence</h4>                <p>//data.OvrPersistence</p>                <br />                <h4>Rule 1: Minimum consequtives</h4>                <p>//data.OvrRule1MinConseq</p>                <br />                <h4>Rule 1: Include Plateaus</h4>                <p>//data.OvrRule1IncPlateaus</p>                <br />                <h4>Rule 2: Minimum Std. Dev.</h4>                <p>//data.OvrRule2MinStdv</p>                <br />                <h4>Rule 2: Bidirectional</h4>                <p>//data.OvrRule2Bidirectional</p>                <br />                <h4>Rule 3: Number of Zones</h4>                <p>//data.OvrRule3NumZones</p>                <br />                <h4>Ruleset</h4>                <p>//data.OvrRuleset</p>                <br />                <h4>Alert via email</h4>                <p>//data.OvrAlert</p>                <br />                <h4>Alert direction</h4>                <p>//data.OvrAlertDirection</p>                <br />                <h4>User email</h4>                <p>//data.OvrEmail</p>                <br />                <h4>Ignore during monitoring</h4>                <p>//data.OvrIgnore</p>                <br />            </body>        </html>        """        # set up table html template code        table_template = """            <html>                <head>                    <script type="text/javascript" src="https://www.gstatic.com/charts/loader.js"></script>                    <script type="text/javascript">                        google.charts.load('current', { packages: ['table'] });                        google.charts.setOnLoadCallback(drawBasic);                        function drawBasic() {                            var data = new google.visualization.DataTable();                            data.addColumn('string', 'Date');                            data.addColumn('number', 'Zone');                            //data.addRows                                                        data.addRows([                            ['2010-01-01', {v: -2, f: '-2'}],                            ['2010-05-11', {v: -3, f: '-3'}],                            ['2010-09-19', {v: -5, f: '-5'}],                            ]);                            var options = {                                //legend: {                                    //position: 'none',                                //},                                //chartArea: {                                    //width: '100%',                                     //height: '100%',                                    //top: 25,                                    //bottom: 100,                                    //left: 75,                                    //right: 25                                    //},                                //hAxis: {                                    //title: '//data.xAxisTitle',                                    //textStyle: {fontSize: '9'},                                    //slantedText: true,                                     //slantedTextAngle: 90                                //},                                //vAxis: {                                    //title: '//data.vAxisTitle',                                    //textStyle: {fontSize: '9'},                                //},                                //colors: ['//data.LineColor'],                            };                            var table = new google.visualization.Table(document.getElementById('table_div'));                            table.draw(data, options);                        }                    </script>                </head>                <body>                    <center>                        <h3>Overview</h3>                        <div id="table_div" style="width: 100%; height: 90%"></div>                    </center>                                    </body>            </html>        """        # set up line html template code        line_template = """            <html>                <head>                    <script type="text/javascript" src="https://www.gstatic.com/charts/loader.js"></script>                    <script type="text/javascript">                        google.charts.load('current', { packages: ['corechart', 'line'] });                        google.charts.setOnLoadCallback(drawBasic);                        function drawBasic() {                            var data = new google.visualization.DataTable();                            data.addColumn('string', 'x');                            data.addColumn('number', 'y');                            //data.addRows                            var options = {                                legend: {                                    position: 'none',                                },                                chartArea: {                                    width: '100%',                                     height: '100%',                                    top: 25,                                    bottom: 100,                                    left: 75,                                    right: 25                                    },                                hAxis: {                                    title: '//data.xAxisTitle',                                    textStyle: {fontSize: '9'},                                    slantedText: true,                                     slantedTextAngle: 90                                },                                vAxis: {                                    title: '//data.vAxisTitle',                                    textStyle: {fontSize: '9'},                                },                                colors: ['//data.LineColor'],                            };                            var chart = new google.visualization.LineChart(document.getElementById('curve_chart'));                            chart.draw(data, options);                        }                    </script>                </head>                <body>                    <center>                        <h3>//data.PopupTitle</h3>                        <div id="curve_chart" style="width: 100%; height: 90%"></div>                    </center>                                    </body>            </html>        """                # set up column html template code        column_template = """            <html>                <head>                    <script type="text/javascript" src="https://www.gstatic.com/charts/loader.js"></script>                    <script type="text/javascript">                        google.charts.load('current', { packages: ['corechart', 'bar'] });                        google.charts.setOnLoadCallback(drawBasic);                        function drawBasic() {                            //data.addRows                            var options = {                                legend: {                                    position: 'none',                                },                                chartArea: {                                    width: '100%',                                     height: '100%',                                    top: 25,                                    bottom: 100,                                    left: 75,                                    right: 25                                    },                                hAxis: {                                    title: '//data.xAxisTitle',                                    textStyle: {fontSize: '9'},                                    slantedText: true,                                     slantedTextAngle: 90                                },                                vAxis: {                                    title: '//data.vAxisTitle',                                    textStyle: {fontSize: '9'},                                },                            };                            var chart = new google.visualization.ColumnChart(document.getElementById('column_chart'));                            chart.draw(data, options);                        }                    </script>                </head>                <body>                    <center>                        <h3>//data.PopupTitle</h3>                        <div id="column_chart" style="width: 100%; height: 90%"></div>                    </center>                                    </body>            </html>        """                # set up calendar html template code        calendar_template = """            <html>                <head>                    <script type="text/javascript" src="https://www.gstatic.com/charts/loader.js"></script>                    <script type="text/javascript">                        google.charts.load('current', { packages: ['calendar'] });                        google.charts.setOnLoadCallback(drawBasic);                        function drawBasic() {                            var data = new google.visualization.DataTable();                            data.addColumn('date', 'Date');                            data.addColumn('number', 'Zone');                            //data.addRows                            var options = {                                title: "Alert History",                                //legend: {                                    //position: 'none',                                //},                                //chartArea: {                                    //width: '100%',                                     //height: '100%',                                    //top: 25,                                    //bottom: 100,                                    //left: 75,                                    //right: 25                                    //},                                //hAxis: {                                    //title: '//data.xAxisTitle',                                    //textStyle: {fontSize: '9'},                                    //slantedText: true,                                     //slantedTextAngle: 90                                //},                                //vAxis: {                                    //title: '//data.vAxisTitle',                                    //textStyle: {fontSize: '9'},                                //},                                //colors: ['//data.LineColor'],                            };                            var chart = new google.visualization.Calendar(document.getElementById('calendar_basic'));                            chart.draw(data, options);                        }                    </script>                </head>                <body>                    <center>                        <h3>Alert History</h3>                        <div id="calendar_basic" style="width: 100%; height: 90%"></div>                    </center>                                    </body>            </html>        """        # set up legend html template code        legend_template = """            <html>              <head>                <style>                  td, th {                    border: 1px solid transparent;                    text-align: left;                    padding: 0px;                  }                </style>              </head>              <body>                <center>                  <h3>Zone Legend</h3>                </center>                <table style="width: 100%;">                  <colgroup>                    <col span="1" style="width: 15%;">                    <col span="1" style="width: 15%;">                    <col span="1" style="width: 70%;">                  </colgroup>                  <tr>                    <th>Symbology</th>                    <th>Zone</th>                    <th>Description</th>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid red; background-color: #FF7F7F"></div>                      </div>                    </td>                    <td>-11</td>                    <td>Change deviation is below -19. Decline.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid red; background-color: #FFA77F"></div>                      </div>                    </td>                    <td>-10</td>                    <td>Change deviation is between -17 and -19. Decline.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid red; background-color: #FFD37F"></div>                      </div>                    </td>                    <td>-9</td>                    <td>Change deviation is between -15 and -17. Decline.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid red; background-color: #FFFF73"></div>                      </div>                    </td>                    <td>-8</td>                    <td>Change deviation is between -13 and -15. Decline.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid red; background-color: #D1FF73"></div>                      </div>                    </td>                    <td>-7</td>                    <td>Change deviation is between -11 and -13. Decline.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid red; background-color: #A3FF73"></div>                      </div>                    </td>                    <td>-6</td>                    <td>Change deviation is between -9 and -11. Decline.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid red; background-color: #73FFDF"></div>                      </div>                    </td>                    <td>-5</td>                    <td>Change deviation is between -7 and -9. Decline.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid red; background-color: #73DFFF"></div>                      </div>                    </td>                    <td>-4</td>                    <td>Change deviation is between -5 and -7. Decline.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid red; background-color: #73B2FF"></div>                      </div>                    </td>                    <td>-3</td>                    <td>Change deviation is between -3 and -5. Decline.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid red; background-color: #DF73FF"></div>                      </div>                    </td>                    <td>-2</td>                    <td>Change deviation is between -1 and -3. Decline.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid red; background-color: #FF73DF"></div>                      </div>                    </td>                    <td>-1</td>                    <td>Change deviation is between 0 and -1. Decline.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid black; background-color: white"></div>                      </div>                    </td>                    <td>0</td>                    <td>No change in either direction.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid blue; background-color: #FF73DF"></div>                      </div>                    </td>                    <td>1</td>                    <td>Change deviation is between 0 and 1. Growth.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid blue; background-color: #DF73FF"></div>                      </div>                    </td>                    <td>2</td>                    <td>Change deviation is between 1 and 3. Growth.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid blue; background-color: #73B2FF"></div>                      </div>                    </td>                    <td>3</td>                    <td>Change deviation is between 3 and 5. Growth.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid blue; background-color: #73DFFF"></div>                      </div>                    </td>                    <td>4</td>                    <td>Change deviation is between 5 and 7. Growth.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid blue; background-color: #73FFDF"></div>                      </div>                    </td>                    <td>5</td>                    <td>Change deviation is between 7 and 9. Growth.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid blue; background-color: #A3FF73"></div>                      </div>                    </td>                    <td>6</td>                    <td>Change deviation is between 9 and 11. Growth.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid blue; background-color: #D1FF73"></div>                      </div>                    </td>                    <td>7</td>                    <td>Change deviation is between 11 and 13. Growth.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid blue; background-color: #FFFF73"></div>                      </div>                    </td>                    <td>8</td>                    <td>Change deviation is between 13 and 15. Growth.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid blue; background-color: #FFD37F"></div>                      </div>                    </td>                    <td>9</td>                    <td>Change deviation is between 15 and 17. Growth.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid blue; background-color: #FFA77F"></div>                      </div>                    </td>                    <td>10</td>                    <td>Change deviation is between 17 and 19. Growth.</td>                  </tr>                  <tr>                    <td>                      <div>                        <div style="width: 50px; height: 15px; border: 2px solid blue; background-color: #FF7F7F"></div>                      </div>                    </td>                    <td>11</td>                    <td>Change deviation is above 19. Growth.</td>                  </tr>                </table>              </body>            </html>        """                # # # # # #        # get cube filepath from c#        in_cube_path = parameters[0].value       # full path with filename to change cube from c#        in_area_params = parameters[1].value     # string of area params seperated by ";"                # unpack parameters         params = in_area_params.split(';')                       # check if inputs exist        if in_cube_path == '' or in_area_params == '':            arcpy.AddMessage('<h3>Could not generate graph information.</h3>')            return        elif in_cube_path is None or in_area_params is None:            arcpy.AddMessage('<h3>Could not generate graph information.</h3>')            return        # check if cube file exists         if not os.path.exists(in_cube_path):            arcpy.AddMessage('<h3>Could not generate graph information.</h3>')            return                # # # # # #        # safe open current dataset        try:            ds = xr.open_dataset(in_cube_path)            ds = ds.load()             ds.close()         except:            arcpy.AddMessage('<h3>Could not generate graph information.</h3>')            return                            # # # # # #        # do basic checks on dataset         if 'time' not in ds: #or 'x' not in ds or 'y' not in ds:            arcpy.AddMessage('<h3>Could not generate graph information.</h3>')            return        elif len(ds['time']) == 0:            arcpy.AddMessage('<h3>Could not generate graph information.</h3>')            return                    # reduce to single value per time and remove last date         try:            #ds = ds.mean(['x', 'y'])            ds = ds.isel(time=slice(0, -1))  # -1 when slice, -2 when not        except:            arcpy.AddMessage('<h3>Could not generate graph information.</h3>')            return                # remove all dates before training period         s_year = int(params[2])        ds = ds.where(ds['time.year'] >= s_year, drop=True)                # remove all coinciding indices where nans        # fix this up for when certain vars are empty         #if ds.to_array().isnull().any():            #nan_dts = ds.where(ds.isnull(), drop=True).time            #ds = ds.drop_sel(time=nan_dts)                                # check if anything exists after removal of nan        if len(ds['time']) == 0:            arcpy.AddMessage('<h3>Could not generate graph information.</h3>')            return                #with open(r"C:\Users\Lewis\Desktop\temp.txt", 'a') as f:            #f.write(str(ds.load()))                        # # # # # #        # check if required variables exist         #for field in ['veg_idx', 'static', 'zones', 'cands_inc', 'cands_dec', 'consq_inc', 'consq_dec']:            #if field not in ds:                #arcpy.AddMessage('<h3>Could not generate graph information.</h3>')                #return                # get value vectors for time, veg, change, etc...        try:            dts = ds['time'].dt.strftime('%Y-%m-%d').values        # datetimes as string            veg = ds['veg_clean'].values                           # mean veg index            stc = ds['static_clean'].values                        # static change deviations            zns = ds['static_zones'].values                        # static zones                        # prepare zone data where canidates exist             #zns = ds['zones'].where((ds['cands_inc'] == 1) | (ds['cands_dec'] == 1), 0)            #zns = zns.values                except:            arcpy.AddMessage('<h3>Could not generate graph information.</h3>')            return        # construct html template for overview data        if params != '' or params is not None:            ovr_html = overview_template.replace('//data.OvrAreaId', params[0])            ovr_html = ovr_html.replace('//data.OvrPlatform', params[1])            ovr_html = ovr_html.replace('//data.OvrSYear', params[2])            ovr_html = ovr_html.replace('//data.OvrEYear', params[3])            ovr_html = ovr_html.replace('//data.OvrIndex', params[4])            ovr_html = ovr_html.replace('//data.OvrPersistence', params[5])            ovr_html = ovr_html.replace('//data.OvrRule1MinConseq', params[6])            ovr_html = ovr_html.replace('//data.OvrRule1IncPlateaus', params[7])            ovr_html = ovr_html.replace('//data.OvrRule2MinStdv', params[8])            ovr_html = ovr_html.replace('//data.OvrRule2Bidirectional', params[9])            ovr_html = ovr_html.replace('//data.OvrRule3NumZones', params[10])            ovr_html = ovr_html.replace('//data.OvrRuleset', params[11])            ovr_html = ovr_html.replace('//data.OvrAlert', params[12])            ovr_html = ovr_html.replace('//data.OvrAlertDirection', params[13])            ovr_html = ovr_html.replace('//data.OvrEmail', params[14])            ovr_html = ovr_html.replace('//data.OvrIgnore', params[15])            arcpy.AddMessage(ovr_html)        else:            arcpy.AddMessage('<h3>Could not generate overview information.</h3>')        # construct html template for vegetation index data        if len(dts) == len(veg):            veg_data = [[dts[i], veg[i]] for i in range(len(dts))]              veg_data_block = "data.addRows(" + str(veg_data) + ");"            veg_html = line_template.replace('//data.addRows', veg_data_block)            veg_html = veg_html.replace('//data.PopupTitle', 'Raw Vegetation')            veg_html = veg_html.replace('//data.xAxisTitle', 'Date')            veg_html = veg_html.replace('//data.vAxisTitle', 'Raw Vegetation')            veg_html = veg_html.replace('//data.LineColor',  '#129632')            arcpy.AddMessage(veg_html)        else:            arcpy.AddMessage('<h3>Could not generate vegetation graph.</h3>')                    # construct html template for change data        if len(dts) == len(stc):                        stc_data = [[dts[i], stc[i]] for i in range(len(dts))]              stc_data_block = "data.addRows(" + str(stc_data) + ");"            stc_html = line_template.replace('//data.addRows', stc_data_block)            stc_html = stc_html.replace('//data.PopupTitle', 'Change Deviation')            stc_html = stc_html.replace('//data.xAxisTitle', 'Date')            stc_html = stc_html.replace('//data.vAxisTitle', 'Change Deviation')            stc_html = stc_html.replace('//data.LineColor',  '#961212')            arcpy.AddMessage(stc_html)                    else:            arcpy.AddMessage('<h3>Could not generate change deviation graph.</h3>')                    # construct html template for conseq data        if len(dts) == len(zns):                            # create cmap             cmap = {                -12: "black",                -11: "#FF7F7F",                -10: "#FFA77F",                -9:  "#FFD37F",                -8:  "#FFFF73",                -7:  "#D1FF73",                -6:  "#A3FF73",                -5:  "#73FFDF",                -4:  "#73DFFF",                -3:  "#73B2FF",                -2:  "#DF73FF",                -1:  "#FF73DF",                0:   "#FFFFFF",                1:   "#FF73DF",                2:   "#DF73FF",                3:   "#73B2FF",                4:   "#73DFFF",                5:   "#73FFDF",                6:   "#A3FF73",                7:   "#D1FF73",                8:   "#FFFF73",                9:   "#FFD37F",                10:  "#FFA77F",                11:  "#FF7F7F",                12: "black"}                    # quick check to see if cmap values in cube            try:                [cmap[v] for v in zns]            except:                arcpy.AddMessage('<h3>Could not generate change deviation graph.</h3>')                return            # prepare data statement and header row             zns_data_block = "var data = google.visualization.arrayToDataTable(["            zns_data_block += "['X', 'Y', {role: 'style'}],"                    # construct data array with colors, stringify, append            zns_data = [[dts[i], zns[i], cmap.get(zns[i])] for i in range(len(dts))]              zns_data = ','.join([str(s) for s in zns_data])            zns_data_block += zns_data + ']);'                        # prepare data            zns_html = column_template.replace('//data.addRows', zns_data_block)            zns_html = zns_html.replace('//data.PopupTitle', 'Alert History')            zns_html = zns_html.replace('//data.xAxisTitle', 'Date')            zns_html = zns_html.replace('//data.vAxisTitle', 'Zone when alert triggered.')            arcpy.AddMessage(zns_html)                   else:            arcpy.AddMessage('<h3>Could not generate alert history graph.</h3>')                # construct html template for legend         arcpy.AddMessage(legend_template)         return# needs tidying up for progressor, qolclass NRT_Monitor_Areas(object):    def __init__(self):        """Define the tool (tool name is the name of the class)."""        self.label = "NRT Monitor Areas"        self.description = "Monitor designated monitoring areas."        self.canRunInBackground = False    def getParameterInfo(self):        """        Set various ArcGIS Pro UI controls. Data validation        is enforced via ArcGIS Pro API.        """                # input monitoring area features        par_in_feat = arcpy.Parameter(                        displayName='Input monitoring area features',                        name='in_feat',                        datatype='GPFeatureLayer',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_in_feat.filter.list = ['Polygon']                # input continuous monitoring        par_ongoing = arcpy.Parameter(                        displayName='Continuously monitor areas',                        name='in_ongoing',                        datatype='GPBoolean',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_ongoing.value = False                # time interval months        par_time_interval = arcpy.Parameter(                              displayName='Set hours between monitoring checks',                              name='in_time_interval',                              datatype='GPLong',                              parameterType='Required',                              direction='Input',                              multiValue=False)        par_time_interval.filter.type = 'Range'        par_time_interval.filter.list = [1, 10000]        par_time_interval.value = 12        # email from         par_email_from = arcpy.Parameter(                           displayName='Host email address',                           name='in_email_from',                           datatype='GPString',                           parameterType='Optional',                           direction='Input',                           category='Email Alert Service',                           multiValue=False)        par_email_from.value = 'mrlewie@outlook.com'        # smtp email server        par_smtp_server = arcpy.Parameter(                           displayName='SMTP server address',                           name='in_smtp_server',                           datatype='GPString',                           parameterType='Optional',                           direction='Input',                           category='Email Alert Service',                           multiValue=False)        par_smtp_server.value = 'smtp.office365.com'                # smtp port        par_smtp_port = arcpy.Parameter(                           displayName='SMTP server port',                           name='in_smtp_port',                           datatype='GPLong',                           parameterType='Optional',                           direction='Input',                           category='Email Alert Service',                           multiValue=False)        par_smtp_port.value = 587        # smtp username        par_smtp_username = arcpy.Parameter(                              displayName='SMTP server username',                              name='in_smtp_username',                              datatype='GPString',                              parameterType='Optional',                              direction='Input',                              category='Email Alert Service',                              multiValue=False)        par_smtp_username.value = 'mrlewie@outlook.com'                # smtp username        par_smtp_password = arcpy.Parameter(                              displayName='SMTP server password',                              name='in_smtp_password',                              datatype='GPString',                              parameterType='Optional',                              direction='Input',                              category='Email Alert Service',                              multiValue=False)        par_smtp_password.value = '***'        # # output epsg        # par_output_epsg = arcpy.Parameter(                            # displayName='Output EPSG',                            # name='in_output_epsg',                            # datatype='GPLong',                            # parameterType='Required',                            # direction='Input',                            # category='Warping Options',                            # multiValue=False)        # par_output_epsg.filter.list = [3577]        # par_output_epsg.values = 3577                # # set oa class values        # par_fmask_flags = arcpy.Parameter(displayName='Include pixels flags',                            # name='in_fmask_flags',                            # datatype='GPString',                            # parameterType='Required',                            # direction='Input',                            # category='Satellite Quality Options',                            # multiValue=True)        # flags = [            # 'NoData',             # 'Valid',             # 'Cloud',             # 'Shadow',             # 'Snow',             # 'Water'            # ]        # par_fmask_flags.filter.type = 'ValueList'                # par_fmask_flags.filter.list = flags        # par_fmask_flags.values = ['Valid', 'Snow', 'Water']                # # max cloud cover        # par_max_cloud = arcpy.Parameter(                          # displayName='Maximum cloud cover',                          # name='in_max_cloud',                          # datatype='GPDouble',                          # parameterType='Required',                          # direction='Input',                          # category='Satellite Quality Options',                          # multiValue=False)        # par_max_cloud.filter.type = 'Range'        # par_max_cloud.filter.list = [0.0, 100.0]        # par_max_cloud.value = 0.0                # # input interpolate        # par_interpolate = arcpy.Parameter(                            # displayName='Interpolate NoData pixels',                            # name='in_interpolate',                            # datatype='GPBoolean',                            # parameterType='Required',                            # direction='Input',                            # category='Satellite Quality Options',                            # multiValue=False)        # par_interpolate.value = True                # combine parameters        parameters = [            par_in_feat,            par_ongoing,            par_time_interval,            par_email_from,            par_smtp_server,            par_smtp_port,            par_smtp_username,            par_smtp_password,            #par_output_epsg,            #par_fmask_flags,            #par_max_cloud,            #par_interpolate        ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the NRT Monitor Areas module.        """                # set gdal global environ        import os        os.environ['GDAL_DISABLE_READDIR_ON_OPEN'] = 'EMPTY_DIR'        os.environ['CPL_VSIL_CURL_ALLOWED_EXTENSIONS '] = 'tif'        os.environ['VSI_CACHE '] = 'TRUE'        os.environ['GDAL_HTTP_MULTIRANGE '] = 'YES'        os.environ['GDAL_HTTP_MERGE_CONSECUTIVE_RANGES '] = 'YES'                # also set rasterio env variables        rasterio_env = {            'GDAL_DISABLE_READDIR_ON_OPEN': 'EMPTY_DIR',            'CPL_VSIL_CURL_ALLOWED_EXTENSIONS':'tif',            'VSI_CACHE': True,            'GDAL_HTTP_MULTIRANGE': 'YES',            'GDAL_HTTP_MERGE_CONSECUTIVE_RANGES': 'YES'        }        # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)                # safe imports        import sys                      # arcgis comes with these        import shutil                   # arcgis comes with these        import datetime                 # arcgis comes with these        import numpy as np              # arcgis comes with these        import arcpy                    # arcgis comes with these        import tempfile                 # arcgis comes with these        from datetime import datetime   # arcgis comes with these                # risky imports (not native to arcgis)        try:            import xarray as xr            import dask            import rasterio            import pystac_client            import osr            from odc import stac            from osgeo import gdal            from osgeo import ogr            from osgeo import osr        except:            arcpy.AddError('Python libraries xarray, dask, rasterio, pystac, or odc not installed.')            return                    # import tools        try:            # shared folder            sys.path.append(FOLDER_SHARED)            import arc, satfetcher, tools            # module folder            sys.path.append(FOLDER_MODULES)            import nrt, cog_odc, cog        except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            return           # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)        warnings.simplefilter(action='ignore', category=RuntimeWarning)        warnings.simplefilter(action='ignore', category=UserWarning)        warnings.simplefilter(action='ignore', category=dask.array.core.PerformanceWarning)                             # grab parameter values         in_feat = parameters[0]                            # input monitoring area polygon features        in_ongoing = parameters[1].value                   # perform ongoing monitoring        in_time_interval = parameters[2].value             # hours between checks        in_email_from = parameters[3].value                # email from         in_smtp_server = parameters[4].value               # email smtp server         in_smtp_port = parameters[5].value                 # email smtp port         in_smtp_username = parameters[6].value             # email smtp username         in_smtp_password = parameters[7].value             # email smtp password         #in_epsg = parameters[8].value                      # input epsg        #in_fmask_flags = parameters[9].valueAsText         # fmask flag values        #in_max_cloud = parameters[10].value                # max cloud percentage        #in_interpolate = parameters[11].value              # interpolate missing pixels        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning NRT Monitoring of areas.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0, max_range=20)        # convert fmask as text to numeric code equivalents              #in_fmask_flags = [e for e in in_fmask_flags.split(';')]                #in_fmask_flags = arc.convert_fmask_codes(in_fmask_flags)                # check if cloud cover is valid        #if in_max_cloud < 0 or in_max_cloud > 100:            #arcpy.AddError('Cloud cover must be between 0 and 100.')            #raise                    # check if time interval is > 0        in_time_interval = in_time_interval * 60 * 60        if in_time_interval <= 0:            arcpy.AddError('Time interval must be above 0 hours.')            return                    # get and check feat count        #feat_count = lyr.GetFeatureCount()        #if feat_count == 0:            #print('No monitoring areas found in feature, flagging as invalid.')            #is_valid = False        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Checking parameters...')        arcpy.SetProgressorPosition(1)        # prepare features shapefile        shp_desc = arcpy.Describe(in_feat)        in_feat = os.path.join(shp_desc.path, shp_desc.name)                                    # validate monitoring area feature class        if not nrt.validate_monitoring_areas(in_feat):            arcpy.AddError('Monitoring areas feature is invalid.')            return                    # get input featureclass file, get dir and filename        in_name = os.path.basename(in_feat)     # name of monitor fc        in_gdb = os.path.dirname(in_feat)       # path of gdb                # check gdv extension        if not in_gdb.endswith('.gdb'):            arcpy.AddError('Feature class is not in a geodatabase.')            return        else:            in_path = os.path.splitext(in_gdb)[0]   # path of gdb without ext            in_data_path = in_path + '_' + 'cubes'  # associated cube data folder                    # check if cubes folder exists        if not os.path.exists(in_data_path):            arcpy.AddError('Could not find cube folder for selected monitoring areas.')            return                        # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Loading monitoring area features...')        arcpy.SetProgressorPosition(2)                # get features (will always have at least one, as we validated earlier)        try:            driver = ogr.GetDriverByName('OpenFileGDB')             data_source = driver.Open(os.path.dirname(in_feat), 0)            feats = data_source.GetLayer('monitoring_areas')        except:            arcpy.AddError('Could not open monitoring areas feature.')            return                        # # # # #        # notify and set progress bar to defaukt        arcpy.SetProgressor(type='default', message='Iterating through monitoring areas...')                        # begin monitoring process iteration        continue_monitoring = True        while continue_monitoring:            for feat in feats:                            # # # # #                # notify                 arcpy.AddMessage('Preparing parameters for area: {}...'.format(feat['area_id']))                                # set up expected netcdf file name and path                out_nc = os.path.join(in_data_path, 'cube' + '_' + feat['area_id'] + '.nc')                                # get start year and latest year for end from dataset                 s_year = '{}-01-01'.format(feat['s_year'])                e_year = '{}-12-31'.format(datetime.now().year)  # now                                # get parameters for platform                # note, this does not consider sentinel nrt at moment.                 # see nrt.sync_nrt_cube for temp solution                # change when coll 3 sentinel avail                #params = nrt.get_satellite_params(platform=feat['platform'])                                                 # transform from albers to wgs84 if needed                geom = feat.geometry()                geom_prj = ogr.CreateGeometryFromWkb(geom.ExportToIsoWkb())  # quick copy                if 'GDA_1994_Australia_Albers' not in geom_prj.ExportToWkt():                    geom_prj = nrt.reproject_ogr_geom(geom=geom_prj, from_epsg=3577, to_epsg=4326)                                    # get bbox in wgs84 from geometry                bbox = geom_prj.GetEnvelope()                bbox = [bbox[0], bbox[2], bbox[1], bbox[3]]                                                   # # # # #                # notify                 arcpy.AddMessage('Checking monitoring area: {}...'.format(feat['area_id']))                                                # check if feature is valid                is_valid = nrt.validate_monitoring_area(area_id=feat['area_id'],                                                        platform=feat['platform'],                                                         s_year=feat['s_year'],                                                         e_year=feat['e_year'],                                                         index=feat['index'])                                                                        # check if monitoring area is valid                if not is_valid:                    arcpy.AddWarning('Invalid monitoring area: {}, skipping.'.format(feat['area_id']))                    continue                # # # # #                # notify                 arcpy.AddMessage('Creating / Syncing monitoring area cube: {}...'.format(feat['area_id']))                                # get parameters for platform                # note, this does not consider sentinel nrt at moment.                 # see nrt.sync_nrt_cube for temp solution                # change when coll 3 sentinel avail                params = nrt.get_satellite_params(platform=feat['platform'])                                  # open existing cube if exists, get date information                ds_existing = None                if os.path.exists(out_nc):                    try:                        ds_existing = xr.open_dataset(out_nc)                        num_times = len(ds_existing['time'])                        s_year = ds_existing.isel(time=-1)                        s_year = str(s_year['time'].dt.strftime('%Y-%m-%d').values)                    except:                        arcpy.AddWarning('Could not open cube: {}, skipping.'.format(feat['area_id']))                        continue                                # sync cube to now. combines old cube with new images if exist                ds = nrt.sync_nrt_cube(out_nc=out_nc,                                       collections=params.get('collections'),                                       bands=params.get('bands'),                                       start_dt=s_year,                                       end_dt=e_year,                                       bbox=bbox,                                       in_epsg=3577,  # always albers                                       slc_off=False,                                       resolution=params.get('resolution'),                                       ds_existing=ds_existing,                                       chunks={})                                                                                         # download, overwrite existing, and compute                with rasterio.Env(**rasterio_env):                    tools.export_xr_as_nc(ds=ds.astype('int16'), filename=out_nc)                    ds = ds.compute()                                                        # if existingt was found, notify and close                 if ds_existing is not None:                    arcpy.AddMessage('Added {} images to cube.'.format(len(ds['time']) - num_times))                # # # # #                # notify                 arcpy.AddMessage('Extracting attributes from cube...')                                # get dataset and band attributes                ds_attrs = ds.attrs                ds_band_attrs = ds[list(ds.data_vars)[0]].attrs                ds_spatial_ref_attrs = ds['spatial_ref'].attrs                                    # # # # #                # notify                 arcpy.AddMessage('Removing invalid pixels and empty dates...')                                # check if expected band name exists                mask_band = arc.get_name_of_mask_band(list(ds.data_vars))                # remove invalid pixels and empty scenes                ds = cog.remove_fmask_dates(ds=ds,                                             valid_class=[1, 4, 5],    # always valid, snow, water                                            max_invalid=0,            # always cloudless images only                                            mask_band=mask_band,                                             nodata_value=np.nan,                                             drop_fmask=True)                # # # # #                # notify                 arcpy.AddMessage('Calculating vegetation index: {}...'.format(feat['index']))                                # conform dea aws band names based on platform                ds = satfetcher.conform_dea_ard_band_names(ds=ds, platform=feat['platform'].lower())                                 # calculate vegetation index                 ds = tools.calculate_indices(ds=ds,                                              index=feat['index'].lower(),                                              custom_name='veg_idx',                                              rescale=False,                                              drop=True)                # append original attributes on to new band                ds['veg_idx'].attrs = ds_band_attrs                # # # # #                # notify                 arcpy.AddMessage('Building and applying edge mask...')                                                # convert feature to layer and use to mask                geom = ogr.Open(feat.ExportToJson(), 0)                lyr = geom.GetLayer()                mask = nrt.mask_xr_via_polygon(geom=lyr,                                                x=ds['x'].data,                                                y=ds['y'].data,                                                bbox=ds.geobox.extent.boundingbox,                                                transform=ds.geobox.transform,                                                ncols=len(ds['x']),                                                nrows=len(ds['y']),                                                mask_value=1)                                                               # apply mask to current dataset, set everything outside to nan                ds = ds.where(mask)                                # # # # #                # notify                 arcpy.AddMessage('Performing static and dynamic change detection...')                                                # perform static, dynamic and summary methods                 ds_change = nrt.build_change_cube(ds,                                                   training_start_year=int(feat['s_year']),                                                   training_end_year=int(feat['e_year']))                                # # # # #                # notify                 arcpy.AddMessage('Cleaning up static and dynamic change detection outputs...')                # re-mask summary and change cubes as nan pixels now set to non-nan                ds_change = ds_change.where(mask)                                # append attributes back on to dataset                 ds_change.attrs = ds_attrs                ds_change['spatial_ref'].attrs = ds_spatial_ref_attrs                for var in list(ds_change.data_vars):                    ds_change[var].attrs = ds_band_attrs                                    # temp                               ds_change.to_netcdf(r'C:\Users\Lewis\Desktop\test_change\change_{}.nc'.format(feat['area_id']))                                                # # # # #                # notify                 arcpy.AddMessage('Applying traffic light algorithm...')                # todo                 # todo                # todo                                                 # # # # #                # email user alert if detected and requested                if feat['alert'] == 'Yes':                                        # if traffic light == True:                                    # notify                     arcpy.AddMessage('Emailing alrert...')                                        #in_email_from = parameters[3].value                # email from                     #in_smtp_server = parameters[4].value               # email smtp server                     #in_smtp_port = parameters[5].value                 # email smtp port                     #in_smtp_username = parameters[6].value             # email smtp username                     #in_smtp_password = parameters[7].value             # email smtp password                                         # send email                    #nrt.send_email_alert(sent_from='mrlewie@outlook.com',                                          #sent_to='mrlewie@outlook.com',                                          #subject='Area ID: {} has been flagged.'.format(feat['area_id']),                                          #body_text='Area ID: {} has been found to have a decline. Please check.'.format(feat['area_id']),                                          #smtp_server='smtp.office365.com',                                          #smtp_port=587,                                          #username='mrlewie@outlook.com',                                          #password='halfLife1985micr')                # close dataset                  ds.close()                ds_change.close()                del ds                del ds_change                                # safely close existing                if ds_existing is not None:                    ds_existing.close()                    del ds_existing                                                                        # set processing to false if not requested to end this            in_time_interval = 0.01                                    if in_ongoing:                arcpy.AddMessage('Hibernating for {} hours. Press stop to cancel.'.format(in_time_interval / 60 / 60))                time.sleep(in_time_interval)            else:                continue_monitoring = False                            # # # # #        # notify and increment progress bar        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(3)        # notify user        arcpy.AddMessage('Monitoring areas synced successfully.')        returnclass Tool(object):    def __init__(self):        """Define the tool (tool name is the name of the class)."""        self.label = "Tool"        self.description = "Tool Template"        self.canRunInBackground = False    def getParameterInfo(self):        """Define parameter definitions"""                params = None        return params    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """The source code of the tool."""                            return# testingclass Test(object):    def __init__(self):        """Define the tool (tool name is the name of the class)."""        self.label = "Test"        self.description = "Tool Template"        self.canRunInBackground = False    def getParameterInfo(self):        """Define parameter definitions"""                param1 = arcpy.Parameter(                   displayName='Baseline years',                   name='in_baseline_years',                   datatype='GPValueTable',                   parameterType='Required',                   multiValue=False,                   direction='Input')                        param1.columns = [['GPLong', 'Start year'], ['GPLong', 'End year']]        param1.filters[1].type = 'ValueList'        param1.filters[1].list = [2000, 2001, 2002, 2003, 2004]        #param1.values = [['NAME', 'SUM']]                                        params = [param1]        return params    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """The source code of the tool."""                import time         import arcpy        #import arcgisscripting                # shared folder        sys.path.append(FOLDER_SHARED)        import arc                            # get current project and active map        p = arcpy.mp.ArcGISProject('CURRENT')        m = p.activeMap        # remove temp layer         for layer in m.listLayers():            if 'NRT Create Monitoring Areas Draw' in layer.name:                m.removeLayer(layer)        # apply symbology        for layer in m.listLayers('monitoring_areas'):            try:                arc.apply_monitoring_area_symbology(layer)            except Exception as e:                arcpy.AddMessage(e)                                    # # set transparency            # alpha = 80                            # # get symbology, update renderer, target color field            # sym = layer.symbology            # sym.updateRenderer('UniqueValueRenderer')            # sym.renderer.fields = ['color']                        # # iter group items and colorize            # for grp in sym.renderer.groups:                # for itm in grp.items:                    # try:                        # # get class value and convert to int                        # val = int(itm.values[0][0])                        # # apply fill color                        # if val == 0:                            # itm.symbol.color = {'RGB': [255, 255, 255, alpha]}                        # elif abs(val) == 1:                            # itm.symbol.color = {'RGB': [255, 115, 223, alpha]}                         # elif abs(val) == 2:                            # itm.symbol.color = {'RGB': [115, 178, 255, alpha]}                        # elif abs(val) == 3:                            # itm.symbol.color = {'RGB': [115, 178, 255, alpha]}                        # elif abs(val) == 4:                            # itm.symbol.color = {'RGB': [115, 223, 255, alpha]}                        # elif abs(val) == 5:                            # itm.symbol.color = {'RGB': [115, 255, 223, alpha]}                        # elif abs(val) == 6:                            # itm.symbol.color = {'RGB': [163, 255, 115, alpha]}                        # elif abs(val) == 7:                            # itm.symbol.color = {'RGB': [209, 255, 115, alpha]}                        # elif abs(val) == 8:                            # itm.symbol.color = {'RGB': [255, 255, 115, alpha]}                        # elif abs(val) == 9:                            # itm.symbol.color = {'RGB': [255, 211, 127, alpha]}                        # elif abs(val) == 10:                            # itm.symbol.color = {'RGB': [255, 167, 127, alpha]}                        # elif abs(val) == 11:                            # itm.symbol.color = {'RGB': [255, 127, 127, alpha]}                        # # apply border style now for stable, incline, decline                        # if val == 0:                            # itm.symbol.size = 2                            # itm.symbol.outlineColor = {'RGB': [0, 0, 0, alpha]}                         # elif val > 0:                            # itm.symbol.size = 2                            # itm.symbol.outlineColor = {'RGB': [0, 112, 255, alpha]}                        # elif val < 0:                            # itm.symbol.size = 2                            # itm.symbol.outlineColor = {'RGB': [255, 0, 0, alpha]}                             # except:                        # print('Symbology class value not supported, skipping.')                        # continue                                    # # finally, apply the symbology            # layer.symbology = sym                                                return        